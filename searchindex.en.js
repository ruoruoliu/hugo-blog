var relearn_searchindex = [
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "This is a log.",
    "description": "This is a log.",
    "tags": [],
    "title": "Log",
    "uri": "/hugo-blog/log/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/hugo-blog/tags/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 LangGraph框架学习 LangGraph框架学习 LangGraph学习手册 知识 LangGraph [!warning] 待办",
    "description": "总结 LangGraph框架学习 LangGraph框架学习 LangGraph学习手册 知识 LangGraph [!warning] 待办",
    "tags": [
      "周记"
    ],
    "title": "Week15 LangGraph框架学习",
    "uri": "/hugo-blog/weekly/week15/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Weeklies",
    "uri": "/hugo-blog/weekly/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: 周记",
    "uri": "/hugo-blog/tags/%E5%91%A8%E8%AE%B0/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Blogs",
    "uri": "/hugo-blog/blogs/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Agents 通过create_agent函数创建，底层通过LangGraph实现基于图的agent运行时，图包含节点（node）和边（edge），agent在节点间移动，包括model、tools和中间件 核心组件包括：\nModel： 支持静态模型，即初始化固定后，交互过程中不变 支持动态选择的，可以使用wrap_model_call中间件，基于请求拦截并override模型 Tools： 多工具串行调用、工具并行调用 基于之前结果的工具动态选择 工具的错误重试和处理逻辑：使用wrap_tool_call中间件 保持工具调用之间的状态一致 动态工具：即在运行过程中进行工具变更，包括： 根据当前状态和上下文，利用wrap_model_call中间件动态筛选已注册工具，这种方式适用于所有工具在创建agent时已明确 如果工具是运行时动态发现的，比如来自MCP server、用户数据等，需要运行时注册工具，包括添加（wrap_model_call）和处理（wrap_tool_call）逻辑 System prompt： 可以接受str或者SystemMessage类型（支持claude的prompt caching） 通过dynamic_prompt中间件基于request修改system prompt Invocation： state就是message的序列，通过invoke来输入一条新的message 可以通过stream触发流式输出 高级概念：\nStructured output 通过response_format对agent的回复进行格式化： 使用ToolStrategy对一个结构化数据格式进行包装，利用模型tool的能力来进行格式化 如果模型支持native structured output，可以直接使用ProviderStrategy进行包装 Memory： agent本身通过State（message序列）记录对话过程，可以通过以下方式扩展memory： 定义基于AgentState的扩展state，再通过中间件（比如内部的before_model的hook）实现state的修改，这种方式可以把逻辑和所需要的工具绑定在state修改的时刻，而不用在create_agent的时候绑定 定义基于AgentState的扩展state，传入state_schema字段，只适合大模型直接通过工具（而不是中间件）访问的memory，不够可控 Middleware： 在模型调用前：message裁剪、上下文注入 修改或者验证模型输出：guardrails和内容过滤 处理工具调用失败 基于state或者context进行动态模型选择 加入定制化log、监控和分析 Models Model的常见使用：\n模型可以以两种形式使用： agent内部调用，用于agent行为的决策 离线调用，即脱离agent框架直接调用，用于做一些文本生成、分类等任务 通过init_chat_model，然后通过invoke/stream/batch调用 通过.bind_tools可以绑定工具 在agent内可以自动调用，而在离线下需要手动调用 当模型认为需要多个tool call时，response.tool_calls会返回一个列表 通过pydantic、typeddict或者json进行结构化输出，返回字典 可以通过.with_structured_output绑定，主要用于model直接调用，agent中不适用 pydantic提供字段校验，而typeddict和json需要人工校验 通过.profile获取模型基础 根据context window size触发summarization的中间件 判断模型是否native结构化输出 根据模型是否支持多模态来限制输入 通过content_blocks进行多模态的输入和输出 通过content_blocks中type=“reasoning”的部分获取模型的思考内容 某些模型厂商提供： 隐式（前缀命中自动cache）以及显式（基于参数）的prompt caching 某些模型厂商提供server侧的工具调用，也会以content_blocks的形式返回 基于base URL/Proxy方式的调用 输出token的log prob token使用情况 通过invoke中的config参数，可以额外给每轮带入特定信息： 方便通过LangSmith来debug、加入log和监控、控制token使用等 配置每轮不同的模型：“configurable”: {“model”: “gpt-5-nano”} Messages message包含三部分：\nRole：角色，比如system、user Content：内容，包括文本、图片、文档等 Metadata：可选属性，比如回复元数据（模型信息、log prob等）、消息id、token usage等 message的基本用法：\ntext prompts：纯文本，当不需要交互对话历史时使用，最简单 message prompts： SystemMessage：system prompt HumanMessage：用户输入 AIMessage：大模型输出，包括对工具发起调用的message ToolMessage：工具调用输出 dict：通过字典包含“role”、“content”等信息，适用于openai chat completion的格式 message的内容可能包含：\n纯文本 模型厂商支持的内容列表 LangChain标准的content_blocks，可以根据上述内容转化为content_blocks Tools 构造工具的方法：\n@tool：使用函数装饰器，要求函数包含明确且简介的docstring，且type hint完整 在装饰器上可以添加自定义参数： @tool(“web_search”)：覆盖tool的名字 @tool(“calculator”, description=“Performs arithmetic calculations…\")：覆盖tool的描述 @tool(args_schema=XXX)：通过pydantic的方式，给tool的输入提供更详细的描述 注意：config和runtime是保留关键字，不能用于tool的参数名 通过ToolRuntime可以获取工具所需要的信息，包括：\nState：短期记忆，包括对话历史等 通过Command命令更新State：Command(update={“user_name”: new_name}) Context：外部传入的属性信息，可以基于用户属性定制化回复 Store：长期记忆，跨对话的数据，包括用户偏好、知识库等 通过定义存储和读取的tool，来和runtime.store进行交互 store是一个层次化结构，由命名空间（Namespace，类型是tuple）和具体的key组成 store.put((“users”,), user_id, user_info) store.get((“users”,), user_id) Stream Writer：实时输出过程信息，类似进度条 Config：执行配置信息 Tool Call ID：当前工具调用id ToolNode是LangGraph中预定义的节点，负责并行工具执行、错误处理和状态注入：\nbuilder = StateGraph(MessagesState)之后 并行工具执行：builder.add_node(“tools”, tool_node)，tool_node包含多个tool 错误处理： tool_node = ToolNode(tools, handle_tool_errors=True) tool_node = ToolNode(tools, handle_tool_errors=“Something went wrong.”) tool_node = ToolNode(tools, handle_tool_errors=handle_error)，填入处理逻辑 tool call调用判断：builder.add_conditional_edges(“llm”, tools_condition) 状态注入：tool_node = ToolNode([get_message_count])，get_message_count通过runtime.state来计数的工具 Short-term memory 通过在create_agent中加载checkpointer来支持对话内的状态保持，即short-term memory：\nfrom langchain.agents import create_agent from langgraph.checkpoint.memory import InMemorySaver agent = create_agent( \"gpt-5\", tools=[get_user_info], checkpointer=InMemorySaver(), ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]}, {\"configurable\": {\"thread_id\": \"1\"}}, ) 在生产环境中，可以使用数据库来存储：\nfrom langchain.agents import create_agent from langgraph.checkpoint.postgres import PostgresSaver DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" with PostgresSaver.from_conn_string(DB_URI) as checkpointer: checkpointer.setup() # auto create tables in PostgresSql agent = create_agent( \"gpt-5\", tools=[get_user_info], checkpointer=checkpointer, ) 解决过长的context，可以通过以下几种方式：\nTrim messages：利用before_model或after_model，messages数量过多时，保留最近的几条 Delete messages：利用内置的RemoveMessage删除指定id的message Summarize messages：为了避免直接删除损失信息，在用总结的方式，利用SummarizationMiddleware中间件，设置参数举例： model=“gpt-4.1-mini” trigger=(“tokens”, 4000) keep=(“messages”, 20) Streaming 流式输出总体有四类：\nagent进度（updates）：基于每个agent step的状态更新，比如model、tools、model… LLM token（messages）：基于每个token的生成 用户自定义（custom）：比如每获取10个文件一次输出，类似log 多模式：以上updates、messages、custom的混合 常见的几种情景：\n工具调用：通过updates捕捉tool call的调用结果，也可以通过custom在工具中写入，然后在主逻辑里打印 human-in-the-loop：通过HumanInTheLoopMiddleware中间件，在某些工具调用时拦截，收集到decisions后利用Command的resume继续主逻辑 LangChain中interrupt_on触发后的流程 多agent：当涉及多agent时，子agent创建时设定name，stream过程中通过metadata的lc_agent_name字段确定当前输出的agent #todo 基于react构建生成式UI： # Frontend\nStructured output 通过response_format可以让模型输出指定格式的回复：\nToolStrategy(T)：把格式包装成工具，让模型通过工具的方式调用，tool的参数就是指定格式 ProviderStrategy(T)：模型厂商原生支持某种格式，比如OpenAI的json_schema（受限解码），不需要包装为工具，需要在prompt里配合，减少token消耗 type(T)：不用strategy封装，model.with_structured_output(T)，langchain自动判断模型是否支持此格式 LangChain的response_format中，json_schema是什么？\nMiddleware 中间件帮助你控制和定制化agent的每一步：\n通过log、分析和debug追踪agent的行为 转换prompt、工具选择和输出格式 添加重试、兜底和提前结束策略 添加rate limit、guardrails、PII检测（personally identifiable information） 中间件在agent loop中可以在下图紫色部分中使用： Prebuilt middleware LangChain提供一下内置中间件：\nSummarization：当context超长时自动总结，保留最近的轮次，压缩之前的轮次 Human-in-the-loop：对于高风险操作进行截断，并寻求用户对于工具的许可、编辑或拒绝 Model call limit：防止模型进入循环而导致调用过多 Tool call limit：防止工具调用过多 Model fallback：当主模型失败后，回退到兜底模型 PII detection：通过可配制策略，检测是否存在PII内容 To-do list：给agent配备todo list，来解决复杂问题 给agent提供了write_todos的工具，以及在prompt中引导进行有效的任务规划 还有专为deep agent配置的特殊版本中间件，在create_deep_agent时默认自动加载 LLM tool selector：专门使用LLM处理多个tool的筛选问题 Tool retry：工具失败后的自动重试 Model retry：模型调用失败后的自动重试 LLM tool emulator：模拟工具执行，不实际调用工具，方便测试模型行为 Context editing：通过应规则对不必要的工具记录进行删减，控制context长度 Shell tool：提供持久化的shell session，用于命令行执行 File search：提供glob和grep操作，搜索文件系统 FilesystemMiddleware：提供了一整套交互工具，包括ls、read_file、write_file和edit_file，方便进行短期记忆，另外可以通过StoreBackend指定保存到/memories/，存储为长期记忆（不同的会话可以共享） Subagent：为当前agent指定subagent，包括他的名字、描述、任务（system prompt）和可用工具 Custom middleware 通过hook自定义中间件可以在agent执行的指定节点实现拦截逻辑：\nNode-style：作为一个执行步，通常用于log、验证和状态更新 before_agent和after_agent：在agent交互（invocation）前后 before_model和after_model：在模型调用前后 Wrap-style：在handler执行过程中进行拦截，通常用于重试、缓存和转换 什么是handler：handler封装了当前序列后续所有操作 如果当前中间件后面还有其他中间件，handler就是下一个中间件 如果当前中间件已经是最后一个，handler就是模型调用 wrap_model_call：包裹每次模型调用 wrap_tool_call：包裹每次工具调用 创建中间件有以下两种方式：\n基于装饰器：通过装饰器包裹函数，即可实现简单的单hook中间件 基于类，通常在以下场景使用： 包含多个hook 包含复杂配置 同时包含同步/异步版本 通过中间件可以更新自定义state schema，从而可以扩展agent的state：\n贯穿多轮的状态信息，实现用户状态跟踪，可以进而做条件判断 在中间件之间共享信息，比如从before_model到after_model 中间件的执行顺序，对于以下代码：\nagent = create_agent( model=\"gpt-4.1\", middleware=[middleware1, middleware2, middleware3], tools=[...], ) before_*：从前向后执行 after_*：从后向前执行 wrap_*：从前向后执行（前面的包裹后面的所有） 中间件通过返回字典，key=jump_to，可以实现逻辑跳转：\nend：到agent执行的最后（如果有after_agent，到after_agent之前） tools：到工具节点 model：到模型节点（如果有before_model，到before_model之前） Guardrails Guardrails通常使用于以下场景：\n防止PII泄露 检测和阻止prompt注入攻击 阻止有害内容 强制商业规则和合规要求 验证输出质量和准确性 Guardrails的实现：\n通过确定性策略（关键字匹配等）和基于模型的方式互补 LangChain提供了内置的PII detection和Huma-in-the-loop中间件 自定义guardrails可以通过中间件的方式实现（before_agent和after_agent） Runtime LangChain的create_agent底层在LangGraph的runtime上运行，runtime包含：\nContext：静态信息，包括用户id，数据库连接等 通过create_agent的context_schema参数明确context结构，每次invoke的时候填入内容 通过tools获取context（runtime.context）、读写长期记忆、写入custom stream 通过中间件 node-style：直接通过runtime.context获取 wrap-style：通过ModelRequest的runtime.context获取 Store：长期记忆的存储 Stream writer：用于custom模式的agent写出 Context engineering LangChain通过中间件的方式在以下三个方面做agent的context engineering：\nModel Context：瞬时，只在模型调用时存在的context System Prompt：通过@dynamic_prompt中间件控制prompt内容，过长时进行提示 Messages：从state、store和runtime context提取内容（如文件内容、风格描述等），加载到message中 Tools：工具选择来控制工具数量，简化context Model：根据context长度可以动态选择模型，不同模型的context大小不同 Response format：根据对话选择不同的回复格式，简单或者详细 Tool Context：工具的调用结果会更新state 读：可以从state、store和runtime context中读取信息 写：可以将信息直接写入model context，也可以更新state或者store，供后续调用 Lift-cycle Context：指生命周期中的context，包括中间件等，也会更新state 中间件可以更新context 中间件可以跳转到指定节点 Model Context Protocol LangChain中使用MCP的方法：\nLangChain内置了langchain_mcp_adapter模块，帮助调用mcp server的工具 通过MultiServerMCPClient初始化client，加载mcp server 通过await client.get_tools()获取所有的工具，在create_agent时候填入 通过FastMCP库将本地函数封装为mcp server，包括两种连接方式： stdio：通过本地文件标准输出获取结果 HTTP：通过http协议请求获取结果，填入url即可，服务可本地也可远程 通常MCP请求是无状态的，如果想在生命周期保持，可以通过client.session()维持 通过client.get_resources()可以获取MCP的数据资源，load_mcp_resources可以直接载入session，方便后续处理 client.get_prompt()和load_mcp_prompt和上面resources一样，针对prompt 更多高级用法，如工具拦截、进度通知、日志以及交互式对话，参考： Advanced features\nHuman-in-the-loop Human-in-the-loop的流程：\n通过HumanInTheLoopMiddleware中间件配置每个工具是否需要人工干预 中间件的原理是实现了一个after_model的hook，在调用工具前判断是否需要interrupt LangChain中HITL中间件实现是after_model的原因 当工具被interrupt，LangGraph的persistence layer会固定graph state，用于之后的resume 用户的decision用来判断resume之后具体怎么做，包括approve、edit和reject 当有多个action需要干预时，decision按工具的请求顺序返回 Multi-agent Subagents 主agent给多个subagent分配任务，决定什么时候调用哪个subagent，最终由主agent回复\nSubagents包括以下特点：\n集中控制，所有路径最终经过主agent 只有主agent与用户交互，subagent不会 subagent以类似tool的方式被调用 主agent可以同时调用多个subagent，实现并行处理 实现方式：\n通过create_agent创建subagent，通过tool的方式调用subagent的invoke 设计策略：\nSync vs async：主agent是否需要等待subagent结果 异步一般通过三个tool完成： 通过启动一个job，并返回job ID 根据job ID检查状态 当完成后通知用户，点击确认后返回结果（类似HITL方式） Tool patterns，有以下两种方式： Tool per agent：每个agent一个tool，精确控制输入输出 Single dispatch tool：只有一个task工具，这个工具可以调用不同的subagent，subagent独立开发，可扩展性强，有自己独立的context，且被渐进式披露，增加subagent几乎对主agent逻辑没有影响 Context engineering，主agent和subagent如何交互： Subagent specs：给subagent设置合适的name和description，包括 System prompt enumeration：当subagent数量较少，且相对固定的时候，可以在prompt中写死 Enum constraint：通过enum类型保证选择subagent时的类型正确，可以提前报错 Tool-based discovery：当subagent数量较大或动态加载时，使用工具进行search，达到渐进式披露的效果 Subagent inputs：通过state获取subagent需要的context信息，而不是直接把主agent的context传给subagent Subagent outputs：对subagent的回复做后处理，包括 Prompt the subagent：通过prompt要求subagent如何回复 Format in code：通过Command(update=XXX)，subagent可以更新主agent的state Handoffs 基于state进行agent的切换，具体指行为的切换，包括tools和prompt，或者agent本体的切换\n使用场景：\n一个顺序的逻辑，条件触发进入到下一个环节 每个环节都需要和用户进行直接交互（相比于通过主agent间接交互） 常见包括人工客服：需要收集一系列用户信息，然后推进流程（退款等） 可以通过以下两种方式实现：\nSingle agent with middleware：通过动态配置的方式切换行为，使用@wrap_model_call中间件重载agent的system prompt和tools 通过LangGraph的多个node实现mutiple agent，通过状态机判断去哪一个node 需要传递上一轮的AIMessage（触发handoff）以及ToolMessage（实施handoff）给下一轮的subagent，防止subagent失去背景信息 Skills 主agent根据需要加载特定的prompt和知识，相当于虚拟handoff，即主agent行为能力发生变化\nSkills具有如下特点：\nprompt驱动的能力：skills是基于prompt定义的 渐进式披露：skills是基于状态和context来加载的 skills可以是层级式的，高层级的skill触发暴露低层级的skill 分布式开发：不同的团队并行开发skills 相比其他的subagent系统更简单 可以参考脚本、模板文件和其他资源 Skills的使用场景：\n当你希望一个agent具备很多能力时，比如code assistant（不同语言、技术栈） Router 通过一个router将query进行拆分，分配给一个或多个subagent并发执行，最终将结果汇总\nRouter可以设计成stateless和stateful：\nstateless：router调用subagent as tool stateful：将router包装为一个tool，外层用一个agent调用router，router本身stateless，外层agent有状态 Custom workflow 通过LangGraph构建执行图，混合确定性逻辑和agentic逻辑，将不同模式作为node嵌入图中 LangGraph学习手册\nRetrieval 如何在LangChain中搭建向量知识库： # Build a semantic search engine with LangChain\nRAG的几种方式：\n2-step RAG：先检索，再生成， # Build a RAG agent with LangChain Agentic RAG：通过LLM判断检索的时间点和目标，这种方式与普通agent唯一的差异在于拥有一个能够搜索外界知识的工具 Hybrid：加入一些中间步，相比2-step更灵活，相比Agentic更可控 query预处理：模糊query改写、多query扩展，query细化 检索验证：判断检索结果是否与query相关，不相关就修改query重新检索 答案验证：判断答案是否准确、完整且与query相关，如果需要的话就重新生成或修改答案 Long-term memory Long-term memory的实现方式\n通过LangGraph的store来实现long-term memory 每个memory保存在namespace下，再用key区分 namespace通常包含user或组织id信息 从runtime获取store，通过工具实现get（或者search）和put来读写memory 参考链接：\nLangChain Docs: LangChain",
    "description": "Agents 通过create_agent函数创建，底层通过LangGraph实现基于图的agent运行时，图包含节点（node）和边（edge），agent在节点间移动，包括model、tools和中间件 核心组件包括：\nModel： 支持静态模型，即初始化固定后，交互过程中不变 支持动态选择的，可以使用wrap_model_call中间件，基于请求拦截并override模型 Tools： 多工具串行调用、工具并行调用 基于之前结果的工具动态选择 工具的错误重试和处理逻辑：使用wrap_tool_call中间件 保持工具调用之间的状态一致 动态工具：即在运行过程中进行工具变更，包括： 根据当前状态和上下文，利用wrap_model_call中间件动态筛选已注册工具，这种方式适用于所有工具在创建agent时已明确 如果工具是运行时动态发现的，比如来自MCP server、用户数据等，需要运行时注册工具，包括添加（wrap_model_call）和处理（wrap_tool_call）逻辑 System prompt： 可以接受str或者SystemMessage类型（支持claude的prompt caching） 通过dynamic_prompt中间件基于request修改system prompt Invocation： state就是message的序列，通过invoke来输入一条新的message 可以通过stream触发流式输出 高级概念：\nStructured output 通过response_format对agent的回复进行格式化： 使用ToolStrategy对一个结构化数据格式进行包装，利用模型tool的能力来进行格式化 如果模型支持native structured output，可以直接使用ProviderStrategy进行包装 Memory： agent本身通过State（message序列）记录对话过程，可以通过以下方式扩展memory： 定义基于AgentState的扩展state，再通过中间件（比如内部的before_model的hook）实现state的修改，这种方式可以把逻辑和所需要的工具绑定在state修改的时刻，而不用在create_agent的时候绑定 定义基于AgentState的扩展state，传入state_schema字段，只适合大模型直接通过工具（而不是中间件）访问的memory，不够可控 Middleware： 在模型调用前：message裁剪、上下文注入 修改或者验证模型输出：guardrails和内容过滤 处理工具调用失败 基于state或者context进行动态模型选择 加入定制化log、监控和分析 Models Model的常见使用：",
    "tags": [
      "技术笔记"
    ],
    "title": "LangChain学习手册",
    "uri": "/hugo-blog/blogs/langchain%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: 技术笔记",
    "uri": "/hugo-blog/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "LangGraph是一个底层的agent编排工具\nfrom langgraph.graph import StateGraph, MessagesState, START, END def mock_llm(state: MessagesState): return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]} graph = StateGraph(MessagesState) graph.add_node(mock_llm) graph.add_edge(START, \"mock_llm\") graph.add_edge(\"mock_llm\", END) graph = graph.compile() graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]}) 参考链接：\nLangChain Docs: LangGraph",
    "description": "LangGraph是一个底层的agent编排工具\nfrom langgraph.graph import StateGraph, MessagesState, START, END def mock_llm(state: MessagesState): return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]} graph = StateGraph(MessagesState) graph.add_node(mock_llm) graph.add_edge(START, \"mock_llm\") graph.add_edge(\"mock_llm\", END) graph = graph.compile() graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]}) 参考链接：\nLangChain Docs: LangGraph",
    "tags": [
      "技术笔记"
    ],
    "title": "LangGraph学习手册",
    "uri": "/hugo-blog/blogs/langgraph%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "TypedDict 通常的字典类型无法限制dict的key字段：\ntype User = dict[str, str | int] bob: User = {'job': 'chef', 'wage': 1000} TypedDict帮助限制dict的字段结构:\nfrom typing import TypedDict, Required, NotRequired, ReadOnly class User(TypedDict, total=False): name: ReadOnly[str] age: int email: Required[str] TypeDict的使用方式：\nTypedDict可以嵌套 TypedDict中的成员名称要和dict中的key一致，包括大小写 total字段设置False表示某些字段是optional的 可以使用Required和NotRequired表示字段级别的是否必需 可以使用ReadOnly表示某个字段是否只读 注意：上述如果不符合要求，只会警告提示，不会报错\n参考链接：\n# TypedDict is Awesome in Python Pydantic 相比TypedDict的约定形式，Pydantic是更严格的运行时格式校验，是dataclass的第三方强化版，通常用于构建健壮的API、配置和复杂数据模型\nfrom pydantic import BaseModel class UserModel(BaseModel): id: int name: str # 1. 自动转换：将 \"123\" 转为 int user = UserModel(id=\"123\", name=\"Gemini\") print(user.id) # 输出 123 (整数类型) # 2. 严格校验：如果类型实在转不动，直接报错 try: UserModel(id=\"abc\", name=\"Gemini\") except Exception as e: print(\"验证失败！\") Pydantic的使用方式：\n尝试修复数据，比如将ISO格式的日期字符串转成Python的datetime对象 支持通过.model_dump()或者.json()序列化，和.model_validate_json()反序列化 支持通过Field对字段进行细粒度设置，如工厂函数、大小限制 支持通过field_validator进行字段内容的验证和修改，包括前置和后置的处理顺序 参考链接：\n# Pydantic is Awesome in Python # Pydantic 让数据结构与验证融为一体 Asyncio Asyncio用单线程实现了高并发，使用方法：\n使用async def来将一个函数定义为协程对象 使用await来挂起，等待任务完成后继续 使用asyncio.run()来启动所有协程 使用asyncio.gather()来同时触发多个协程开始，避免一个一个触发 import asyncio async def fetch_data(): print(\"开始下载数据...\") await asyncio.sleep(2) # 模拟 IO 耗时操作，不阻塞 CPU print(\"数据下载完成！\") return {\"data\": 123} async def main(): # 同时启动两个任务 print(\"主程序启动\") result = await fetch_data() print(f\"结果是: {result}\") asyncio.run(main()) asyncio.gather的例子：\nasync def task(name, delay): await asyncio.sleep(delay) print(f\"任务 {name} 完成\") async def main(): # 就像发令枪，让所有任务一起跑 await asyncio.gather( task(\"A\", 2), task(\"B\", 1), task(\"C\", 3) ) asyncio.run(main()) # 总耗时约为 3 秒（最长的那个），而不是 2+1+3=6 秒 注意异步代码里不要加入同步阻塞代码：\n如time.sleep(3)，而使用对应的异步函数asyncio.sleep(3) 同步IO库：比如requests.get()、open().read()等 CPU密集型计算 可以使用asyncio.to_thread来使用没有异步版本的阻塞函数 参考链接：\n# 异步IO AsyncIO 使用详解",
    "description": "TypedDict 通常的字典类型无法限制dict的key字段：\ntype User = dict[str, str | int] bob: User = {'job': 'chef', 'wage': 1000} TypedDict帮助限制dict的字段结构:\nfrom typing import TypedDict, Required, NotRequired, ReadOnly class User(TypedDict, total=False): name: ReadOnly[str] age: int email: Required[str] TypeDict的使用方式：\nTypedDict可以嵌套 TypedDict中的成员名称要和dict中的key一致，包括大小写 total字段设置False表示某些字段是optional的 可以使用Required和NotRequired表示字段级别的是否必需 可以使用ReadOnly表示某个字段是否只读 注意：上述如果不符合要求，只会警告提示，不会报错\n参考链接：\n# TypedDict is Awesome in Python Pydantic 相比TypedDict的约定形式，Pydantic是更严格的运行时格式校验，是dataclass的第三方强化版，通常用于构建健壮的API、配置和复杂数据模型",
    "tags": [
      "技术笔记"
    ],
    "title": "Python学习手册",
    "uri": "/hugo-blog/blogs/python%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 LangChain框架学习 LangChain框架学习 LangChain学习手册 知识 typedDict通过约定的方式说明dict中的字段有哪些，如果不符合则会在运行前警告；而pydantic是dataclass的第三方强化版本，定义一个数据类，并保证类中的字段，如果不符合则会在运行时报错，pydantic可以进行自动转化，并支持字段级别的初始化处理逻辑 asyncio支持python中的异步逻辑，使用单线程避免任务等待 将函数定义为async，使用await来挂起等待结果返回 如果多个任务同时触发，可以使用asyncio.gather 使用asyncio.run来启动事件循环（EventLoop），触发全部协程 LangChain是帮助用户快速搭建agent的平台，用户通过设计system prompt、tool、中间件等，实现agentic逻辑，在交互过程中更新agent的state，控制模型上下文，得到结果 Model：LangChain支持各大厂商和开源model Tools：LangChain支持用@tool将函数封装为工具使用 中间件：中间件方便用户在模型调用前后，工具调用前后进行拦截，并实现特定逻辑，包括模型、工具、system prompt的修改等 一些内置的中间件，包括Human-in-the-loop中间件等 Memory：LangChain的短期记忆就是state，长期记忆就是store state就是对话序列，包含AIMessage、ToolMessage、HumanMessage等，以及用户定义的一些字段，通过state_schema传入 store就是外部存储，默认内存，可以选择数据库 response_format：通过ToolStrategy或者厂商原生能力，支持结构化输出 待办 LangGraph框架学习",
    "description": "总结 LangChain框架学习 LangChain框架学习 LangChain学习手册 知识 typedDict通过约定的方式说明dict中的字段有哪些，如果不符合则会在运行前警告；而pydantic是dataclass的第三方强化版本，定义一个数据类，并保证类中的字段，如果不符合则会在运行时报错，pydantic可以进行自动转化，并支持字段级别的初始化处理逻辑 asyncio支持python中的异步逻辑，使用单线程避免任务等待 将函数定义为async，使用await来挂起等待结果返回 如果多个任务同时触发，可以使用asyncio.gather 使用asyncio.run来启动事件循环（EventLoop），触发全部协程 LangChain是帮助用户快速搭建agent的平台，用户通过设计system prompt、tool、中间件等，实现agentic逻辑，在交互过程中更新agent的state，控制模型上下文，得到结果 Model：LangChain支持各大厂商和开源model Tools：LangChain支持用@tool将函数封装为工具使用 中间件：中间件方便用户在模型调用前后，工具调用前后进行拦截，并实现特定逻辑，包括模型、工具、system prompt的修改等 一些内置的中间件，包括Human-in-the-loop中间件等 Memory：LangChain的短期记忆就是state，长期记忆就是store state就是对话序列，包含AIMessage、ToolMessage、HumanMessage等，以及用户定义的一些字段，通过state_schema传入 store就是外部存储，默认内存，可以选择数据库 response_format：通过ToolStrategy或者厂商原生能力，支持结构化输出 待办 LangGraph框架学习",
    "tags": [
      "周记"
    ],
    "title": "Week14 LangChain框架学习",
    "uri": "/hugo-blog/weekly/week14/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Andrej Karpathy 与动物的进化和学习过程类比： 动物在出生的时候就用一套基础配置（物理设备和算法），比如斑马出生一个小时就会跟着妈妈走，这不是强化学习学到的，而是类似DNA中遗传的某种算法 这部分类比大模型的预训练，但是不同的是： 大模型花费大量时间（动物很快）从零开始（动物出生有DNA）进行预训练， 不光把思维模式学到了，还学习了知识 应该有一种方式可以剥离预训练学到的知识，因为这部分知识反而会阻碍后续的发展（比如知识陈旧或者思维定式） RL应该只在运动类任务（如走跑跳），但在需要智力任务中不应该完全依赖RL RL在LLM中的问题： RL基于一个reward对路径上的所有token进行加权或者减权，这不合理，因为中间的过程可能是错的 人类在做智力任务（比如数学题）的时候，会反思，判断哪部分做对了，哪部分做错了，而不是基于最终的一个答案来调整所有的路径 需要过程奖励，但是目前只能用LLM as judge来判断，而LLM是参数化的，因此可以进行reward hacking，因此这个过程也不可信 LLM的预训练过程缺乏深度理解： 预训练过程需要反思，但是这个很难，因为LLM通常都有坍缩的问题，缺少多样性，如果在这种样本（比如阅读某一本书，然后让他总结这本书）继续训练，会加大坍缩的问题 儿童就比成年人多样性强，随着经验增多，熵变小；人类因为记忆能力不如LLM，因此强迫需要掌握规律；这里又一次说明是否有一种方式可以将学习过程中的知识去掉，保留逻辑推理能力 如何解决预训练带来的多样性问题？用naive的方式增加多样性（比如强制prob平滑）也不合理，因为有些任务就是需要确定性 参考链接：\n# Andrej Karpathy — “We’re summoning ghosts, not building animals” Richard Sutton 关于LLM是不是智能的体现 应该关注人类与动物相同的部分，这部分才是智能的体现（了解松鼠的进化已经完成了90%的工作），而不应该关注人类与动物不同的部分（语言） LLM只是通过在静态数据集上训练，预测下一句话说什么，静态数据集不预测世界会发生什么，所以不是真正的世界模型 智能需要经历真实世界，交互并获得反馈，持续学习，而LLM生成的时候没有反馈，不知道生成的句子是好是坏，也没有从对方的回复中获得反馈，进而改变自己 John McCarthy关于智能的定义：智能是通往目标的计算能力 参考链接：\n# Richard Sutton – Father of RL thinks LLMs are a dead end",
    "description": "Andrej Karpathy 与动物的进化和学习过程类比： 动物在出生的时候就用一套基础配置（物理设备和算法），比如斑马出生一个小时就会跟着妈妈走，这不是强化学习学到的，而是类似DNA中遗传的某种算法 这部分类比大模型的预训练，但是不同的是： 大模型花费大量时间（动物很快）从零开始（动物出生有DNA）进行预训练， 不光把思维模式学到了，还学习了知识 应该有一种方式可以剥离预训练学到的知识，因为这部分知识反而会阻碍后续的发展（比如知识陈旧或者思维定式） RL应该只在运动类任务（如走跑跳），但在需要智力任务中不应该完全依赖RL RL在LLM中的问题： RL基于一个reward对路径上的所有token进行加权或者减权，这不合理，因为中间的过程可能是错的 人类在做智力任务（比如数学题）的时候，会反思，判断哪部分做对了，哪部分做错了，而不是基于最终的一个答案来调整所有的路径 需要过程奖励，但是目前只能用LLM as judge来判断，而LLM是参数化的，因此可以进行reward hacking，因此这个过程也不可信 LLM的预训练过程缺乏深度理解： 预训练过程需要反思，但是这个很难，因为LLM通常都有坍缩的问题，缺少多样性，如果在这种样本（比如阅读某一本书，然后让他总结这本书）继续训练，会加大坍缩的问题 儿童就比成年人多样性强，随着经验增多，熵变小；人类因为记忆能力不如LLM，因此强迫需要掌握规律；这里又一次说明是否有一种方式可以将学习过程中的知识去掉，保留逻辑推理能力 如何解决预训练带来的多样性问题？用naive的方式增加多样性（比如强制prob平滑）也不合理，因为有些任务就是需要确定性 参考链接：\n# Andrej Karpathy — “We’re summoning ghosts, not building animals” Richard Sutton 关于LLM是不是智能的体现 应该关注人类与动物相同的部分，这部分才是智能的体现（了解松鼠的进化已经完成了90%的工作），而不应该关注人类与动物不同的部分（语言） LLM只是通过在静态数据集上训练，预测下一句话说什么，静态数据集不预测世界会发生什么，所以不是真正的世界模型 智能需要经历真实世界，交互并获得反馈，持续学习，而LLM生成的时候没有反馈，不知道生成的句子是好是坏，也没有从对方的回复中获得反馈，进而改变自己 John McCarthy关于智能的定义：智能是通往目标的计算能力 参考链接：\n# Richard Sutton – Father of RL thinks LLMs are a dead end",
    "tags": [],
    "title": "大模型观点：Andrej Karpathy和Richard Sutton",
    "uri": "/hugo-blog/blogs/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%82%E7%82%B9andrej-karpathy%E5%92%8Crichard-sutton/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 跟进大模型强化学习技术 跟进大模型强化学习技术 LLM训练技术学习手册 知识 DAPO 通过对gradient的clip的上下限解耦，防止模型探索不够的问题 通过只采样reward不都等于0或1的batch，来提升训练效率，防止噪声引入的波动 通过缓冲区软长度惩罚，配合token-level的loss，提升模型对过长序列和错误的区分 GSPO 将策略梯度转化为序列级，使之与reward粒度对齐 通过几何平均的形式避免数值溢出，并对长度归一化 解决MoE模型需要Routing Replay的复杂度问题 待办 LangChain框架学习",
    "description": "总结 跟进大模型强化学习技术 跟进大模型强化学习技术 LLM训练技术学习手册 知识 DAPO 通过对gradient的clip的上下限解耦，防止模型探索不够的问题 通过只采样reward不都等于0或1的batch，来提升训练效率，防止噪声引入的波动 通过缓冲区软长度惩罚，配合token-level的loss，提升模型对过长序列和错误的区分 GSPO 将策略梯度转化为序列级，使之与reward粒度对齐 通过几何平均的形式避免数值溢出，并对长度归一化 解决MoE模型需要Routing Replay的复杂度问题 待办 LangChain框架学习",
    "tags": [
      "周记"
    ],
    "title": "Week13 大模型强化学习技术跟进",
    "uri": "/hugo-blog/weekly/week13/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "经典结构 参考链接：\nAttention Is All You Need 进展 注意力机制 MHA、MQA、GQA MHA（multi-head attention）：多头注意力，每个head维护自己的KV，在KV-Cache中占用显存很大 MQA（multi-query attention）：所有head共享KV，只有Q会被切分为多份，KV-Cache显存占用减小head-size倍 GQA（Grouped-Query Attention）：在MHA和MQA（效果和缓存）之间折中，KV分为若干份 参考链接：\nFast Transformer Decoding: One Write-Head is All You Need GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints # 理解Attention:从起源到MHA,MQA和GQA MLA KV是从输入 $x$ 线性变换得到的（先得到 $c$， 再得到 $k$ 和 $v$），如果KV-Cache里面只存 $c$（对 $x$ 进行压缩），就可以实现所有头共享（类似MQA，甚至更小）的显存占用： $$q_t^{(s)} k_i^{(s)\\top} = \\boldsymbol{x}_t \\left( W_q^{(s)} W_k^{(s)\\top} \\right) \\mathbf{c}_i^\\top$$ 在推理时，不用从 $c$ 还原 $k$ 和 $v$（会显存爆炸），而是直接对 $q$ 进行映射，然后直接与 $c$ 做乘法： $$\\text{Score} = (q_t^{(s)} W_{uk}^{(s)\\top}) \\cdot c_i^\\top$$ MLA下如何支持RoPE位置编码？ 参考链接：\n缓存与效果的极限拉扯：从MHA、MQA、GQA到MLA 位置编码 原始的位置编码分为：\n绝对位置编码：通过向量表示每个位置，但只能表示有限个（512个），且没有相对位置的概念 sinusoidal编码： 通过向量中每两位表示一个波长的绝对位置$$\\begin{aligned} PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}}) \\ PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})\\end{aligned}$$ 再利用sin和cos的性质巧妙表示相对位置关系 $$\\begin{bmatrix} \\sin(\\omega_i(pos+k)) \\ \\cos(\\omega_i(pos+k)) \\end{bmatrix} = \\begin{bmatrix} \\cos(\\omega_i k) \u0026 \\sin(\\omega_i k) \\ -\\sin(\\omega_i k) \u0026 \\cos(\\omega_i k) \\end{bmatrix} \\begin{bmatrix} \\sin(\\omega_i pos) \\ \\cos(\\omega_i pos) \\end{bmatrix}$$ 对于远大于训练预料的长文本，编码的组合还是模型没有见过的，由于编码加在词向量上，KQV做点积的时候，score会发生分布偏移 RoPE RoPE的表示：\n和sinusoidal类似，把位置编码两位一组，每两位表示同一个旋转角度 对KV进行位置编码的旋转（V不参与），在KV进行点积的时候，相对位置的差异可以保持 $$f_{{q,k}}(\\mathbf{x}m, m) = \\begin{pmatrix} \\cos m\\theta \u0026 -\\sin m\\theta \\ \\sin m\\theta \u0026 \\cos m\\theta \\end{pmatrix} \\begin{pmatrix} W{{q,k}}^{(11)} \u0026 W_{{q,k}}^{(12)} \\ W_{{q,k}}^{(21)} \u0026 W_{{q,k}}^{(22)} \\end{pmatrix} \\begin{pmatrix} x_m^{(1)} \\ x_m^{(2)} \\end{pmatrix}$$ RoPE通过$W_q$ 和 $W_k$ 这两个参数矩阵学习距离的含义：\n特征解耦：模型发现，如果把某些需要考虑近距离关系的语法特征投影到旋转频率高的维度，那么点积结果就会对位置变化非常敏感 长程依赖：如果把需要长距离关联的语义特征投影到旋转频率低的维度，那么即便 $m$ 和 $n$ 相距很远，旋转后的向量依然能保持较高的相似度 相对位置的含义：由于点积结果严格遵循 $\\cos((m-n)\\theta)$，模型在训练过程中会意识到：“当这个维度的点积达到某个值时，意味着这两个词距离是 $k$“ RoPE对于长距离间的点积，距离越长，结果越小： 参考链接：\nROFORMER: ENHANCED TRANSFORMER WITH ROTARY POSITION EMBEDDING 归一化 Pre-Norm RMSNorm 网络结构 MoE Linear Transformers",
    "description": "经典结构 参考链接：\nAttention Is All You Need 进展 注意力机制 MHA、MQA、GQA MHA（multi-head attention）：多头注意力，每个head维护自己的KV，在KV-Cache中占用显存很大 MQA（multi-query attention）：所有head共享KV，只有Q会被切分为多份，KV-Cache显存占用减小head-size倍 GQA（Grouped-Query Attention）：在MHA和MQA（效果和缓存）之间折中，KV分为若干份 参考链接：",
    "tags": [],
    "title": "Transformer学习手册",
    "uri": "/hugo-blog/blogs/transformer%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "字节跳动-懂车帝（2023.11-2025.4） 懂车帝AI助手懂咔咔项目 构建具有专业汽车知识及工具辅助功能的多轮对话AI助手 多轮对话Query改写与扩展以及实体（车系/车款）识别，改写准确率达91%，车系识别f1指标达91%,车款识别f1指标达82%，配合召回侧增强召回内容准确性和丰富性 构建多轮对话意图需求一二级分类体系，基于大模型prompt打标构建训练数据集，分类准确率达89.2%，拒答率下降77%，业务需求支持选车卡片等多种工具辅助功能 针对特定选车场景，设计构建并迭代多轮Memory模块，实现选车标签识别能力，模型线上准确率达83%，支持新车和二手车选车业务多轮对话能力接入 在多轮对话最终回复效果上与标注团队精细打磨，并进行汽车领域知识和回复样式风格模型微调，在与Base模型的汽车领域多轮对话SBS评估中整体正向6% 背景：\n懂车帝C端大模型尝试，在app中加入口，解决用户汽车领域问题，在app中多个页面连接大模型回复功能 大模型基座训练\u0026应用，我们负责应用，应用大模型sft训练及基于rag的整体对话链路的算法部分 技术细节：\n由于首字延迟问题，希望全链路除了回复模型采用参数较大的模型（100B左右），其他均采用bert或者lite版本的模型（13B） 多轮改写： 主要解决指代消解、主题和属性省略、以及表述标准化（价格/多少钱/贵不贵）等问题，改写query给下一步做rag召回使用 基于GPT4构造多轮对话及改写query，改写query（主query）拆分多个子query 主query包含完整信息，走意图判断；如果需要，全部query同时并发召回 改写准确率91%，主query和子query拆分都正确 实体识别： 最终用于品牌、车系、车款识别，替代原本懂车帝的da ner使用bert+global pointer，识别start和end，处理一下嵌套 识别出实体后，做检索召回，属性包括年款（25款、新款）、车类型（自动、手动）等，然后根据字短匹配规则返回，整体车系f1在90，车款f1在78左右 意图识别： router的概念，对不同意图的回复策略不同 比如查参配信息（价格、轴距）就更看重事实性，直接查参数库 对比类，就需要识别多个车款，然后进行全方位的对比 泛知识类，就依赖文章内容做rag 对话类，不用走rag，可以响应速度更快 某些类别也是一些前端样式的触发器，比如卡片、新闻链接、创作者 基于大模型prompt生成百万量级训练数据，训练bert prompt分为粗筛和精筛：粗筛用一个prompt找所有类别，精筛对每一个类别写一个prompt，在测试集上迭代，88左右的整体f1 memory模块： 基于选车类别query才生效，存选车意图memory，其他类型对话只走working memory（最大32轮对话，不压缩） memory以json字典方式保存，在一个session中存在，不跨session，支持增删改操作 包含固定几个字段，比如品牌，类型，用车需求等，字段内容基于用户app原始选车字段，保持尽可能一致，可以复用app选车逻辑 训练一个lite模型基于对话内容更新memory字典，数据基于线上app选车日志用大模型生成 基于更新的memory字典识别具体字段内容，走选车逻辑（召回同学负责），出选车卡片 模型sft 基本只做格式（回复长短）、风格化（比如对车辆介绍的分点）以及对召回内容权威性/时效性判断方面上的sft，保证以markdown格式输出一些内容 拒绝采样（人工标注负例）的方法来筛选训练数据 追问使用lite模型直接生成三个下一步可能的问题 按意图类别统计了用户下一个问题的意图分布，基于这个分布生成的问题 后期有专门同学做召回排序了，但是不清楚具体细节了 评估\u0026指标： 每周都会有标注同学基于线上结果和豆包进行整体sbs打分，同时在事实性（改写query/召回/幻觉/模型选择错误等）、相关性、用户体验等方面进行标注，每周50q，24年中达到6%正向 线上dau从一开始10w，涨到50w左右，但是轮次很低，主要由于入口在搜索的占比不小，年中后开始减少投入 懂车帝用户增长Push策略 基于uplift的付费push策略 通过Uplift模型建模用户对Push条数敏感度，基于Treatment条数设计Push规划策略，在Vivo品牌上验证带来6.77%DAU增长，较大提升广告与线索收益\n背景：\nc端大模型业务收缩，转入用户增长团队，做push push的业务知识 不同品牌发送条数不同，发送时间间隔不同（半小时/一小时） push主要有条数模型和时机模型来控制 通过uplift值判断每个用户的最优规划条数 通过用户每个小时的点击率，规划最优发送时间表（哪个小时发） vivo业务有运营付费条数（年包）：单设备发送2条免费后，可以继续发，点击后付费，需要有个策略，在预算下怎么发能收益最大（dau） 技术细节：\n训练uplift模型，判断用户对发送条数的敏感度，即多少条后用户成为dau 样本收集：开样本收集实验，每个用户选择一种条数，固定发 模型为s-learner，基础用户特征+treatment（条数）作为输入，二分类问题 离线规划： uplift模型预估每个用户不同条数dau分数 时机模型计算每个用户不同条数下的规划，从而确定付费预估（除前2条外的点击率和） 带约束规划问题求解： 约束为总预算（硬性要求）和人均条数（自驱要求） 最大化dau 得到规划结果后，写入hive表，再通过kafka写入设备正排，第二天按规划push 指标：\npush拉活率+4%，dau+0.7% vivo push拉活率+40%，dau+6.8% 设备点击率、人均发送、到达、点击均正向 推全日均收益19000元 腾讯-腾讯视频（2019.9-2023.5） 腾讯视频标准化 视频标签识别 从零开始构建融合基于Bert模型多分类和基于向量召回+倒排检索结合相关性两套策略的标签识别系统，并进一步利用标签相互关系及层级关系提升识别效果。在千级别标签体系中，周均f1指标71%提升至75%，服务腾讯视频下游推荐搜索业务场景\n背景：\n腾讯视频每天几十万入池视频，对于低等级视频需要自动打标，需要视频标签识别系统 技术细节：\n整体方案：end2end+标签召回打分 end2end： 模型结构：bert底座，与标签特征交互，n个head的二分类任务 特征：包括title、描述、视频向量特征、asr/ocr、创作者特征等 加入多目标（视频分类）：分类label有ground truth作为训练，线上预测没有 模型结构加入一二级标签分类预测（每个标签有自己的一二级分类），希望预测一级和二级尽量吻合，最终loss加权和作为整体loss 标签召回打分： 目的：充分利用标签信息（文本、层级关系、向量等） 三路召回：文本召回（es）、创作者召回、视频向量召回，top20召回覆盖率96% 相关性模型：bert底座，二分类任务 两路分数加权融合最终识别结果 指标： f1 60-\u003e75 腾讯视频搜索Sug场景及社区弹幕场景优化 搜索Sug场景 在原有DNN排序模型优化的基础上添加后验条件概率召回融合模块，首次为该场景排序模型设计并引入用户画像和相关匹配特征，实现个性化候选展示。使Sug模块点击率提升8.7%, 平均点击位置减少16.9%，大幅提升用户搜索效率\n背景：\n转入搜索团队，sug搜索入口占比超50%，很重要 负责sug模块业务指标迭代 技术细节：\n整体逻辑是召回+排序 召回： 包括精品doc、人名、热榜、运营、querylog 召回逻辑 原始的sug召回引擎，可以看成是es逻辑，前缀+关键词等 加入后验统计策略召回，基于时间衰减 querylog天级别臆造、qc流程，语义重复很难处理（xxx中文完整版 和 xxx中文版） 排序 日志拼接特征，小时级更新sug rank模型 特征重要度分析和整理 曝光位置作为训练特征不合适，下线后有线上收益 原因：sug固定窗口且严格排序，推荐场景滑动窗口且多样性打散 特征除了query侧、doc侧外，加入用户画像特征， 用户侧（用户时长、品类、基础画像、行为聚类） 用户和doc的match（标签相关） 后处理： - 数据源之间的归一化、去重逻辑 - 运营干预逻辑，黑白名单 指标：\nsug点击率：相对提升8.7% 平均点击位置：相对降低16.9% 社区弹幕场景 为腾讯视频弹幕排序打通离线和在线特征链路，引入DNN模型排序能力，结合弹幕前端样式优化排序，弹幕互动率提升60%、人均点赞次数提升77%，点赞渗透率达到业界领先\n背景：\n从统计排序到支持DNN模型排序，打通链路 前端样式主要是弹幕样式的一些工程策略优化 文本生成能力探索及AIGC应用落地 视频标题风格改写 在Seq2Seq结构基础上探索多种实体词拷贝机制，对视频标题进行风格改写。通过单边预训练、BPE分词、引入标签特征等手段优化指标，人工采纳率提升至89%，大幅节省标准化成本\n背景：视频标题风格改写，把教学类视频的标题进行改写，提升采纳率，降低人工成本\n技术细节：\nseq2seq模型，baseline版本容易出现实体词缺失 bpe分词解决低频实体词词表unk问题 bpe逻辑：从单char开始，不断按统计概率把子词连接为新子词，直到词表达到指定大小 seq2seq预训练： encoder引入视频标题分类任务 decoder引入mlm任务（masked lm），随机和实体词掩码 seq2seq训练： encoder引入标签特征，方便模型快速找到相关实体 高频句式loss权重衰减，基于beam search的topN+策略筛选，防止多样性缺失 模型结构尝试 copynet：每个token判断是应该拷贝还是生成，拷贝就从source里选，然后把两个概率（某个词的拷贝概率和生成概率）相加 pointer generator：类似，利用attention来完成拷贝词的选择 指标\n评测集1000条 线上日均6000+视频，人工采纳率73到89，节省20%成本 对话模型定制 收集业务方人设语料，通过prompt生成风格化对话语料，基于DeepSpeed框架进行10B量级对话模型人设与风格的Post-Training\n背景：游戏业务方希望对话语料风格化，基于prompt构造风格化对话语料，进行sft训练\n搜狗-输入法（2016.9-2019.9） 输入法用户词预测功能 词预测 优化LSTM/GRU及变种RNN深度语言模型指标及推断速度，为搜狗输入法首次提供线上基于深度模型的用户词预测功能，线上服务点击率提升1.2%\n输入法智能回复(句联想)功能 句联想 通过搜狗对话语料构建高相关性多轮对话训练数据，基于Seq2Seq模型并构造基于损失的意图多样性模块，从零实现输入法智能回复功能, 服务使用率从4.7%提升至10.3%",
    "description": "字节跳动-懂车帝（2023.11-2025.4） 懂车帝AI助手懂咔咔项目 构建具有专业汽车知识及工具辅助功能的多轮对话AI助手 多轮对话Query改写与扩展以及实体（车系/车款）识别，改写准确率达91%，车系识别f1指标达91%,车款识别f1指标达82%，配合召回侧增强召回内容准确性和丰富性 构建多轮对话意图需求一二级分类体系，基于大模型prompt打标构建训练数据集，分类准确率达89.2%，拒答率下降77%，业务需求支持选车卡片等多种工具辅助功能 针对特定选车场景，设计构建并迭代多轮Memory模块，实现选车标签识别能力，模型线上准确率达83%，支持新车和二手车选车业务多轮对话能力接入 在多轮对话最终回复效果上与标注团队精细打磨，并进行汽车领域知识和回复样式风格模型微调，在与Base模型的汽车领域多轮对话SBS评估中整体正向6% 背景：\n懂车帝C端大模型尝试，在app中加入口，解决用户汽车领域问题，在app中多个页面连接大模型回复功能 大模型基座训练\u0026应用，我们负责应用，应用大模型sft训练及基于rag的整体对话链路的算法部分 技术细节：\n由于首字延迟问题，希望全链路除了回复模型采用参数较大的模型（100B左右），其他均采用bert或者lite版本的模型（13B） 多轮改写： 主要解决指代消解、主题和属性省略、以及表述标准化（价格/多少钱/贵不贵）等问题，改写query给下一步做rag召回使用 基于GPT4构造多轮对话及改写query，改写query（主query）拆分多个子query 主query包含完整信息，走意图判断；如果需要，全部query同时并发召回 改写准确率91%，主query和子query拆分都正确 实体识别： 最终用于品牌、车系、车款识别，替代原本懂车帝的da ner使用bert+global pointer，识别start和end，处理一下嵌套 识别出实体后，做检索召回，属性包括年款（25款、新款）、车类型（自动、手动）等，然后根据字短匹配规则返回，整体车系f1在90，车款f1在78左右 意图识别： router的概念，对不同意图的回复策略不同 比如查参配信息（价格、轴距）就更看重事实性，直接查参数库 对比类，就需要识别多个车款，然后进行全方位的对比 泛知识类，就依赖文章内容做rag 对话类，不用走rag，可以响应速度更快 某些类别也是一些前端样式的触发器，比如卡片、新闻链接、创作者 基于大模型prompt生成百万量级训练数据，训练bert prompt分为粗筛和精筛：粗筛用一个prompt找所有类别，精筛对每一个类别写一个prompt，在测试集上迭代，88左右的整体f1 memory模块： 基于选车类别query才生效，存选车意图memory，其他类型对话只走working memory（最大32轮对话，不压缩） memory以json字典方式保存，在一个session中存在，不跨session，支持增删改操作 包含固定几个字段，比如品牌，类型，用车需求等，字段内容基于用户app原始选车字段，保持尽可能一致，可以复用app选车逻辑 训练一个lite模型基于对话内容更新memory字典，数据基于线上app选车日志用大模型生成 基于更新的memory字典识别具体字段内容，走选车逻辑（召回同学负责），出选车卡片 模型sft 基本只做格式（回复长短）、风格化（比如对车辆介绍的分点）以及对召回内容权威性/时效性判断方面上的sft，保证以markdown格式输出一些内容 拒绝采样（人工标注负例）的方法来筛选训练数据 追问使用lite模型直接生成三个下一步可能的问题 按意图类别统计了用户下一个问题的意图分布，基于这个分布生成的问题 后期有专门同学做召回排序了，但是不清楚具体细节了 评估\u0026指标： 每周都会有标注同学基于线上结果和豆包进行整体sbs打分，同时在事实性（改写query/召回/幻觉/模型选择错误等）、相关性、用户体验等方面进行标注，每周50q，24年中达到6%正向 线上dau从一开始10w，涨到50w左右，但是轮次很低，主要由于入口在搜索的占比不小，年中后开始减少投入 懂车帝用户增长Push策略 基于uplift的付费push策略 通过Uplift模型建模用户对Push条数敏感度，基于Treatment条数设计Push规划策略，在Vivo品牌上验证带来6.77%DAU增长，较大提升广告与线索收益",
    "tags": [],
    "title": "简历项目Q\u0026A",
    "uri": "/hugo-blog/blogs/%E7%AE%80%E5%8E%86%E9%A1%B9%E7%9B%AEqa/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Latency KV Cache 在 Transformer 的 Self-Attention 层中，计算公式为： $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$ 其中：\nQ (Query)：代表当前的字 K (Key) \u0026 V (Value)：代表上下文的信息 由于前面的计算结果已经算完，可以直接存在显存里，方便解码时快速使用：\nPrefill（预填充）阶段：模型处理你输入的 Prompt，计算出所有已存在字符的K和V，并把它们存在显存里 Decoding（解码）阶段：每生成一个新词，模型只需要计算这个新词的K和V，然后直接从显存里读取之前存好的K和V进行拼接，计算Attention KV Cache让生成每个新词的时间复杂度从 $O(n^2)$ 降低到了 $O(n)$，但是显存占用很高，并随着序列长度增长而线性增加，另外传统的显存分配方式会导致很多浪费（类似磁盘碎片），这也是为什么后来出现了vLLM (PagedAttention)技术来优化它\nPD分离 由于Prefill阶段是计算密集型，而Decoding阶段时访存密集型，因此如果用同一种类型的显卡进行两个阶段，对导致在Prefill阶段对显存的浪费和在Decoding阶段对计算能力的浪费，因此将两阶段分离，分开部署在不同的显卡上：\nPrefill节点算力强，显存小，保证首字延迟（TTFT）较低，不负责字间延迟（TPOT） Decode节点算力弱，显存大，可以存储长上文的kv cache PD分离计算流程：\n将prompt发送到Prefill节点，计算所有的kv cache 缓存迁移：将kv cache的结果传送到Decoding节点上 在Decoding节点进行逐字生成 投机解码 Eagle 显存 MQA GQA PagedAttention Continuous Batching Quantization 训推一致 Routing Replay MoE模型中的Routing Replay技巧\n参考链接：\nWhen Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch",
    "description": "Latency KV Cache 在 Transformer 的 Self-Attention 层中，计算公式为： $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$ 其中：\nQ (Query)：代表当前的字 K (Key) \u0026 V (Value)：代表上下文的信息 由于前面的计算结果已经算完，可以直接存在显存里，方便解码时快速使用：\nPrefill（预填充）阶段：模型处理你输入的 Prompt，计算出所有已存在字符的K和V，并把它们存在显存里 Decoding（解码）阶段：每生成一个新词，模型只需要计算这个新词的K和V，然后直接从显存里读取之前存好的K和V进行拼接，计算Attention KV Cache让生成每个新词的时间复杂度从 $O(n^2)$ 降低到了 $O(n)$，但是显存占用很高，并随着序列长度增长而线性增加，另外传统的显存分配方式会导致很多浪费（类似磁盘碎片），这也是为什么后来出现了vLLM (PagedAttention)技术来优化它\nPD分离 由于Prefill阶段是计算密集型，而Decoding阶段时访存密集型，因此如果用同一种类型的显卡进行两个阶段，对导致在Prefill阶段对显存的浪费和在Decoding阶段对计算能力的浪费，因此将两阶段分离，分开部署在不同的显卡上：",
    "tags": [
      "技术笔记"
    ],
    "title": "LLM推理技术学习手册",
    "uri": "/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM训练技术学习手册 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\\mu$ 和 $\\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好： GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题 DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题 RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习 PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率 待办 跟进大模型强化学习技术 强化学习技术实战",
    "description": "总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM训练技术学习手册 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\\mu$ 和 $\\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好： GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题 DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题 RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习 PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率 待办 跟进大模型强化学习技术 强化学习技术实战",
    "tags": [
      "周记"
    ],
    "title": "Week12 强化学习技术跟进",
    "uri": "/hugo-blog/weekly/week12/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction \u0026 Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation \u0026 Exploration 参考链接:\nDeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：\nOnline Network（在线网络 / 评估网络）：负责产生即时的 $Q(s, a)$，并进行反向传播更新参数，每一步都在更新 Target Network（目标网络）：专门用来计算目标值（TD Target）中的 $Q(s’, a’)$ 部分，不经过梯度更新，而是每隔一段时间（比如1000步），从Online Network拷贝参数 $Q_{online}(s, a) \\leftarrow R + \\gamma Q_{target}(s’, \\arg\\max_{a’} Q_{target}(s’, a’))$\n参考链接：\nHuman-level control through deep reinforcement learning Double DQN DQN采用Bootstrapping估计Q，而Bootstrapping与Q-learning中的最大化操作会导致对Q的高估：\n在训练初期或由于函数近似的不精确，预估通常包含噪声，而max操作会自动选择噪声并将包含噪声的Q+noise作为更新目标 Bootstrapping用高估的目标去估计下一个目标，导致高估被传递并放大 高估问题对当前state的不同action是不均匀的，因为每次数据采样到就会被高估一次，采样多的 $（s_t, a_t）$ 就会被高估的严重，导致最终最优policy有问题\n为了缓解DQN的高估问题，Double DQN将选action和计算action的value拆分为两部分，分别交给Online Network和Target Network完成：\nOnline Network：负责选动作 Target Network：负责估计动作的value 这样即便由于高估选择了一个错误的动作，在计算value的时候，由于是另一个网络，仍然会给他较低的分数，防止高估传递下去\n$Q_{online}(s, a) \\leftarrow R + \\gamma Q_{target}(s’, \\arg\\max_{a’} Q_{online}(s’, a’))$\n参考链接：\nDeep Reinforcement Learning with Double Q-learning Dueling Network 在Dueling Network中，DQN中的Q被分成了两部分，$Q(s, a) = V(s) + A(s, a)$：\n状态价值函数 $V(s)$：评估当前state本身有多好 优势函数 $A(s, a)$：评估在当前state下，选择某个action比平均水平好多少 为了让优势函数A的均值为0，强制V分支去捕捉状态的平均价值，采用对A去中心化的方法： $Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{n} \\sum_{a’} A(s, a’) \\right)$\n参考链接：\nDueling Network Architectures for Deep Reinforcement Learning Experience Replay 经验回放有两个主要优点：\n避免训练数据($s_t, a_t, r_t, s_{t+1}$)的浪费，可以重复使用 避免训练过程中采样的同分布，即相邻序列在训练过程中相邻，防止过拟合 经验回放引入了off-policy，即训练数据的策略（旧）已经不是当前学习的策略（新），因此对于policy-based的算法，需要重要性采样（Importance Sampling）进行修正 Policy-Based在经验回放时使用重要性采样修正的必要性\nPrioritized Experience Replay 用非均匀采样代替均匀采样，对于TD error（$\\delta_t$）较大的样本采样权重更大：\n基于TD error：$p_t \\propto |\\delta_t| + \\epsilon$ 基于TD error的排序：$p_t \\propto \\frac{1}{\\text{rank}(t)}$ Prioritized Experience Replay中的采样效率问题 由于采样权重导致数据分布发生变化（参考off policy），需要对学习率进行调整，即采样权重大的样本，学习率调小：$w_i = \\left( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\right)^\\beta$，其中在均匀采样中 $P(i)=\\frac{1}{N}$，不影响学习率 DQN在PER时使用重要性采样修正的必要性\n参考链接：\nPRIORITIZED EXPERIENCE REPLAY Policy-based Actor-Critic TRPO TRPO的背景在于RL中梯度更新的难度远比监督学习大，按监督学习的learning rate方案极不稳定： RL与监督学习在SGD上的差异\n置信域算法（Trust Region Methods）分为两个阶段：\nApproximation：在当前点 $x_k$ 附近，用一个简单的数学模型来代替复杂的原始函数 $f(x)$ Maximization：在信任范围内（满足单调性），找到能让模型下降（或增益极大化）的最佳位移 $p$ 置信域算法通过自适应方法来确定置信域区间，从而在置信域区间进行迭代求解原函数最小值： $$\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}$$ 其中：\n当 $\\rho_k$ 接近1时，近似非常准确，优化阶段可以更激进，增大 $\\Delta_k$ 当 $\\rho_k$ 接近0或负数时，近似完全失效，缩小 $\\Delta_k$，重新进行近似 当 $\\rho_k$ 适中时，近似尚可，保持现状 什么是置信域算法（Trust Region Methods）？\nTRPO不直接约束参数空间的步长，而是约束策略空间的距离（新旧policy之间的KL 散度），确保了新旧policy之间的差距在“置信域”内，尽可能取最大的有效步长，避免了手动设置学习率，使训练收敛更稳定： $$\\max_{\\theta} E_{s \\sim \\rho_{\\theta_{old}}, a \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\theta_{old}}(s,a) \\right]$$\n$$\\text{subject to } E_{s \\sim \\rho_{\\theta_{old}}} [D_{KL}(\\pi_{\\theta_{old}}(\\cdot|s) || \\pi_{\\theta}(\\cdot|s))] \\le \\delta$$ 这转化为一个约束最优化问题的求解，使用泰勒展开：\n对目标函数进行一阶展开（得到梯度 $g$） 对约束条件进行二阶展开（得到费雪信息矩阵 $H$，也叫Hessian矩阵） 最后推导出来的参数更新位置公式为： $$\\Delta \\theta \\approx \\sqrt{\\frac{2\\delta}{g^T H^{-1} g}} H^{-1} g$$ 直接把 $\\theta$ 推到了那个理论上在约束范围内最完美的位置 参考链接：\nTrust Region Policy Optimization # TRPO 置信域策略优化 (Trust Region Policy Optimization) PPO PPO的背景在于TRPO中 $H^{-1}g$ 计算量巨大，于是简化约束条件KL散度，而采用截断的方式来控制新旧策略的差异，假设新旧策略差异为： $$r_t(\\theta) = \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}$$ 截断损失函数为： $$L^{CLIP} = \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right)$$ 假设 $\\epsilon=0.2$，有以下几种情况：\n$r_t(\\theta) \u003c 0.8$：新策略action概率变得过小 如果 $\\hat{A}_t \u003e 0$（action是好动作）：min取真实$r_t(\\theta)$，即修正错误（变大）不设限 如果 $\\hat{A}_t \u003c 0$（action是坏动作）：min取0.8，梯度停止更新，即不会继续让action概率更小 $r_t(\\theta) \u003e 1.2$：新策略action概率变得过大 如果 $\\hat{A}_t \u003e 0$（action是好动作）：min取1.2，梯度停止更新，即不会继续让action概率更大 如果 $\\hat{A}_t \u003c 0$（action是坏动作）：min取真实$r_t(\\theta)$，即修正错误（变小）不设限 $0.8 \u003c r_t(\\theta) \u003c 1.2$：新策略action小幅变化，clip不生效，取真实$r_t(\\theta)$ 如果 $\\hat{A}_t \u003e 0$（action是好动作）：继续梯度更新，让action概率变大 如果 $\\hat{A}_t \u003c 0$（action是坏动作）：继续梯度更新，让action概率变小 PPO两大工程优势：\n防止策略崩溃：即使你把 Learning Rate 设得稍微大了一点，clip也会挡住那些过度的更新 数据高效利用：只要 $r_t$ 还没被clip，这批数据就可以反复用来跑好几次 SGD PPO中采样数据被高效利用的原因 PPO中Clip操作对Bias和Variance的Trade-Off\n参考链接：\nProximal Policy Optimization Algorithms DDPG 离散控制与连续控制：\n离散动作空间只包含有限个动作，比如grid中的四个方向的运动 连续动作空间的动作是连续的，比如机械手转动的角度，属于一个连续范围 对连续动作离散化可以近似解决，但是会有维度爆炸的问题，所以只适用于维度小的问题 参考链接：\n# 离散控制与连续控制 (连续控制 1/3)) 由于连续动作空间中，action是一个数值变量：\n如果还按离散控制的方式，采样来计算梯度，会发现由于维度爆炸，采样数据稀疏导致极不稳定 action是数值变量意味着可以求导，因此可以通过链式法则，直接对 $\\theta$ 求导： $$\\mathbf{g} = \\frac{\\partial q(s, \\pi(s; \\boldsymbol{\\theta}); \\mathbf{w})}{\\partial \\boldsymbol{\\theta}} = \\frac{\\partial a}{\\partial \\boldsymbol{\\theta}} \\cdot \\frac{\\partial q(s, a; \\mathbf{w})}{\\partial a}$$ DPG中由于critic和actor的梯度更新都依赖于critic的 $Q$ ，会导致不稳定相比离散控制更强烈：\ntarget network：引入target actor和target critic 参数软更新： target网络的参数 $w^-$ 以极小的比例 $\\tau$（如 0.001）缓慢跟随主网络 参考链接：\nDeterministic Policy Gradient Algorithms CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING # 确定策略梯度 Deterministic Policy Gradient, DPG (连续控制 2/3) SAC SAC通过随机策略来解决连续控制问题，预测action的均值 $\\mu$ 和方差 $\\sigma$，将确定性action转化为高斯分布，从而最大化动作的累积收益\n在 SAC 的目标函数中，除了奖励 $r$ 之外，还最大化熵 ($H$)： $$J(\\pi) = \\sum_{t=0}^{T} E_{(s_t, a_t) \\sim \\rho_\\pi} [r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t))]$$\n防止模型过早陷入局部最优解，鼓励去尝试不同的动作 随机策略可以学到多种种同样好的解法，如果环境发生轻微扰动，表现得更稳健 连续控制中使用 $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}{a \\sim \\pi{\\theta}} [ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\cdot Q(s, a) ]$ 求解策略梯度方差极大，因此SAC采用了和DDPG同样的确定性梯度的路径（链式法则）；由于action不是数值变量，而是从分布中采样得到的，因此需要重参数化的技巧，将随机采样的过程转化为一个确定的函数： $$a = f_{\\theta}(s, \\epsilon) = \\mu_{\\theta}(s) + \\sigma_{\\theta}(s) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\nSAC单峰假设遇到多峰情况时如何解决？\n参考链接：\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor # 随机策略做连续控制 (连续控制 3/3) Exploitation \u0026 Exploration #todo 看下面这篇blog\n参考链接：\n# Exploration Strategies in Deep Reinforcement Learning 相关领域 #todo 动手学强化学习这些章节\nSparse Rewards 真实场景中奖励过于稀疏，学习很难，解决方向有以下几个：\n设计奖励： 在初期对Q建模不够好或者未来价值衰减过大时，需要人为设计奖励来鼓励进行特定action 人为设计的奖励需要领域知识，使策略与目标更近 好奇心驱动的奖励： 基于当前状态和动作，对下一次状态进行预测，预测越不准奖励越大 为防止预测与动作无关（单纯环境的不可预测），另外对状态进行特征提取，并基于特征学习状态间的动作，特征网络学习好后，特征只与动作相关 课程学习： 通过安排学习的难度从易到难，需要人为设计 通过逆课程学习，从目标状态倒退找到合适学习的状态，避免人为设计 分层强化学习： 将一个复杂强化学习问题分解成多个子问题，每个子问题（包含高层次策略和低层次策略），多agent采用合作的模式，分别处理不同层次的策略 参考链接：\n蘑菇书EasyRL 第10章 稀疏奖励 Imitation Learning 在模仿学习中，有一些专家的示范，智能体也可以与环境交互，但它无法从环境里得到任何的奖励，它只能通过专家的示范来学习动作的好坏\nBehavior Cloning 行为克隆可以看成是监督学习的问题，给定一组专家数据（ $s_t$ 到 $a_t$ 的映射），agent进行拟合。\n专家数据覆盖不全，out-of-distribution的情况agent不知道怎么处理：\n数据集聚合（DAgger）：通过agent运行过程中遇到危险情况时，专家给出的action建议，加入数据中，进一步训练agent 行为克隆的其他问题：\n行为克隆会学习专家全部action，即便有些action是无意义或者低效的 任何时刻action偏差了一点，会导致后续状态的偏差，慢慢累积，最终会相差很多 Inverse RL 通过专家数据以及环境，反向找到奖励函数，有了奖励函数后，我们就可以用强化学习的方法来处理问题，这个过程称为逆强化学习\n逆强化学习流程：\n观察专家：收集大量人类专家的驾驶轨迹（状态 $s$ 和动作 $a$ 的序列） 假设与反推：不断尝试建立一个奖励函数 $R(s, a)$，使得专家在这个函数下看起来是最优的 验证与循环：用奖励函数去跑一遍，如果结果和专家不像，回来修改奖励函数，直到一致 逆强化学习问题：\n计算量巨大：每推测一次奖励函数，都要完整地进行一次强化学习训练来验证，非常耗时 专家未必完美：人类专家也非最优，把这些“坏习惯”背后的逻辑也学走也不好 多解性：专家的一个动作可能有多种解释 参考链接：\n蘑菇书EasyRL 第11章 模仿学习 Model Predictive Control Offline RL Multi-agent RL 多agent之间的关系分为以下几种：\n完全合作（fully cooperative）：多agent目标相同 完全竞争（fully competitive）：零和博弈 合作/竞争混合（mixed cooperative \u0026 competitive）：自己团队内部为合作，对方团队为竞争 利己主义（self-interested）：只考虑自己，不在乎对方 多agent策略收敛：\n纳什均衡时策略收敛，即当一个agent改变策略而其他agent不变时，他的return不会变好 除非所有agent彼此独立，才能用单agent方法求解最优策略 单agent方法有可能无法收敛，因为改变自己参数时，会改变其他agent的目标 多agent（合作）需要通过通信共享信息从而达到最优策略的收敛 多agent的通信方式与训练：\n完全去中心化（fully decentralized）：多agent不通信 agent视角与单agent完全一样，最终可能无法收敛 完全中心化（fully centralized）：agent将信息发送给controller，controller为所有agent决策 由于controller的决策需要用到所有agent的观测，因此不能在agent上独立执行 可以把多agent理解为一个大的agent，汇合了所有agent的state、action和reward 同步比较耗时，很难做到实时性 中心训练、去中心执行：训练时使用controller，执行时不使用 controller根据全部的信息为每个agent训练专属的critic网络 每个agent根据controller的critic训练自己的actor，然后在执行中使用 参考链接：\n# 多智能体强化学习(1/2)：基本概念 Multi-Agent Reinforcement Learning # 多智能体强化学习(2/2)：三种架构 Multi-Agent Reinforcement Learning 应用 Gymnasium 官网链接\nTianShou Sim2Real AlphaGo 参考链接：\nMastering the game of Go with deep neural networks and tree search AlphaStar 参考链接：\n蘑菇书EasyRL 第13章 AlphaStar 论文解读 AlphaZero MuZero 参考链接：\n# MuZero: Mastering Go, chess, shogi and Atari without rules Dreamer 参考链接：\n# Introducing Dreamer: Scalable Reinforcement Learning Using World Models # Mastering Atari with Discrete World Models # Mastering Diverse Control Tasks through World Models # Training Agents Inside of Scalable World Models 参考链接：\n# The FASTEST introduction to Reinforcement Learning on the internet 动手学强化学习 # 深度强化学习基础【王树森】 蘑菇书EasyRL OpenAI Spinning Up",
    "description": "基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction \u0026 Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation \u0026 Exploration 参考链接:\nDeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：",
    "tags": [
      "技术笔记"
    ],
    "title": "Reinforcement Learning学习手册",
    "uri": "/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 强化学习基础知识 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Prediction DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Control DeepMind x UCL Introduction to RL 2015 课程笔记-Value Function Approximation DeepMind x UCL Introduction to RL 2015 课程笔记-Policy Gradient DeepMind x UCL Introduction to RL 2015 课程笔记-Integrating Learning and Planning DeepMind x UCL Introduction to RL 2015 课程笔记-Exploration and Exploitation 知识 当我们不知道环境的具体信息（model）时，如何进行策略的评估（prediction）和最优策略的选择（control），称为model-free RL model-free主要基于采样，即在环境中通过采样拿到真实reward，来迭代value function，从而评价state/action的好坏 采样完整序列的reward的方式称为Monte-Carlo采样，采样下一步然后基于下一步的value function进行bootstrapping的方式称为TD learning；进一步地，通过TD($\\lambda$)可以在MC和TD之间平滑，效果一般更优 选择最优策略的方式一般是基于评估后通过greedy或者$\\epsilon-greedy$的方式选取action；如果行为策略和目标策略相同，称为on-policy，如果不同，称为off-policy TD learning + $\\epsilon-greedy$ = SARSA（on-policy） TD learning + max = Q-learning（off-policy） 简单问题可以通过查表完成，但是真实环境的RL问题状态空间通常很大，需要使用Value Function Approximator来表示，一般可以理解为神经网络的近似，比如DQN policy gradient的方法直接计算最优策略，避免了value-based的max操作，具有更好的收敛性质 可以通过采样reward结合策略梯度的方式，计算最优策略对应的action的分布 actor-critic通过同时优化evaluation（critic）和improvement（actor）减小variance 另外我们可以先通过采样求解model，把问题转化为model-based RL，再求解策略 同时结合model-based以及model-free来充分的压榨样本数据，比如Dyna-Q 基于价值函数进行动作采样，基于model生成sub MDP，再通过model-free求解，比如MCTS 利用和探索是希望我们在”最大化即时收益“和”获取新信息“之间做出权衡，人们通过多臂赌博机问题发展出很多策略：包括UCB和Thompson Sampling，且这些策略均可以在MDP上应用 待办 强化学习基础知识 跟进大模型强化学习技术",
    "description": "总结 强化学习基础知识 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Prediction DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Control DeepMind x UCL Introduction to RL 2015 课程笔记-Value Function Approximation DeepMind x UCL Introduction to RL 2015 课程笔记-Policy Gradient DeepMind x UCL Introduction to RL 2015 课程笔记-Integrating Learning and Planning DeepMind x UCL Introduction to RL 2015 课程笔记-Exploration and Exploitation 知识 当我们不知道环境的具体信息（model）时，如何进行策略的评估（prediction）和最优策略的选择（control），称为model-free RL model-free主要基于采样，即在环境中通过采样拿到真实reward，来迭代value function，从而评价state/action的好坏 采样完整序列的reward的方式称为Monte-Carlo采样，采样下一步然后基于下一步的value function进行bootstrapping的方式称为TD learning；进一步地，通过TD($\\lambda$)可以在MC和TD之间平滑，效果一般更优 选择最优策略的方式一般是基于评估后通过greedy或者$\\epsilon-greedy$的方式选取action；如果行为策略和目标策略相同，称为on-policy，如果不同，称为off-policy TD learning + $\\epsilon-greedy$ = SARSA（on-policy） TD learning + max = Q-learning（off-policy） 简单问题可以通过查表完成，但是真实环境的RL问题状态空间通常很大，需要使用Value Function Approximator来表示，一般可以理解为神经网络的近似，比如DQN policy gradient的方法直接计算最优策略，避免了value-based的max操作，具有更好的收敛性质 可以通过采样reward结合策略梯度的方式，计算最优策略对应的action的分布 actor-critic通过同时优化evaluation（critic）和improvement（actor）减小variance 另外我们可以先通过采样求解model，把问题转化为model-based RL，再求解策略 同时结合model-based以及model-free来充分的压榨样本数据，比如Dyna-Q 基于价值函数进行动作采样，基于model生成sub MDP，再通过model-free求解，比如MCTS 利用和探索是希望我们在”最大化即时收益“和”获取新信息“之间做出权衡，人们通过多臂赌博机问题发展出很多策略：包括UCB和Thompson Sampling，且这些策略均可以在MDP上应用 待办 强化学习基础知识 跟进大模型强化学习技术",
    "tags": [
      "周记"
    ],
    "title": "Week11 强化学习基础知识",
    "uri": "/hugo-blog/weekly/week11/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Intro 强化学习与机器学习的差别：\n没有supervisor，只有reward 反馈是滞后的，不是实时的 时间序列，每一个时间步不是i.i.d的 agent的行为会影响后续数据 reward是什么：\n一个标量的反馈信号 表示agent在t时刻的表现 agent的目标是最大化累积reward 序列决策是什么：\n目标是选择action，最大化累积reward action有长期的影响，因为reward是滞后的，意味着可能需要牺牲短期reward来获取长期reward 序列H包含observation、action、reward的循环：O1、A1、R1、O2、A2、R2、… state是什么：\nstate是序列的函数，即当前的状态包含了观察、行为、奖励序列的全部信息 environment state是指环境的内部状态表示，通常是agent不可见的；即使可见，也通常包含一些不相关的噪音 agent state是agent的内部状态表示，作为下一次action选择的输入，也可理解为RL算法的输入 markov state是指包含之前全部信息的当前状态，只依赖于当前状态，与之前状态独立 在完全可观测环境中，agent state等于environment state，构成MDP（markov decision process）；在部分可观测环境中，agent state不等于environment state，构成POMDP（partially observable markov decision process） agent agent的组成：\npolicy：agent的行为函数，是从state到action的map，可以是确定的，也可以是概率分布 value function：agent基于某个policy下，对于当前状态的未来累积价值评估 model主要有两种： transition model：agent基于当前状态和行为，对于环境下一时刻状态的预测 reward model：agent基于当前状态和行为，对于reward的预测 agent的分类：\nvalue-based：没有显式的policy，根据value function选择最大的action policy-based：没有显式的value function，通过尝试不同的policy来找到最好的policy（action） actor-critic：结合policy和value function，通过value function给出baseline，基于相对baseline的好坏来优化policy 基于是否包含model来分类，即是否对环境进行学习，是否预测下一时刻环境的状态 MDP RL中的MDP：\nMDP描述了RL问题的environment，当environment是完全可观测的。 几乎所有RL问题都可以形式化为MDP，部分可观测的问题可以转化为MDP Markov Process Markov Process通过状态和状态转移矩阵来描述： Markov Reward Process 如果加上reward，就可以得到一个带衰减的累积reward： Markov Reward Process中折扣因子(Discount Factor) 必要性\n而状态value function就是在某个状态下上述return的期望： Bellman Equation for MRPs 将value function分成当前reward和未来的衰减value两部分： 未来的状态由转移矩阵确定： 直接求解线性方程复杂度为 $O(n^3)$，因此只适合小规模MRP的求解； 对于大规模MRP，通常使用迭代求解的方式：\n动态规划 Monte-Carlo evaluation TD learning（Temporal-Difference） Markov Decision Process 在MRP的基础上加入decisions，得到MDP： Policy policy定义为在给定状态下选择action的概率分布： 通过policy可以完全表示agent的行为 MDP policy依赖于当前状态（markov性质） policy是静态的，即不随时间变化 Value Function 基于policy的value function看作是在policy下的value期望； 其中state value function可以看成是action value function在policy下的加权平均： Bellman Expectation Equation state/action value function也都可以转化为Bellman方程的形式： 而他们互相之间的转换为： 相应地，通过两次转换，可以得到state/action value function的迭代： Optimal Value Function 最优state/action value function分别是： 对于 $q_(s,a)$，因为知道了当前状态不同action的value，直接选择最大的action就可以了；但是如果知道了 $v_(s)$，即知道我下一步想去哪个state，但是由于不知道action对应的状态转移矩阵（一般情况），没办法知道要选择哪个action最终value最大\nOptimal Value Function中q和v的差别\n在任意state上实现optimal value function的policy，称为optimal policy 根据 $q_*(s,a)$ 选择action的policy即optimal policy\nBellman Optimality Equation state/action optimal value function表述为Bellman Optimality Equation的形式： Bellman Optimality Equation由于包含max操作，是非线性的方程，一般没有closed form solution，可以使用迭代的算法求解：\nValue Iteration Policy Iteration Q-learning Sarsa 什么时候MDP的bellman方程求解可以转化为MRP？\nPlanning by Dynamic Programming Markov Process满足动态规划的两个性质：\n最优子结构：最优解可以被分解为多个子问题 Bellman方程就是一种子问题的拆解表示 重叠子问题：子问题重复很多次，且能被缓存和复用 value function保存了信息且被复用 Policy Evaluation 对所有state的value设定初始值后，每一轮基于当前policy可达state的value加权求和，得到新一轮的value；经过足够多轮次迭代后，value逐渐收敛到当前policy的value真值：\nPolicy Iteration 基于原始 $\\pi$ 迭代计算所有state的value，收敛后得到 $v_\\pi(s)$：\n在某一个state下选择value最大的action： 在每一个state选择value最大的action，那么你就得到了一个在全局所有状态下都表现得更好的新策略 $\\pi^{\\prime}$，更接近 $\\pi^*$： 通过不断地：\n评估：根据当前policy计算全部状态的value 优化：根据上述value贪婪地选择动作 就可以不断地优化policy，直到最优policy ## GridWorld: Dynamic Programming Demo\nValue Iteration 我们是否需要value收敛到真值后再优化policy？\n可以选择value的 $\\epsilon$ 足够小的时候停止迭代value并优化 可以选择每k步优化 是否可以每步优化？ 每步评估进行一次优化（取max）：每次采用可达state中最大的value来计算当前state的新一轮value，通过这种方式不断更新value，最终得到每个状态的 $v^*(s)$，称为value iteration\nPolicy Evaluation、Policy Iteration和Value Iteration的对比 Model-Free Prediction Model-Free中的model指的是环境，即我们不知道环境如何基于我们的action进行状态转移，也不知道会得到一个怎么样的reward，是一个未知的MDP。\n对于一个未知的MDP的value function的估计，称为Model-Free Prediction\nMonte-Carlo Learning MC从完整的实际经验中学习，如果经验不完整，无法使用MC，因为MC要求最终的return 基于最简单的想法，即value就是实际经验的平均return MC Policy Evaluation分为两类： First-Visit：只统计轨迹中到达该state的第一次，即便同一条轨迹中多次到达该状态，是无偏估计，收敛稳健 Every-Visit：每次到达都记为一次，即一条轨迹贡献多个样本分，可以更充分利用数据 通过增量更新value function： 对于非静态系统，可以将增量的系数设为常数，可以更有效忘记旧的样本： Temporal-Difference Learning TD可以从不完整的实际经验中学习，而MC不行 通过bootstrapping在不确定的环境中边走边学： TD learning中的bootstrapping 相比于MC中的目标值最终return $G_t$，TD（以TD(0)为例）基于下一状态的return作为目标值： $V(S_t) \\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1}) - V(S_t))$ 其中： $R_{t+1}+\\gamma V(S_{t+1})$ 被称为TD target $\\delta_t = R_{t+1}+\\gamma V(S_{t+1}) - V(S_t)$ 被称为TD error TD和MC在bias/variance上的对比： TD target是有偏的，因为是基于一个有偏的 $V（S_{t+1}）$ 来估计的；而MC是无偏的，因为是真实样本的平均 TD的variance是小的，因为只包含当前的reward和对value的估计；而MC的variance是大的，因为MC中每一步的action和transition都引入variance TD对初始值敏感；而MC不敏感 TD learning相比MC learning效率高的原因 TD和MC在处理有限经验数据时的对比： MC 的目标是让价值函数 $V(s)$ 尽可能地贴近return，不考虑状态转移关系，只关注结果，可以理解为最小化均方误差（MSE） TD的目标是寻找一个最符合当前数据的MDP（状态转移和奖励），然后基于这个模型计算价值，可以理解为最大似然markov模型（MLE） 由于TD利用了Markov性质，在Markov环境中通常比MC更有效率，因为能够充分利用数据在状态之间传递信息 MC、TD和dynamic programming的更新对比： n-step TD learning，每次更新考虑n步，而不只是TD(0)的一步： 定义n步的TD target： 得到n步的TD learning： 这里以random walk为例，观察不同的n和alpha在10个episodes数据上的error： 曲线主要体现了bias/variance的trade-off：\n小n：variance小，bias大，因此较大的alpha更有利于快速优化 大n：variance大，bias小，因此较小的alpha更有利于稳定优化 中n：bias和variance合适，可以较快的收敛到更优解 对比online和offline：\nonline意味着每一步都更新：可以在较大的alpha达到最优解，因为它可以实时修正，所以敢于用更大的步长去快速吸收新信息 offline意味着batch更新：需要在较小的alpha达到最优解，因为一次性大量的更新会导致模型极其不稳定（因为这些更新都是基于旧的、可能错误的估计值同时发生的） 提升效果的手段：\n增加训练数据量，可以减小“小n”的bias，也可以减小“大n”的variance alpha如果固定，最终error会震荡，大n的震荡更明显，如果alpha衰减，最终error都会归零 TD($\\lambda$) 通过对不同step的TD进行加权平均，既利用了大步数带来的快速信息传播，又通过小步数维持了系统的稳定性；这个算法能够在一个统一的框架下，自动实现从 TD(0)到 MC的平滑过渡： TD($\\lambda$)对近期步数分配较高权重，对远期步数分配较低权重： 距离越近，因果关系越明确，因果关系越明确 给远期步数分配较小的权重可以抑制方差的累积 数学上的考虑，可以利用资格迹实现反向视角 TD($\\lambda$)的正向视角是坐在现在看未来，需要完整序列；而如果采用反向视角，即站在现在看过去，可以利用资格迹（Eligibility Traces）保存每一个state对当前state的return的贡献，从而实时更新当前步的return到之前的state上，且不需要序列是完整的： TD(λ)的反向视角\nModel-Free Control Model-Free Control用来解决那些未知MDP，或者已知MDP，但由于复杂所以只能采样的问题\nModel-Free Control主要分为：\nOn Policy：基于策略 $\\pi$ 上的采样学习策略 $\\pi$，在自己的认知范围内，行动后调整 Sarsa Off Policy：基于策略 $\\mu$ 上的采样学习策略 $\\pi$，可以复用其他策略的数据，总结别人的经验 Q-Learning、DQN（经验回放） On-Policy Monte-Carlo Control 为了使MC能在policy iteration中使用，由于我们是model-free的，不能采样 $v(s)$，而需要采样 $q(s, a)$，从而直接基于采样结果选择最佳action，来进行greedy policy improvement\n如何进行greedy policy improvement：\n与MDP已知不同，我们需要探索所有行为的可能，而没办法确定某个行为是最优的，这也是从planning到learning的跨越 通过 $\\epsilon-greedy$ 策略来选择action，在保证探索的前提下，同时保证收敛到更优： 不需要过多采样来精确得到 $q(s,a)$，可以只采样一条序列，更新序列中每一个s和a的pair的action value function，然后进行 $\\epsilon-greedy$ 操作来加速迭代 如果探索最终满足GLIE（Greedy in the Limit with Infinite Exploration），则能够保证最终收敛到全局最优，比如 $\\epsilon_k=\\frac{1}{k}$ 就是一个例子： GLIE条件及其工程实现 On-Policy TD Learning（SARSA） 将On-Policy的MC改成TD，就是Sarsa算法： Sarsa算法收敛到最优策略的条件：\n满足GLIE，例如通过 $\\epsilon-greedy$ 选择动作即可 步长 $\\alpha$ 满足Robbins-Monro条件： $\\sum\\limits_{t=1}^{\\infty} \\alpha_t = \\infty$：确保算法有足够的动力消除初始误差 $\\sum\\limits_{t=1}^{\\infty} \\alpha_t^2 \u003c \\infty$：确保随机噪声最终会被抵消，实现稳定 实际应用中，大部分不考虑Robbins-Monro条件，甚至有时候不考虑GLIE，Sarsa也能work 与TD($\\lambda$)类似的思考，通过对n-step Sarsa的average，我们可以得到Sarsa($\\lambda$)： 同样得到基于资格迹的后向视角的工程实现： Off-Policy Off-Policy的意义：\n复用数据：参考其他策略的action和结果，帮助优化自己的policy 在采用exploration的policy的情况下，学习optimal policy 在采用某个policy得情况下，学习多个policy 通过important of sampling来利用策略Q的采样来评估策略P： 对于策略Q产生的样本，由于策略不同，导致这些样本在策略P中出现的概率不同，因此在用于评估策略的时候的权重也不同，因此如果要准确地利用这些样本评估策略P，需要做权重的调整： 由于以下两个原因，off-policy的MC在实际应用中几乎不可用：\n数据饥渴：当 $\\mu(A|S)=0$ 但是 $\\pi(A|S)\\neq0$ 的时候，这条样本不可用，即便可以通过 $\\epsilon-greedy$ 的策略保证 $\\mu(A|S)\\neq0$，但是这时候的权重会极小，$\\epsilon/m$，依然不可用 数值爆炸：重要性采样会因为概率的连乘很大程度增加方差 off-policy的TD Learning使用了bootstrapping，重要性权重只加在最近的一次reward上，大大缓解数据饥渴和数值爆炸的问题： Q-Learning Q-Learning可以避免使用重要性采样，更方便地进行off policy的control：\n通过Q和行为策略 $\\mu$（通常使用 $\\epsilon-greedy$ 来保证探索）选择当前state $S_t$ 要更新的action $A_t$ 确定 $S_t$ 和 $A_t$ 后，环境给出reward，$R_{t+1}$ 和下一个state，$S_{t+1}$ 这部分称为SARS四元组，可以使用历史数据进行回放的方式复用数据 在计算Q的更新时，$A’$ 是通过目标策略 $\\pi$ 选择出来的（max选择最优action），这样可以避免使用重要性采样（因为目标策略 $\\pi$ 是确定性的，不是分布，不需要纠偏） 为什么Q-Learning有逃避“重要性采样”的特权？ 可以理解为：行为策略 $\\mu$ 决定更新哪个state和action，目标策略 $\\pi$ 决定怎么更新（具体数值） 可以理解成：SARSA通过迭代 $Q_{\\pi}$，然后最终 $Q_{\\pi}=Q_$ 的方式找到 $Q_$；而Q-Learning直接迭代 $Q_*$ Q-Learning和Sarsa之间的关系\nTD和DP的关系对比： Value Function Approximation 现实世界中，问题的状态规模是巨大的，这带来两个问题：\n无法存储全部的state/action在memory中 全部更新所有的state/action太慢 Value Function Approximation通过函数来近似表示value： 可以从已知的状态泛化到未知的状态 通过MC或者TD来更新参数w function approximator：\n典型结构： 实现方式： 特征线性组合 神经网络 决策树 最近邻居 傅立叶/小波变换 训练数据特点： 非固定策略：即训练过程中数据有分布会随策略优化而变化 none-iid：即训练数据来自序列且前后关联的，模型可能会在短时间内过度拟合某一段连续的轨迹，导致参数更新剧烈波动 Incremental Methods 采用SGD的方式拟合，由于RL问题中没有label，我们用return作为target来计算梯度：\nMC：$\\Delta \\mathbf{w} = \\alpha( G_t - \\hat{v}(S_t, \\mathbf{w}))\\nabla_{\\mathbf{w}}\\hat{v}(S_t, \\mathbf{w})$ TD(0)：$\\Delta \\mathbf{w} = \\alpha( R_{t+1} + \\gamma\\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}))\\nabla_{\\mathbf{w}}\\hat{v}(S_t, \\mathbf{w})$ TD($\\lambda$)：$\\Delta \\mathbf{w} = \\alpha( G_t^{\\lambda} - \\hat{v}(S_t, \\mathbf{w}))\\nabla_{\\mathbf{w}}\\hat{v}(S_t, \\mathbf{w})$ 反向视角： 资格迹的更新规则要求累积价值函数关于参数的梯度： 预测更新：$E_t = \\gamma \\lambda E_{t-1} + \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})$ 控制更新：$E_t = \\gamma \\lambda E_{t-1} + \\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})$ Function Approximation中资格迹的更新规则 当解决控制问题时，将上述方法使用在action value function上，然后做policy evaluation/improvement的迭代即可\nprediction算法收敛性： On-Policy TD在非线性拟合价值函数时不收敛的原因 Off-Policy TD在函数拟合时不收敛的原因是，死亡三要素条件共同作用下，算法往往不稳定：\nFunction Approximation：参数耦合，导致更新一个状态的价值可能会改变其他状态的价值 Bootstrapping：估计有偏，偏差就会在迭代过程中不断积累和循环 Off-Policy：行为策略不一定向价值高的地方走，导致偏差不会及时发现 control算法收敛性： Batch Methods 为了更有效率地利用样本，我们通常利用agent的经验序列作为数据集，不断采样（经验回放）然后进行SGD求解Least Squares Prediction： DQN DQN使用经验回放和固定Q-targets的方式：\n基于Q-network的计算结果 $Q(s,a,w_i)$，根据 $\\epsilon-greedy$ 策略选择action $a_t$ 存储四元组 $(s_t, a_t, r_{t+1}, s_{t+1})$ 进入回放memory 每次从memory中采样minibatch 基于旧参数 $w^-$ 计算Q-targets 利用SGD优化Q-network和Q-targets之间的MSE，更新Q-network的参数权重 每隔一段时间（～1000步），更新 $w^-$ 到最新参数 DQN能够稳定收敛的两个原因：\n经验回放打散了序列训练数据，让minibatch样本直接减少关联 使用两套网络参数，每次冻结Q-target的网络，更新Q-network，避免在不固定的target上进行bootstrapping 另一种绕过梯度更新的方式是LSPI，基于特征的线性映射，直接通过矩阵运算一次求解参数： Least-Squares Policy Iteration是什么？\nPolicy Gradient 相比求解value function的方式来迭代policy，policy gradient直接优化policy Policy Gradient的优点，主要在于避免了value-based方法的max操作：\n更好的收敛性质：相比value-based方法中取max的操作会导致参数剧烈波动，policy-based的学习目标是一个分布，不会跳变 在高纬度或连续action空间更有效：这种空间本身就很难取max 可以学习随机策略：即最终的策略是一个分布，而不是基于value的max Policy Gradient的缺点：\n通常收敛到局部最优，而不是全局最优 评估policy通常效率低，且高variance Policy Objective Function，即如何评价policy：\n在周期性任务中以初始状态 $s_1$ 起始的value：$J_1(\\theta) = V^{\\pi_\\theta}(s_1) = \\mathbb{E}{\\pi\\theta} [v_1]$ 在连续性任务中的平均value：$J_{avV}(\\theta) = \\sum_{s} d^{\\pi_\\theta}(s) V^{\\pi_\\theta}(s)$ 其中 $d^{\\pi_\\theta}(s)$ 是策略 $\\pi_\\theta$ 下状态的平稳分布 在连续性任务中的每步平均value：$J_{avR}(\\theta) = \\sum_{s} d^{\\pi_\\theta}(s) \\sum_{a} \\pi_\\theta(s, a) \\mathcal{R}_s^a$ Finite Difference Policy Gradient 通过对当前policy不同参数进行微小扰动，评估objective得到对应的delta，作为该参数的梯度： Monte-Carlo Policy Gradient（REINFORCE） 目标 $J(\\theta)$ 是在当前策略下，所有可能路径 $\\tau$ 的总回报 $R(\\tau)$ 的期望值：\n$J(\\theta) = \\mathbb{E}{\\pi\\theta}[R(\\tau)] = \\sum_{\\tau} P(\\tau|\\theta) R(\\tau)$\n对 $J(\\theta)$ 求导，将策略的导数转化为策略对数的导数的期望： $$\\begin{flalign*} \\nabla_\\theta J(\\theta) \u0026= \\sum_{\\tau} \\nabla_\\theta P(\\tau|\\theta) R(\\tau) \u0026\\ \u0026= \\sum_{\\tau} P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta) R(\\tau) \u0026\\ \u0026= \\mathbb{E}{\\pi\\theta} [\\nabla_\\theta \\log P(\\tau|\\theta) R(\\tau)] \u0026 \\end{flalign*}$$\n展开 $P(\\tau|\\theta)$，即一条路径的概率等于：初始状态概率 × 策略概率 × 环境转移概率： $P(\\tau|\\theta) = \\mu(s_0) \\prod_{t=0}^{T} \\pi_\\theta(a_t|s_t) P(s_{t+1}|s_t, a_t)$\n取log得到： $\\log P(\\tau|\\theta) = \\log \\mu(s_0) + \\sum_{t=0}^{T} \\log \\pi_\\theta(a_t|s_t) + \\sum_{t=0}^{T} \\log P(s_{t+1}|s_t, a_t)$\n其中 $\\mu(s_0)$（初始状态）和 $P(s_{t+1}|s_t, a_t)$（环境物理规则）都与 $\\theta$ 无关，可以舍弃：\n因此梯度简化为： $\\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n最终我们得到： $\\nabla_\\theta J(\\theta) = \\mathbb{E}{\\pi\\theta} \\left[ \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) R(\\tau) \\right]$\n可以用一次或几次真实的采样来近似这个期望： $\\nabla_\\theta J(\\theta) \\approx \\sum_{t=0}^{T} R(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n其中，log prob的梯度我们称为score function\n当动作空间是离散的，假设策略是softmax policy，策略 $\\pi$ 的prob是通过 $\\phi(s, a)^\\top \\theta$ 的softmax得到，那么：$\\nabla_\\theta \\log \\pi_\\theta(s, a) = \\phi(s, a) - \\mathbb{E}{\\pi\\theta} [\\phi(s, \\cdot)]$ 其中：\n$\\phi(s, a)$：是你实际采样做出的动作 $a$ 的特征 $\\mathbb{E}{\\pi\\theta} [\\phi(s, \\cdot)]$：是你当前模型下，对所有可能动作特征的平均预期 当动作空间是连续的，假设策略是gaussian policy，均值为 $\\mu(s) = \\phi(s)^\\top \\theta$，即 $a \\sim \\mathcal{N}(\\mu(s), \\sigma^2)$ 那么：$\\nabla_\\theta \\log \\pi_\\theta(s, a) = \\frac{(a - \\mu(s))\\phi(s)}{\\sigma^2}$\n上述两者都可以理解为，当你的action包含这个特征较多且最终证明是正确的时候，加强这个特征\nPolicy Gradient定理 只要你的目标函数可以表示为某种加权状态分布下的期望收益，其梯度都可以统一写成以下公式： 总结Monte-Carlo Policy Gradient（REINFORCE）的思路：\n通过采样获得一组轨迹，以及最终的return 根据return（一般+1或者-1，代表方向）与log prob的梯度得到整体参数关于objective的梯度 更新参数，优化objective Actor-Critic Policy Gradient REINFORCE的奖励是最终累积，且训练数据来自于相同序列，导致方差较大 为减小variance，添加critic部分：\ncritic： 采用MC或者TD（更常用），基于最小化MSE的目标更新参数 $w$ 进行对当前状态的价值 $Q_w(s, a)$ 进行预测，避免累积奖励导致的波动问题 actor： 根据critic给出的价值进行policy gradient的计算和更新 critic的引入带来的bias，即目标价值不一定是无偏的： Compatible Function Approximation解决critic的bias\nAdvantage Actor-Critic（A2C）引入value function作为baseline可以进一步减少方差 同时计算Q和V（维护两套参数w和v），相减得到优势函数： 进一步的，V的TD error就是优势函数的期望： 只需要维护一套参数v，来计算状态的价值函数V，就可以把TD error当成优势函数 Actor-Critic中用V的TD error作为actor的policy gradient？\n与value-based的方案一样，我们考虑到TD(0)的更新方式bias较高，希望引入TD($\\lambda$)减小bias： 如何通过资格迹解决actor的在线更新？\n总结actor的不同实现： Deterministic Policy Gradient 相比于上述随机策略（stochastic）输出动作的概率分布，DPG直接输出确定的动作值：\nSPG由于使用的是似然比技巧（Likelihood Ratio Trick），当训练接近后期，概率分布的variance非常小，很难探索其他动作 DPG采用链式法则（Chain Rule），即使策略已经非常稳定，只要Critic发现更好的Q，动作也会调整： 换一个角度理解： SPG$\\approx$On-policy：为了计算 $\\nabla_\\theta J(\\theta) = \\mathbb{E}{a \\sim \\pi\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a)]$，最直接的方法就是让策略 $\\pi_\\theta$ 去环境中跑出数据，一旦策略更新了，旧的数据就不能直接用了（除非important sampling） DPG$\\approx$Off-policy：由于DPG输出的是确定的动作 $a = \\mu_\\theta(s)$，它本身缺乏探索能力。为了训练它，我们必须使用一个“带噪声的动作”去探索环境，这本质上就是Off-policy Integrating Learning and Planning 相比value-based（通过经验学习value function）和policy-based（通过经验学习policy），我们还可以通过经验学习model，然后通过planning来构造value function和policy RL中Planning和Control的区别： 对于求解最优策略：model-based称为planning，model-free称为control\n优点：\n可以快速利用监督学习的方法学习model 可以基于model的不确定性进行推理：agent会针对不确定的区域进行保守动作/主动学习 缺点： 先学model，再学value function，引入了两次误差 model是一个MDP的表示，包含S（状态空间）、A（动作空间）、P（转移方程）、R（reward方程），其中我们一般假设S和A是已知的，因此model的学习是基于监督数据： 其中：\nR的学习是一个regression问题 P的学习是一个密度估计问题（概率分布） Model-Based RL 通过不同类型的方法学习model：包括Table Lookup、Linear Expectation、Linear Gaussian、Gaussian Process、Deep Belief Network等 基于model，利用model-based的planning方法：Value Iteration、Policy Iteration、Tree Search等，求解MDP 或者基于model生成样本，利用model-free的control方法：Monte-Carlo、Sarsa、Q-learning等 Sample-Based Planning相比直接Model-free Planning的优势 基于不准确的model，使用model-based的方法势必得到非最优解： 当模型完全错误的时候，转而采用model-free的方法 对模型不确定性显示建模： RL中对Model Uncertainty显式建模的方法 Integrated Architectures Dnya-Q Dyna将model-free和model-based结合在一起：\n从真实样本学习model 从真实和模拟样本学习value function和policy Simulation-Based Search Forward Search 基于当前state进行模拟：随机生成样本，截止到n步，构造sub-MDP 基于sub-MDP进行model-free的学习，更新Q和policy 利用MC叫Monte-Carlo Search，利用Sarsa叫TD Search 学习只在sub-MDP上，每次学完就抛弃，并不维护全局Q Forward Search和Dyna的优劣对比 Monte-Carlo Tree Search 在MC Search的基础上，不对每一个动作采样，而是根据动作的价值期望采样 价值大的动作采样更多，保证探索充分，预估精准 价值期望一般通过UCB策略来判断 相比MC Search按固定policy采样，MCTS每次采样分为两个阶段，不断迭代这两个阶段，使采样policy不断提升，最终搜索树会逐渐长成最有效的形状： in-tree：当前state在已探索过的tree内部，采用 $\\epsilon-greedy$ 或者UCB策略 out-of-tree：当前state在tree外部，采用random来快速获得大致价值 Dyna-2 基于Dyna的思路，TD learning+TD search： Long-term memory：通过TD learning迭代全局价值信息 Short-term memory：通过TD search模拟当前局部价值信息 最终的value function是两者的和 Exploration and Exploitation 寻求exploration和exploitation的平衡：\nexploration收集信息 exploitation基于当前信息选择最好的action 探索的三种策略：\n随机探索：探索随机的action（$\\epsilon-greedy$，softmax） 不确定性乐观：判断不确定性的价值，倾向于探索高不确定性的action（UCB） information state space（信息状态空间）： 将agent的信息作为状态的一部分（state=环境（agent外部）+信息（agent内部）） 预判信息是否对reward有帮助 探索的两种维度：\nstate-action探索：在state和action中探索，比较常见 parameter探索：尝试不同的参数，比如policy gradient中的policy参数 优点：在一定步数内保持相同的探索（参数），相比state-action基本上每一步确定是否探索 缺点：对state/action未知，相当于在黑盒中按不同parameter探索 Multi-Armed Bandits 多臂赌博机问题中，我们希望最小化总体regret（与最优action的gap）\nRandom Exploration greedy和 $\\epsilon-greedy$ 策略都是随次数线性增加的总体regret Optimism in the face of Uncertainty Optimistic Initialization： 所有action初始设为最大值，通过采样慢慢收敛到真实值，保证所有action都会被探索到 policy使用greedy或者 $\\epsilon-greedy$，仍然线性增加的总体regret Decaying $\\epsilon_t-greedy$：假设我们知道gap（现实中不可能），可以设置一个衰减速率达到logarithmic asymptotic的总体regret Lai\u0026Robbins定理说明多臂赌博机问题的总体regret下界是logarithmic asymptotic的： $\\triangle_a$ 是最优action收益和其他action收益的差值 $KL(R^a||R^{a^*})$ 是最优action的收益分布和其他action的收益分布的KL散度（分布差异大小） 如果最优和其他action的平均差异大，但是分布又很接近，则总体regret就会大 UCB：通过动作的置信上界（upper confidence）来判断action的好坏： 根据Hoeffding不等式，如果我们希望真值超过UCB的概率是p，则 $U_t(a) = \\sqrt{\\frac{-\\log p}{2N_t(a)}}$ 我们希望随次数变大，p变小，可以设 $p=t^{-4}$，则 $U_t(a) = \\sqrt{\\frac{2 \\log t}{N_t(a)}}$ 最终：$a_t = \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} , Q(a) + \\sqrt{\\frac{2 \\log t}{N_t(a)}}$ Bayesian Bandits：基于价值的先验分布，根据采样得到后验分布，通过后验分布判断action： Bayesian UCB：利用后验分布的variance作为UCB的不确定分数，$a_t = \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} , \\mu_a + c \\sigma_a / \\sqrt{N(a)}$ Probability Matching：基于一个action是最优action的概率来进行采样 Thompson Sampling： 通过对每个动作的value随机采样实现 Information State Space Information State Search：基于信息的价值（长期收益-短期损失）来判断 衡量value of information：不确定性高且与最优action相关的action，包含较大信息价值 RL中信息价值与不确定性的关系 Information State Space Bandits：通过将历史统计作为state的一部分，构成一个新的MDP，可以使用不同的方式求解： Model-free RL：Q-learning Bayesian Model-based RL：Gittins Indices、Bayesian-adaptive MDPs Contextual Bandits 什么是Contextual Bandits？\nMDPs 上述所有的探索策略，都可以应用于MDP上：\n以UCB为例，$a_t = \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} , Q(s_t, a) + U_1(s_t, a) + U_2(s_t, a)$： 评估不确定性可以简单的加入UCB的不确定项 $U_1$ 对于Q的预测不管是评估不确定性，还有策略改进带来的不确定性 $U_2$，这部分计算比较困难 Classic Games Game Theory 作为单独的agent：\n其他agent变成了环境的一部分，游戏变成了MDP，最优策略就是这个MDP的最优策略 纳什均衡是self-play RL中的一个固定点，$\\pi^i = \\pi^i_*(\\pi^{-i})$： 经验序列通过agent间进行游戏产生 每个agent学习如何迭代policy，适应其他agent并构成其他agent的环境 在学习的过程中，每个agent的最优策略都会被学习到 寻找纳什均衡的两种方法：\nGame tree search（planning） Self-play RL 游戏分为perfect游戏和imperfect游戏：\nperfect即完全可观测：国际象棋、围棋等 imperfect即不完全可观测：扑克等 Minimax Search 以两个玩家的游戏为例，$\\pi = \\langle \\pi^1, \\pi^2 \\rangle$\nminimax value function是指在最小化玩家二的value的策略下（玩家二的目标就是最小化reward，从他的视角），最大化玩家一的value：$v_{*}(s) = \\max_{\\pi^1} \\min_{\\pi^2} v_{\\pi}(s)$ minimax policy是指达到minimax value function的policy，即纳什均衡 通过minimax search的树状结构，搜索每一条路径的value，然后交替使用min和max来决策： 树的大小指数增长 替代查表，可以使用value function approximator来表示value 通过固定步深度left的value，向上进行min和max的操作 Self-Play RL value function：可以直接应用MC或者TD算法 policy improvement：由于游戏是deterministic的，即当前state下action的下一个state是确定的，因此可以直接选择max或者min来迭代policy，即 $A_t = \\arg\\max_a v_{*}(\\operatorname{succ}(S_t, a))$\nCombining RL and Minimax Search 将基础RL算法和minimax search相结合的几种方式：\nSimple TD：先用TD learning计算value function，然后用minimax search再过一遍更新 TD Root：利用下一步state的minimax search得到的value学习当前步的value function TD Leaf：利用下一步state的minimax search选取的leaf的value，更新当前state的minimax search选取的leaf的value TD Leaf算法没落的原因 TreeStrap：利用当前state的minimax search所有选取路径上的search value更新路径上全部对应state的value Simulation-Based Search：用self-play代替minimax search，比如MCTS RL in Imperfect-Information Games 在imperfect游戏中，由于看不到对手的信息，许多不同的真实状态可能对应于同一个状态（以你的视角），因此，双方各自维护自己的搜索树\nSmooth UCT Search是在MCTS的UCT算法的基础上，以一定概率基于对方平均行为进行应对： 参考链接：\nDeepMind x UCL | Introduction to Reinforcement Learning 2015",
    "description": "Intro 强化学习与机器学习的差别：\n没有supervisor，只有reward 反馈是滞后的，不是实时的 时间序列，每一个时间步不是i.i.d的 agent的行为会影响后续数据 reward是什么：\n一个标量的反馈信号 表示agent在t时刻的表现 agent的目标是最大化累积reward 序列决策是什么：\n目标是选择action，最大化累积reward action有长期的影响，因为reward是滞后的，意味着可能需要牺牲短期reward来获取长期reward 序列H包含observation、action、reward的循环：O1、A1、R1、O2、A2、R2、… state是什么：\nstate是序列的函数，即当前的状态包含了观察、行为、奖励序列的全部信息 environment state是指环境的内部状态表示，通常是agent不可见的；即使可见，也通常包含一些不相关的噪音 agent state是agent的内部状态表示，作为下一次action选择的输入，也可理解为RL算法的输入 markov state是指包含之前全部信息的当前状态，只依赖于当前状态，与之前状态独立 在完全可观测环境中，agent state等于environment state，构成MDP（markov decision process）；在部分可观测环境中，agent state不等于environment state，构成POMDP（partially observable markov decision process） agent agent的组成：",
    "tags": [
      "技术笔记"
    ],
    "title": "DeepMind x UCL Introduction to RL 2015 课程笔记",
    "uri": "/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "LLM训练主要包含三部分：\nPre-training：通过大量互联网文本数据，训练base model SFT：通过人工标注对话预料进行微调，让base model掌握对话（ai assistant）能力 RLHF：通过强化学习让模型知道人类偏好，什么是好的回复，什么是不好的回复 参考链接：\n# Deep Dive into LLMs like ChatGPT Pre-training 利用大量文本序列，通过预测下一个token的任务训练，得到基座模型（base model），此时模型具备序列文本生成能力\nDeepSeek MTP 训练过程中通过多个head同时预测后续多个token，强迫模型预测长期未来 预测的时候，MTP作为 投机解码的提议者，由MTP生成n个token，然后主干模型再计算是否正确，即符合主干模型的概率分布，如果正确则保留，不正确则抛弃，以此循环 MTP像一个滑动窗口在每个主干模型截止的位置发起下一次多token预测 MTP可以比主干模型小一些，但整体速度收益还是在于多token的并发预测 接受MTP结果的判断通常使用拒绝采样来完成 投机解码中使用拒绝采样判断是否接受 投机解码中使用拒绝采样的修正逻辑 # 为什么LLM投机推理小模型被拒绝后要从修正分布获取正确的token？ 参考链接：\n# deepseek技术解读(1)-彻底理解MLA（Multi-Head Latent Attention） # deepseek技术解读(2)-MTP（Multi-Token Prediction）的前世今生 # deepseek技术解读(3)-MoE的演进之路 Moe #todo 主流大模型采用moe结构\nSFT 利用人工编写的高质量指令回答数据，在基座模型的基础上训练，让模型模仿人类的回答方式（sft model），此时模型具备问答对话能力\nReject Sampling 拒绝采样一般分为四步：\n生成：基于sft模型利用较高的temperature对同一个prompt进行多次生成 验证：基于reward model或者RLVR来判定 筛选：丢弃所有错误的、低分的回答；选出质量最高的一个或几个正确答案 再训练：将这些被选中的答案加入训练集，对模型进行新一轮的 SFT RLHF 进一步微调模型，使其对齐人类的偏好：\n采样与排序 (Sampling \u0026 Ranking)：让 SFT 模型针对同一个问题生成多个不同的回答，然后评价者根据这些回答的质量、安全性、逻辑性进行排序 奖励模型 (Reward Model）：将上述人类排序的数据喂给另一个较小的模型，学习预测人类给回答打分，代替昂贵的人工评价 强化学习算法（PPO/DPO等）： 模型生成回答 奖励模型给回答打分 使用强化学习算法根据分数更新LLM参数 参考链接：\n# LLM Training \u0026 Reinforcement Learning from Google Engineer | SFT + RLHF | PPO vs GRPO vs DPO PPO 算法原理： Reinforcement Learning学习手册-PPO\n$\\mathcal{J}{\\text{PPO}}(\\theta) = \\mathbb{E}{(q,a) \\sim \\mathcal{D}, o \\le t \\sim \\pi_{\\theta_{\\text{old}}}(\\cdot|q)} \\left[ \\min \\left( \\frac{\\pi_\\theta(o_t | q, o_{\u003ct})}{\\pi_{\\theta_{\\text{old}}}(o_t | q, o_{\u003ct})} \\hat{A}t, \\text{clip} \\left( \\frac{\\pi\\theta(o_t | q, o_{\u003ct})}{\\pi_{\\theta_{\\text{old}}}(o_t | q, o_{\u003ct})}, 1 - \\varepsilon, 1 + \\varepsilon \\right) \\hat{A}_t \\right) \\right]$\n其中：$\\hat{A}t^{\\text{GAE}(\\gamma, \\lambda)} = \\sum{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}, \\quad \\delta_l = R_l + \\gamma V(s_{l+1}) - V(s_l)$\nPPO在RLHF中包含：\nactor模型：LLM本身，输出下一个token，得到当前状态 reference模型：防止对齐过程中离SFT模型太远，保留sft之后的原始模型 reward模型：用于对模型生成序列打分 critic模型：判断当前状态的价值 PPO在RLHF中的流程是：\n经验收集 (Rollout / Generation)：基于prompt生成回答，作为训练数据 评分与计算 (Evaluation)： 基于reward模型给序列打分，为了防止模型为了拿高分而“投机取巧”（比如产生乱码但得分很高），通常会引入KL 散度约束，确保优化后的模型不要偏离reference模型太远 基于critic模型给每个状态打分，计算优势函数 $A_t$，即生成当前token得价值比平均高多少 $A(s_t, a_t) = Q(s_t, a_t) - V(s_t) \\approx r_t + \\gamma V(s_{t+1}) - V(s_t)$ 其中 $r_t$ 为即时奖励， 包含： $R_{penalty}(s_t, a_t) = -\\beta \\cdot \\text{KL}(\\pi_{\\phi}(a_t|s_t) || \\pi_{ref}(a_t|s_t))$ $R_{reward_model} = \\text{RM}(prompt + response)$ if $t = T$ else 0 优化更新 (Optimization)： actor更新：使用PPO算法，如果一个token的 $A_t$ 是正的，就增加生成它的概率；如果是负的，就降低 critic更新：利用TD目标值的MSE损失更新critic模型参数，让评估更准确 GAE在更新Critic模型参数时的作用 GAE和资格迹（Eligibility Trace）的关系？ DPO PPO存在以下问题：\n显存占用很大：critic模型的训练更新、reference和reward模型的前向推理 reward model容易有偏，导致模型“刷分”而不解决问题 训练流程复杂，收敛不稳定，超参敏感 DPO在RLHF中的流程是：\n对一个prompt准备正负样本：$y_w$ 和 $y_l$ 通过下面公式直接使模型生成 $y_w$ 的概率变大，$y_l$ 的概率变小 $\\pi_{ref}$ 是sft后的原始模型，$\\pi_{\\theta}$ 是正在训练的模型 $$L_{DPO} = -\\mathbb{E} [\\log \\sigma (\\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)})]$$ DPO的优缺点： 只维护两套模型参数、训练极其稳定、在大多数场景中效果优于复杂的PPO 容易过拟合：会过度压低某些词项的概率，导致模型说话变得死板或“复读” 缺乏探索：DPO是离线学习，只能学习你给它的 $y_w$ 和 $y_l$，不会在尝试中发现“更优解” Online DPO增加探索机制 参考链接：\nDirect Preference Optimization: Your Language Model is Secretly a Reward Model GRPO DeepSeek R1等模型利用GRPO+RLVR实现逻辑推理能力\nGRPO的目标函数： $\\mathcal{J}{GRPO}(\\theta) = \\mathbb{E}{(q,a) \\sim \\mathcal{D}, {o_i}{i=1}^G \\sim \\pi{\\theta_{\\text{old}}}(\\cdot|q)} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left( \\min \\left( r_{i,t}(\\theta) \\hat{A}{i,t}, \\text{clip}(r{i,t}(\\theta), 1 - \\varepsilon, 1 + \\varepsilon) \\hat{A}{i,t} \\right) - \\beta D{\\text{KL}}(\\pi_\\theta || \\pi_{\\text{ref}}) \\right) \\right]$ 其中：$\\hat{A}{i,t} = \\frac{r_i - \\text{mean}({R_i}{i=1}^G)}{\\text{std}({R_i}{i=1}^G)}$，$r{i,t}(\\theta) = \\frac{\\pi_\\theta(o_{i,t} | q, o_{i,\u003ct})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t} | q, o_{i,\u003ct})}$\n针对PPO训练复杂且显存占用高的问题，相比DPO直接去掉RL框架，GRPO选择只去掉critic模型：\n分组采样：针对同一个Prompt，让模型一次性生成一组回答 计算奖励：用验证器（如RLVR）给这组回答分别打分 相对优势计算 (Relative Advantage)：优势值 = (单个回答的得分 - 组内平均分) / 标准差 舍弃critic计算每一个s下V的估计，用基于最终reward的优势替代 critic模型往往比actor模型学习的慢（滞后性），造成眼高手低或者眼低手高 大规模采样下，最终reward（每个token的credit一样）也可以实现对正确率高路径的收敛 更新模型：增加优势值高路径的生成概率，降低优势值低路径的生成概率 保留了 PPO的Clipped Surrogate Objective 在损失函数里直接加入 KL 散度，确保新模型不要偏离原始模型太远 与DPO使用场景的对比：\nGRPO更适合推理问题，或者对于奖励有定制化定义的问题 DPO更适合偏好对齐 GRPO与DPO在使用场景上的区分 为什么GRPO没有学习过程reward也能实现长链路的推理能力？\n只要你的采样组里有多个回答共享了相同的前缀，GRPO就会自动执行“步骤级”的信用分配，是一个隐形的“PRM” 参考链接：\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models # 【大白话04】一文理清强化学习PPO和GRPO算法流程 | 原理图解 GRPO IS SECRETLY A PROCESS REWARD MODEL DAPO DAPO的目标函数： $\\mathcal{J}{DAPO}(\\theta) = \\mathbb{E}{(q,a) \\sim \\mathcal{D}, {o_i}{i=1}^G \\sim \\pi{\\theta_{\\text{old}}}(\\cdot|q)} \\left[ \\frac{1}{\\sum_{i=1}^G |o_i|} \\sum_{i=1}^G \\sum_{t=1}^{|o_i|} \\min \\left( r_{i,t}(\\theta) \\hat{A}{i,t}, \\text{clip}(r{i,t}(\\theta), 1 - \\varepsilon_{\\text{low}}, 1 + \\varepsilon_{\\text{high}}) \\hat{A}_{i,t} \\right) \\right]$ $\\text{s.t.} \\quad 0 \u003c |{o_i \\mid \\text{is_equivalent}(a, o_i)}| \u003c G$\n其中：$r_{i,t}(\\theta) = \\frac{\\pi_\\theta(o_{i,t} | q, o_{i,\u003ct})}{\\pi_{\\theta_{\\text{old}}}(o_{i,t} | q, o_{i,\u003ct})}$，$\\hat{A}{i,t} = \\frac{r_i - \\text{mean}({R_i}{i=1}^G)}{\\text{std}({R_i}_{i=1}^G)}$\nDAPO的全称：Decouple Clip and Dynamic sAmpling Policy Optimization\nDecouple Clip：GRPO的clip的上下限是固定的（比如0.2），导致actor模型的探索不够，容易出现熵崩溃（重复模式）的情况：对于高概率token，乘1.2都已经超过1了，对于低概率token，乘1.2基本变化不大；对clip的上下限解耦，下限保持0.2，上限提高到0.28 Dynamic Sampling：如果一个batch内reward都是0或者1，则梯度消失，浪费算力，因此DAPO过滤掉那些全对或全错的无效样本组，通过反复采样直到这一组里既有对的也有错的，大幅提升了训练效率 Token-Level Policy Gradient Loss：按token进行损失平均，防止长思维链（Long CoT）训练中的“奖励稀释”问题 Overlong Reward Shaping：一般对于超长回复直接reward置零；DAPO改为分段软惩罚，在长度进入缓冲区后开始慢慢加入惩罚，由于是token级别的loss，可以防止模型不知道过长和做错的区别 参考链接：\nDAPO: An Open-Source LLM Reinforcement Learning System at Scale GSPO GSPO认为reward是在序列级给出的，但GRPO却在token级计算重要性比率，粒度不匹配，token级的局部概率变化会导致重要性权重极端化，累积产生高方差梯度，最终引发不可逆的模型崩溃，因此GSPO将重要性ratio和裁剪改为序列级，可以原生支持MoE训练，不需要 Routing Replay\n$\\mathcal{J}{\\text{GSPO}}(\\theta) = \\mathbb{E}{x \\sim \\mathcal{D}, {y_i}{i=1}^G \\sim \\pi{\\theta_{\\text{old}}}(\\cdot|x)} \\left[ \\frac{1}{G} \\sum_{i=1}^G \\min \\left( s_i(\\theta) \\hat{A}_i, \\text{clip} \\left( s_i(\\theta), 1 - \\varepsilon, 1 + \\varepsilon \\right) \\hat{A}_i \\right) \\right]$\n其中：$s_i(\\theta) = \\left( \\frac{\\pi_\\theta(y_i|x)}{\\pi_{\\theta_{\\text{old}}}(y_i|x)} \\right)^{\\frac{1}{|y_i|}} = \\exp \\left( \\frac{1}{|y_i|} \\sum_{t=1}^{|y_i|} \\log \\frac{\\pi_\\theta(y_{i,t}|x, y_{i,\u003ct})}{\\pi_{\\theta_{\\text{old}}}(y_{i,t}|x, y_{i,\u003ct})} \\right)$\nGSPO牺牲了token级的策略优化，但是在RLHF下，这种牺牲往往是利大于弊的：\n奖励模型的局限性：现有的RM通常也是序列级的，用Token级的GRPO去强行拟合一个序列级的RM，本身就会引入巨大的噪声 训练稳定性：Token 级的重要性权重比率波动极大，极易触发clipping导致训练停滞。GSPO 通过长度归一化（Length Normalization），让梯度的更新更加平滑 GSPO的长度归一化：\n随着生成序列变长，重要性权重会由于乘法效应发生剧烈的数值波动 使用几何平均，将累乘转变为累加，避免了数值溢出 除以 $|y_i|$：将整条路径的总偏离度转化成了平均每个Token的偏离度 参考链接：\nGroup Sequence Policy Optimization Tree-GRPO #todo 看tree-based的方法\n参考链接：\nTREE SEARCH FOR LLM AGENT REINFORCEMENT LEARNING RLVR RLVR在PPO/GRPO中用可验证奖励（Verifiable Rewards）替换原始reward model：\nreward model可能有偏，即一段看起来很专业但逻辑全错的代码，也可能高分 verifiable rewards则只验证明确结论，通过给1分，不通过给0分： 数学题的最终答案是否正确 代码题是否能在沙盒环境中跑test 格式是否严格满足要求 通过RLVR：\n彻底解决reward hacking的问题 支持超长链条推理：因为最终的reward很明确，链条再长也可以把reward回传 对于数学/代码这种领域，数据量大且利用率极高 值得注意的是：\nRL训练只是提升模型在Pass@1上采样正确路径的概率，但并没有提升推理能力 过度RL训练会导致模型多样性坍塌 Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?\nPRM OpenAI o1推理模型利用PRM实现逻辑推理能力\nPRM针对推理过程进行reward预测，细粒度帮助模型学习推理的链路，提高准确性：\n在解决多步推理问题时，对每一个推理步进行人工打标，获得数据集训练PRM模型 在RL学习过程中，critic模型的reward加入PRM的预测，并将这个能力通过critic内化到actor中 参考链接：\nLet’s Verify Step by Step Reward Hacking 不可验证任务如何缓解Reward Hacking？\n#todo 强化学习调整agent行为模式：retroformer、voyager #todo 田渊栋：latent reasoning coconut / attention sync / streaming llm #todo thinking machine lab: tinker api / 自己搭megatron、deepspeed #todo R1论文解析 #todo Search-R1、interleaving thinking后训练\nDistillation #todo on-policy distillation\n框架 #todo 框架学习\nvLLM DeepSpeed Megatron Verl 参考链接：\nRL4LLM PPO workflow 及 OpenRLHF、veRL 初步介绍，ray distributed debugger",
    "description": "LLM训练主要包含三部分：\nPre-training：通过大量互联网文本数据，训练base model SFT：通过人工标注对话预料进行微调，让base model掌握对话（ai assistant）能力 RLHF：通过强化学习让模型知道人类偏好，什么是好的回复，什么是不好的回复 参考链接：\n# Deep Dive into LLMs like ChatGPT Pre-training 利用大量文本序列，通过预测下一个token的任务训练，得到基座模型（base model），此时模型具备序列文本生成能力\nDeepSeek MTP 训练过程中通过多个head同时预测后续多个token，强迫模型预测长期未来 预测的时候，MTP作为 投机解码的提议者，由MTP生成n个token，然后主干模型再计算是否正确，即符合主干模型的概率分布，如果正确则保留，不正确则抛弃，以此循环 MTP像一个滑动窗口在每个主干模型截止的位置发起下一次多token预测 MTP可以比主干模型小一些，但整体速度收益还是在于多token的并发预测 接受MTP结果的判断通常使用拒绝采样来完成 投机解码中使用拒绝采样判断是否接受 投机解码中使用拒绝采样的修正逻辑 # 为什么LLM投机推理小模型被拒绝后要从修正分布获取正确的token？ 参考链接：",
    "tags": [
      "技术笔记"
    ],
    "title": "LLM训练技术学习手册",
    "uri": "/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 强化学习基础知识 搭建商品收货记录系统 搭建商品收货记录系统 学习bootstrap基础知识 前端开源css框架，主要用于快速开发响应式的网站，拥有Grid System和丰富预设组件 预设div的一些class：方便基于grid布局的响应式变化，包括grid的数量、顺序以及offset 提供预设的组件：buttons、下拉栏、以及modal（支持js）等 参考链接 整体思路采用flask+sqlite+bootstrap的方式: 从零开始构建商品收货系统 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-基本概念 DeepMind x UCL Introduction to RL 2015 课程笔记-MDP DeepMind x UCL Introduction to RL 2015 课程笔记-Planning by Dynamic Programming 知识 强化学习主要解决agent在环境中选择policy从而达到最大化序列reward的问题 agent与环境的交互（state和action）可以表示为Markov Process 如果加入reward，就变成MRP（Markov Reward Process） 如果再加入decision（$\\pi$），就变成MDP（Markov Decision Process） value function，即从当前state起到最后的序列reward的带衰减的累积，用来衡量当前状态的好坏： state value function：只基于当前state，称为$v(s)$ action value function：基于当前state和选择的action，称为$q(s,a)$ 只考虑MRP的情况，value function是线性可解的，只是复杂度为$O(n^3)$ 通过基于policy的加权求和，可以在v和q之间转化，也可以经过两次转化（v到q到v）得到状态之间的递推关系，称为Bellman Expectation Equation 对于固定policy，可以简化为MRP，value function是线性可解的； MDP求最优policy，即Bellman Optimality Equation，递推关系中由于引入了max操作，是线性不可解的，只能采用迭代法等： policy iteration：通过迭代value function（减小计算复杂度）来评估policy，然后优化policy（选择最大value的state作为action），可以理解为利用Bellman Expectation Equation+Greedy Policy value iteration：不直接迭代value function，而是每次都选择当前最大value的state作为action，然后再根据新的action来更新value，可以理解为利用Bellman Optimality Equation value iteration可以看成每次只迭代一次求value function的policy iteration，避免把时间浪费在“烂策略”的value迭代上 待办 强化学习基础知识 跟进大模型强化学习技术",
    "description": "总结 强化学习基础知识 搭建商品收货记录系统 搭建商品收货记录系统 学习bootstrap基础知识 前端开源css框架，主要用于快速开发响应式的网站，拥有Grid System和丰富预设组件 预设div的一些class：方便基于grid布局的响应式变化，包括grid的数量、顺序以及offset 提供预设的组件：buttons、下拉栏、以及modal（支持js）等 参考链接 整体思路采用flask+sqlite+bootstrap的方式: 从零开始构建商品收货系统 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-基本概念 DeepMind x UCL Introduction to RL 2015 课程笔记-MDP DeepMind x UCL Introduction to RL 2015 课程笔记-Planning by Dynamic Programming 知识 强化学习主要解决agent在环境中选择policy从而达到最大化序列reward的问题 agent与环境的交互（state和action）可以表示为Markov Process 如果加入reward，就变成MRP（Markov Reward Process） 如果再加入decision（$\\pi$），就变成MDP（Markov Decision Process） value function，即从当前state起到最后的序列reward的带衰减的累积，用来衡量当前状态的好坏： state value function：只基于当前state，称为$v(s)$ action value function：基于当前state和选择的action，称为$q(s,a)$ 只考虑MRP的情况，value function是线性可解的，只是复杂度为$O(n^3)$ 通过基于policy的加权求和，可以在v和q之间转化，也可以经过两次转化（v到q到v）得到状态之间的递推关系，称为Bellman Expectation Equation 对于固定policy，可以简化为MRP，value function是线性可解的； MDP求最优policy，即Bellman Optimality Equation，递推关系中由于引入了max操作，是线性不可解的，只能采用迭代法等： policy iteration：通过迭代value function（减小计算复杂度）来评估policy，然后优化policy（选择最大value的state作为action），可以理解为利用Bellman Expectation Equation+Greedy Policy value iteration：不直接迭代value function，而是每次都选择当前最大value的state作为action，然后再根据新的action来更新value，可以理解为利用Bellman Optimality Equation value iteration可以看成每次只迭代一次求value function的policy iteration，避免把时间浪费在“烂策略”的value迭代上 待办 强化学习基础知识 跟进大模型强化学习技术",
    "tags": [
      "周记"
    ],
    "title": "Week10 强化学习基础知识",
    "uri": "/hugo-blog/weekly/week10/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 需求：老家亲戚是做衣服加工的，分为横机和套口两个种类，现在要做一个功能，来汇总和清晰的展示一些统计数据 现状：目前使用excel来完成：图1包含了横机的单位重量（重量/件）、施工单位和套口的施工单位的基础信息；图2包含了横机按时间、施工单位、件数、重量（件数 * 重量/件）的明细；图3包含了套口按时间、施工单位、件数（包含发出和收回）的明细 痛点：亲戚目前每天手动填写当天的新增条目（哪个单位给了多少，什么款式，如果是套口还包含收回多少），然后需要下拉excel单元格显示结果（excel的列目前用公式指定怎么计算结果）。现在希望可以只用手动添加条目，不用excel下拉的方式，且统一品类在同一页面方便截图 想法：做一个网页，加一个后台数据库，直接访问网页操作，但是考虑数据隐私和亲戚电脑没有联网的情况，设计为本地保存数据 整体思路 基于上述要求，gemini的思路： 方案二细节： 从零开始构建商品收货系统-方案细节\n具体实现 代码链接 product-record-manager\n网页布局 侧边栏分为横机、套口两个子页面，子页面逻辑几乎一致（套口不包含重量） 子页面（以横机为例）： 顶部两个卡片添加款式和加工单位，使用频率低所以默认收缩，可点击展开，展开固定三行，超过部分用scroll方式查看 新增记录部分选择款式、加工单位和件数，通过关联款式基础表的单位重量计算总重量，插入明细表 明细展示部分支持按日期、款式和加工单位筛选，结果中默认展示明细，明细支持修改和删除；点击聚合按钮按日期、款式和加工单位聚合件数和重量，聚合后不支持修改和删除 技术细节 框架: Flask + SQLAlchemy + Bootstrap 5 数据库: SQLite 设计模式: Models (模型层): 定义了数据结构（款式，加工单位，横机记录，套口记录) Templates (视图层): 使用Jinja2模板引擎，index.html作为基布局，hengji.html和taokou.html作为具体页面 Routes (控制层): app.py中定义的函数处理URL请求、数据库操作和页面跳转 PRG 模式 (Post/Redirect/Get) 通过PRG模式，实现用户刷新页面的时候，筛选条件清空，且不弹出提示表单内容重置的警告\n后端接收 POST 请求，不直接查询，而是把筛选条件存入用户的session(类似浏览器的 Cookie 缓存)，然后redirect重定向回首页 (GET)\n@app.route('/', methods=['GET', 'POST']) def hengji_index(): if request.method == 'POST': # Store filters in session and Redirect to GET session['date_range'] = request.form.get('date_range', '') session['unit_filter'] = request.form.getlist('unit_filter') session['style_filter'] = request.form.getlist('style_filter') return redirect(url_for('hengji_index')) # GET request: Consume session filters (Flash pattern) # Pop them so they don't persist on next refresh date_range = session.pop('date_range', '') unit_filters = session.pop('unit_filter', []) style_filters = session.pop('style_filter', []) show_aggregate = session.pop('show_aggregate', False) # 实际处理逻辑 Flash消息机制（Flask Flash） flash用于在请求之间传递临时的“一次性”消息（比如“添加成功”、“删除失败”）\n后端，消息被加密存储在 Session 中\nflash('添加成功', 'success') # 参数1: 消息内容, 参数2: 类别(用于前端样式) return redirect(url_for('hengji_index')) 前端，使用了Bootstrap Toast组件来美观地显示这些消息\n\u003cdiv class=\"toast-container ...\"\u003e {% with messages = get_flashed_messages(with_categories=true) %} \u003c!-- 遍历并渲染所有消息 --\u003e {% endwith %} \u003c/div\u003e Flatpickr 日期选择器 一个轻量级、功能强大的日期选择 JS 库，比原生的\u003cinput type=“date”\u003e更漂亮且支持范围选择\n初始化\nflatpickr(\"#dateRange\", { mode: \"range\", // 开启范围选择模式 dateFormat: \"Y-m-d\", // 提交给后台的格式 altInput: true, // 启用以更友好的格式显示给用户 altFormat: \"Y年m月d日\", // 用户看到的格式 locale: \"zh\", // 汉化 onClose: function(...) { // 当用户关闭选择器（选完日期）时，自动提交筛选表单 document.getElementById('filterForm').submit(); } }); 每次#dataRange修改触发表单提交和页面刷新\nfunction setQuickDate(daysAgo) { const end = new Date(); const start = new Date(); start.setDate(start.getDate() - daysAgo); const el = document.querySelector(\"#dateRange\"); if (el \u0026\u0026 el._flatpickr) { el._flatpickr.setDate([start, end], true); document.getElementById('filterForm').submit(); } } Modal（模态框/弹窗） 在 Bootstrap 中，Modal是一种非常流行的 UI 组件，它会在页面的顶层创建一个覆盖层，弹出一个对话框。模态框通常被用来执行“添加新记录”、“编辑记录”或“确认删除”等操作，这样用户就不必离开当前页面\n在 Bootstrap 的模态框（Modal）结构中：\n.modal（遮罩/容器层）：负责全屏覆盖，包括那个半透明的黑色背景（backdrop）。它控制整个弹窗的显示与隐藏，并且负责监听点击背景关闭弹窗的行为 .modal-dialog（定位/尺寸层）：负责弹窗在屏幕中的位置（居中还是顶部）以及弹窗的宽度 .modal-content（内容/皮肤层）：负责弹窗的视觉样式，通常里面会再细分为modal-header、modal-body和modal-footer \u003c!-- Add Unit Modal --\u003e \u003cdiv class=\"modal fade\" id=\"addUnitModal\" tabindex=\"-1\"\u003e \u003cdiv class=\"modal-dialog\"\u003e \u003cdiv class=\"modal-content\"\u003e ... \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e 模板继承 (Jinja2 Template Inheritance) index.html中，利用Jinja2 模板引擎的占位符（坑位）：{% block content %}{% endblock %}\n\u003cdiv class=\"d-flex w-100 overflow-hidden\"\u003e # 侧边栏内容 ... \u003cdiv class=\"col-md-10 content flex-grow-1\" style=\"overflow-x: hidden;\"\u003e {% block content %}{% endblock %} \u003c/div\u003e \u003c/div\u003e hengji.html中开头写\n{% extends \"index.html\" %} 然后后面写\n{% block content %} ...具体内容... {% endblock %} 模板引擎在渲染时，就会自动把content的部分填充到index的坑位里面\n运行细节 打包为exe（build_portable_mac.sh）：\n下载python的window版本以及依赖的whl 打包并编写.bat文件，在无联网环境下双击直接打开 启动逻辑：\n启动 → 检查端口是否被占用 ├─ 已占用 → 打开浏览器 → 退出 └─ 未占用 → 启动服务器 + 心跳监控 app.py中加入心跳检测，判断网页是否alive，如果用户关闭网页，则后台自动关闭，并关闭windows的cmd窗口 index.html中加入每两秒发送心跳的逻辑 每次用户点击.bat，都会首先检测下5000是否在使用（之前打开过），如果有则打开浏览器并直接关闭新后台，防止多开",
    "description": "背景 需求：老家亲戚是做衣服加工的，分为横机和套口两个种类，现在要做一个功能，来汇总和清晰的展示一些统计数据 现状：目前使用excel来完成：图1包含了横机的单位重量（重量/件）、施工单位和套口的施工单位的基础信息；图2包含了横机按时间、施工单位、件数、重量（件数 * 重量/件）的明细；图3包含了套口按时间、施工单位、件数（包含发出和收回）的明细 痛点：亲戚目前每天手动填写当天的新增条目（哪个单位给了多少，什么款式，如果是套口还包含收回多少），然后需要下拉excel单元格显示结果（excel的列目前用公式指定怎么计算结果）。现在希望可以只用手动添加条目，不用excel下拉的方式，且统一品类在同一页面方便截图 想法：做一个网页，加一个后台数据库，直接访问网页操作，但是考虑数据隐私和亲戚电脑没有联网的情况，设计为本地保存数据 整体思路 基于上述要求，gemini的思路： 方案二细节： 从零开始构建商品收货系统-方案细节\n具体实现 代码链接 product-record-manager",
    "tags": [
      "技术笔记"
    ],
    "title": "从零开始构建商品收货系统",
    "uri": "/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%95%86%E5%93%81%E6%94%B6%E8%B4%A7%E7%B3%BB%E7%BB%9F/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "应用 NLP #todo langextract\n推荐 产品 对话助手 Perplexity Claude Gemini Kimi Kimi K2.5 参考链接：\nKIMI K2.5: VISUAL AGENTIC INTELLIGENCE 参考链接：\n# How I use LLMs 长程任务 Manus Context Engineering的一些思考 Manus中的Context Engineering\nContext Offloading的三层抽象 Function Calling：保留10个左右基础的工具调用，例如文件读写、shell命令执行、网页搜索等，这些原子工具写在system prompt中，基本固定 Sandbox Utilities：每个session都运行在虚拟环境中，环境中预安装了很多自定义的命令供agent调用 Packages \u0026 APIs：通过编写的python脚本直接请求api获取数据 技术细节 Q：agent如何知道有哪些命令，以及如何使用这些命令？ A：由于agent在sandbox中执行，可以在system prompt中加入hint，告诉他查看/usr/bin中的命令，由于这些命令都是manus自己开发的，都包含相同格式的–help文档，供agent查看学习\nQ：Manus在使用file system的时候是否实时构建索引，怎么搜索内容？ A：Manus没有实时构建索引，还是通过grep和glob命令来查询中间的文件结果，但是如果需要长期或者企业级知识库，还是需要建索引来完成\nQ：Manus如何处理长期的跨session的memory呢，是否也有像claude.md文件这样的东西？ A：每次session都会总结并向用户确认是否要保存这份知识（当前session的关键信息），目前也在探索如何自动化的处理用户偏好\nQ：Manus会随着模型进步逐渐简化一些基础逻辑吗？ A：会的，从25年3月到10月已经更新了五个版本，需要根据最新最强的模型的能力变化，预先判断架构是否仍然合理，做好准备\nQ：怎么确定文件的存储格式，是使用markdown、plain text还是什么？ A：一般使用line based的plain text，方便进行grep以及查询一定范围行数的内容，markdown有一个问题是某些模型厂商的模型在markdown上训练过于充分导致太容易出现bullet point\nQ：Summarization的prompt的怎么写的，如果写不好会损失信息，一般的做法是基于高召回来调整，Manus呢？ A：不要让模型自由发挥，设定一个类似schema的东西，一些field让模型填写，保持稳定以及保证某些内容一定会被总结到（比如目标）\nQ：Compaction就是把工具的结果写入文件，然后返回一个文件名吗？ A：是的，但是不光是文件，基本所有操作都有一个外部存储方式，比如网页操作有一个url、搜索操作有一个query，他们都天然的有一个用于compaction的外部存储方式\nQ：比如对于搜索，如果返回的token很多，是先做summarization在返回还是整体返回再等着compaction？ A：如果搜索是个复杂搜索，比如多个query那种，会调用高级搜索，本质就是一个sub-agent，他会进行agentic workflow的处理，然后返回固定格式的结果，但是如果是简单的搜索，比如google一下，就是全文返回，然后等待compaction，这种情况下也会让模型写下一些中间的insight存入文件\nQ：Manus是怎么处理agent和agent之间的交流的？ A：由于Manus是在sandbox中运行的，因此agent之间的sharing context就是共用sandbox就可以了，这不难，难得是怎么确保从sub-agent能得到期望的输出。我们指定主agent在创建sub-agent的时候，就需要制定好返回的structured output，然后在sub-agent返回结果的时候，使用限制解码来控制得到我们期望的输出\nQ：Manus是使用anthropic的模型，但是你们有尝试过开源模型吗，怎么选择的？ A：更多是成本上的考虑。当需要使用distributed kv-cache的时候，更前沿的模型厂商拥有更有保障的架构支持，反而比你自己实现成本更低。另外每个大模型厂商的模型技术优势各不相同（claude在代码上强，gemini在多模态上强，openai在数学和推理上强），可以进行task、subtask甚至step粒度的routing\nQ：Manus是不是一种混合模式，在有些时候使用原子操作，有些时候使用codeact模式？ A：是的，这很重要，因为如果都用codeact模式，没办法进行限制解码，导致结果不可控\nQ：Manus的planning是怎么样的？ A：一开始Manus也是使用todo.md，这样你会发现三分之一的操作都是在执行todo.md的更新，很浪费token，后来我们使用了一个agent as tool来专门规划，节省token\nQ：Manus的multi agent是怎么样规划设计的？ A：Manus的agent不是传统的按角色定义的，只有planner agent、executor agent、knowledge management agent以及api registration agent这么几个，我们对于添加agent很谨慎，因为agent之间的交流很难，更多的是使用agent as tool\nQ：Manus是如何做安全防护的？ A：由于Manus是运行在sandbox里的，只要能连接到互联网都不安全，所以我们会监控从sandbox流出的信息，确保不含有带有token信息的内容，即便这样，Manus在agent使用一些敏感操作时，还是会向用户确认，或者直接让用户接手\nQ：Manus是如何进行评估的？ A：一开始刚发布的时候是使用gaia评估集，但是发现分数高的模型反而用户不喜欢。目前是采取以下三种方式：1、每个完成的session都要求用户打分（1-5分）；2、自己构建了一些关于execution的自动化评测集，可以在sandbox里面跑测试；3、招实习生来标注，因为一些如web generation、data visualization的任务需要人工评估（很难有好的reward model）\nQ：Manus会基于提供的工具进行RL训练吗？ A：如果要支持MCP，agent的action space就会非常自由多变，这个情况下RL的方式很难训练。如果有充足的资源可以尝试，但是大模型的厂商都在做这件事，所以我们不需要重复造轮子，我们尽量使用parameter free的方式（不改变模型权重）\n参考链接：\nContext Engineering for AI Agents with LangChain and Manus #todo monica插件使用体验\nOpenAI Deep Research lead agent根据用户query设计策略，创建子agent进行不同方面的探索 子agent通过多步搜索动态分析每次的搜索结果 citation agent单独在最终完成citation插入的定向任务 主要通过prompt engineering解决multi-agent的以下问题，\n简单query创建过多子agent 在web中持续搜索不存在内容 过度更新context产生互相干扰 如何有效评估multi-agent：\n从很小的评测集开始，快速迭代 到某一阶段后通过llm-as-judge来评估：事实性、引用准确、完整性、来源质量和工具效率等 人工校验自动化疏漏（通过prompt修改）：如自动化流程通常倾向于seo较好的文档来源 工业可靠性：\n通过agent的state来快速恢复出现错误之前的state并retry，包括tool call的错误 添加trace，包括决策、交互结构等，方便debug agent的更新升级，不是同时的（会导致运行中的agent报错），而是以agent粒度渐进的 期待后续实现异步调用，目前的同步方式会使通信成为瓶颈，且耗时（等待最慢的子agent） 参考链接：\n# How we built our multi-agent research system open-source prompts in our Cookbook 其他 参考链接：\n# ART·E: How We Built an Email Research Agent That Beats o3 Agent应用 Clawdbot / Moltbot",
    "description": "应用 NLP #todo langextract\n推荐 产品 对话助手 Perplexity Claude Gemini Kimi Kimi K2.5 参考链接：",
    "tags": [
      "技术笔记"
    ],
    "title": "LLM进展与应用",
    "uri": "/hugo-blog/blogs/llm%E8%BF%9B%E5%B1%95%E4%B8%8E%E5%BA%94%E7%94%A8/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 跟进大模型进展 跟进大模型进展 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Memory Deep Agents Multi-agent 知识 Memory主要包含working、episodic、semantic和procedure四种 working基本就是短期的上文对话 episodic和context engineering主要相关，用于提取长上文摘要信息 semantic和rag主要相关，用于提取外部信息 procedure则更多通过模型训练方式内化到参数中，形成解决任务方式的记忆 Deep Agents是agent解决长程任务的一种范式，其中重要的一类任务是Deep Research，通过Agentic Search方式搜索信息生成详细的技术报告 Multi-agent目前主要受限于context共享和任务分工后产生的矛盾问题，基本采用简单的planner/executor的串行模式，基于上下文压缩解决过长的问题 待办 强化学习基础知识",
    "description": "总结 跟进大模型进展 跟进大模型进展 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Memory Deep Agents Multi-agent 知识 Memory主要包含working、episodic、semantic和procedure四种 working基本就是短期的上文对话 episodic和context engineering主要相关，用于提取长上文摘要信息 semantic和rag主要相关，用于提取外部信息 procedure则更多通过模型训练方式内化到参数中，形成解决任务方式的记忆 Deep Agents是agent解决长程任务的一种范式，其中重要的一类任务是Deep Research，通过Agentic Search方式搜索信息生成详细的技术报告 Multi-agent目前主要受限于context共享和任务分工后产生的矛盾问题，基本采用简单的planner/executor的串行模式，基于上下文压缩解决过长的问题 待办 强化学习基础知识",
    "tags": [
      "周记"
    ],
    "title": "Week9 大模型进展",
    "uri": "/hugo-blog/weekly/week9/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "什么是长程任务 长程任务（Long-horizon tasks）任务通常包含数十步甚至上百步的推理与操作，且往往涉及跨软件交互、长时间跨度和不确定的环境反馈\nDeep Agent Deep Agent一般认为由四部分组成：\nplanning tool：前期任务规划 将计划写入文件，标注状态（pending/working/finished） 后续更新状态，并在最终所有计划中的step完成时，结束任务 sub agents： 防止占用主agent的context 拥有专家能力 能力可复用 灵活的权限管理 file system：通过文件系统扩充context 与外部的交互通常token巨大，如web的html数据、pdf等 context过长容易导致模型能力衰减 context过长带来的成本问题 system prompt： 通常很长（几百甚至上千行）且很详细，结构清晰 包含few-shot prompt tool使用规范，包含好的和坏的使用样例 参考链接：\n# What are Deep Agents? # Implementing deepagents: a technical walkthrough # Build AI Agents That Work While You Sleep | Deep Agents Human-in-the-loop 在长程任务中的三大作用：\n遇到不可逆的高风险操作（例如：删除服务器数据、进行大额支付）时，需要人工确认 进入死循环（例如：网页改版导致爬虫失效，或者逻辑推理陷入死结）时，需要人工提示 人类对Agent每一个步骤的反馈（点赞、踩、修改意见）会被记录下来，作为RLHF的训练数据 长程任务有两大技术痛点，必须考 HITL 解决：\n意图对齐：用户说“帮我策划一场旅行”，Agent很难一次性猜准用户所有的偏好。通过HITL，Agent可以在第一阶段（选目的地）询问用户反馈，再进行第二阶段（订酒店） 长程衰减：即使是高级别的模型，在执行到第 20 步时，往往会忘记第一步的初衷。人类介入可以起到“锚点”作用，重置Agent的专注力 Human-in-the-loop通过checkpointer，记录了任务每个环节的状态，从而方便：\n暂停与恢复：到达需要人工审批的任务节点时，将状态存入数据库，然后进程可以完全关闭 回溯：让Agent回滚到每一个节点，从那个节点重新分支执行 故障恢复：执行过程中遇到网络中断或程序崩溃，保证中间结果不丢失 并发执行：在处理成千上万个长程任务时，系统通过thread_id知道哪个状态属于哪个用户 参考链接：\n# Adding Human-in-the-Loop to DeepAgents Agentic Search Agentic Search是指一种全新的搜索体验或产品形态，它改变了过去输入关键词给出链接列表的模式，而是寻找问题的答案或完成复杂的调研任务\n参考链接：\n# Framework-less Agentic Search Deep Research Deep Agent一个具体应用方向，通过分析用户问题，指定研究计划，agentic search的方式进行搜索和反思，最终形成一份用户问题相关的研究报告\n参考链接：\n# Open Deep Research 能力评估 BrowseCamp Plus 包含了需要多轮检索和推理的复杂的问题集合，被认为是衡量agent在复杂搜索和长程推理任务（Long-horizon tasks）上性能的重要标尺：\n固定语料库：提供一个包含约 10 万份文档的固定库，不会出现原版实时检索的结果波动 多维度解耦：它可以分别测试Retriever和Agent的表现 人工校验： 测试集中的答案和支撑文档都经过了人工验证，确保正确性 参考链接：\nBrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent 产品形态 LLM进展与应用-长程任务",
    "description": "什么是长程任务 长程任务（Long-horizon tasks）任务通常包含数十步甚至上百步的推理与操作，且往往涉及跨软件交互、长时间跨度和不确定的环境反馈\nDeep Agent Deep Agent一般认为由四部分组成：\nplanning tool：前期任务规划 将计划写入文件，标注状态（pending/working/finished） 后续更新状态，并在最终所有计划中的step完成时，结束任务 sub agents： 防止占用主agent的context 拥有专家能力 能力可复用 灵活的权限管理 file system：通过文件系统扩充context 与外部的交互通常token巨大，如web的html数据、pdf等 context过长容易导致模型能力衰减 context过长带来的成本问题 system prompt： 通常很长（几百甚至上千行）且很详细，结构清晰 包含few-shot prompt tool使用规范，包含好的和坏的使用样例 参考链接：\n# What are Deep Agents? # Implementing deepagents: a technical walkthrough # Build AI Agents That Work While You Sleep | Deep Agents Human-in-the-loop 在长程任务中的三大作用：",
    "tags": [
      "技术笔记"
    ],
    "title": "长程任务：Deep Agent",
    "uri": "/hugo-blog/blogs/%E9%95%BF%E7%A8%8B%E4%BB%BB%E5%8A%A1deep-agent/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Agent整体概括 Agent和workflow的主要区别在于，workflow的工作流是用户预定义好的，而Agent基于用户目标自己探索工作流。Agent的优势在于给予模型自由度，从而当模型的能力提升时，Agent的能力也会随着提升 Generative Agents通过在虚拟环境中多个agent的交互，证明拥有plan、reflect和memory能力的agent可以更好的作出符合人类共识的判断 Agent的四个核心部分：LLM + 规划 + 记忆 + 工具使用 参考链接：\n# Building effective agents A practical guide to building agents # LLM Powered Autonomous Agents Planning COT 通过强制模型将问题分成多个步骤，形成思维链（chain of thoughts），提升回复准确性 参考链接：\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models TOT Tree of thoughts：构建解决任务的树状结构路径，在每一层扩展N种方案，并进行评估可行性，如果当前分支看起来行不通，可以回溯到之前的节点，尝试另一个分支 参考链接：\nTree of Thoughts: Deliberate Problem Solving with Large Language Models ReAct 将大模型交互的过程拆分为thought、action和observation的循环 结合大模型推理能力进行observation和thought，以及工具调用能力进行action 参考链接：\nREACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS # Understanding ReACT with LangChain # Building a LangGraph ReAct Mini Agent # Talking to a LangChain ReAct Voice Agent Plan and Excute 为了弥补ReAct中一步步方式可能导致的短视问题，采取先整体规划再执行的策略 经过Agent对task的处理后，判断是否已经完成，否则进行replan 参考链接：\nPlan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models ReWOO Reasoning Without Observation 相比ReAct和Plan and Excute每次执行一步观察的方式，ReWoo预先写下整体流程，中间执行结果用占位符代替，最终使用solver整体汇总给出答案，解耦了推理和执行，极大节省token和延时 参考链接：\nReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models Reflexion Reflection是指在推理-观察之后的反思步骤，相比之下ReAct只是观察后的下一步推理 Reflexion 可以被视为一种将 Reflection模式制度化、结构化的特定 Agent 框架 Reflexion框架包括三个核心部分： Actor（执行者）：负责尝试解决问题（生成尝试） Evaluator（评估者）：负责给执行结果打分（判断对错） Self- reflection（反思者）：这是一个拥有“长期记忆”的组件。它会分析失败的原因，并生成一条自然语言提示词（如：“下次不要用 X 库，改用 Y 库”），存入记忆库中 参考链接：\nReflexion: Language Agents with Verbal Reinforcement Learning # Reflection Agents LLM Compiler 和ReWOO的思想类似，是个进化版，做了以下两点优化： planner的输出变成stream，即在每个token输出后都检查是否有新任务，减少等待时间 task list转化为DAG图，只要发现依赖项确认，就直接执行，从而在一些可以并行的任务上并行调用工具 参考链接：\nAn LLM Compiler for Parallel Function Calling LATS LATS (Language Agent Tree Search)是一种将蒙特卡洛树搜索 (MCTS)与 LLM 的Reflection（反思）能力结合的高级规划框架 LATS主要分为以下六个步骤： selection：基于UCB（upper confidence bound，选择收益大且被选中次数少的）分数选择下一步该做什么 expansion：通过大模型进行扩展，进入下一层节点， evaluation：通过大模型给当前节点打分，用于后续selection，判断是否值得继续探索，同时进行剪枝以及局部reflection，防止扩展到不必要的节点 simulation：继续模拟直到截止条件 backpropagation：将最终结果反向传播回所有祖先节点，更新分数 和TOT的对比： 参考链接：\nLanguage Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models #todo system1、2、3\nTools CodeAct 将生成和执行可执行代码（如 Python）作为智能体与环境交互的统一接口，而不是传统的 JSON 或简单的工具调用 目前普遍大模型的代码能力极强，受益于大量的开源代码训练数据 代码执行后产生的错误信息天然作为reflection的输入，纠正模型的下一次行为 参考链接：\n# Executable Code Actions Elicit Better LLM Agents # LangGraph CodeAct Memory 参考 Cognitive Architectures for Language Agents，agent记忆主要分为四类：\nWorking Memory：对话过程中的工作记忆，即对话上下文 Episodic Memory：需要一段时间（每轮或更久）进行提炼得到的用户习惯，注意事项等，参考 Context Engineering学习手册 Semantic Memory：外部知识，可以理解为RAG手段获取的那部分信息，参考 RAG学习手册 Procedural Memory：大模型本身的行为习惯，通常是训练内化到模型能力中，类似于人的本能，参考 Agent训练 参考链接： # Building Brain-Like Memory for AI | LLM Agent Memory Systems\nMemGPT 提出“虚拟内存管理”概念，将向量数据库视为硬盘，将 Context Window 视为内存，实现按需调入\n参考链接：\nMemGPT: Towards LLMs as Operating Systems Mem0 Mem0采用了一种增量处理模式，主要分为两个阶段：\n提取阶段 (Extraction Phase)：通过结合当前的对话摘要和最近的消息序列，利用LLM动态提取出具有持久价值的“显著记忆”事实 更新阶段 (Update Phase)：对提取出的事实进行评估。系统会通过向量检索查找现有记忆，并使用LLM的“工具调用”能力决定对记忆库执行四种操作之一： 添加 (ADD) 更新 (UPDATE) 删除 (DELETE)（当新信息与旧记忆冲突时） 无操作 (NOOP) Mem0还提出了一个增强版本Mem0g。它利用基于图的记忆表示来捕捉实体之间复杂的关联结构，类似于GraphRAG\n参考链接：\nMem0: Building Production-Ready AI Agents with Scalable Long-Term Memory A-Mem A-Mem的工作流程模仿了卡片笔记的构建过程，主要包含以下四个步骤：\n笔记构建：当有新信息进入时，系统生成包含上下文描述、关键词和标签的结构化笔记卡片 链接生成：系统会分析新笔记与历史笔记之间的关联，自动建立语义链接 记忆进化：当新经验产生时，它不仅被添加，还会触发对旧记忆的更新，实现认知的持续演进 记忆检索：利用建立好的链接网络，进行关联检索 参考链接：\nA-Mem: Agentic Memory for LLM Agents #todo memr3\nMulti-agent multi-agent相比single agent带来的能力提升大多数（80%）归功于更多token的使用，其次是工具的数量和模型的选择，相比chatbot，agent的token使用量是4x，multi-agent一般是15x。\n这引出了multi-agent的使用场景：\n高价值产出的任务、可以并行化的任务、context过长的任务、使用复杂工具数量过多的任务 如果多个agent之间需要共享全部context或者有许多相互依赖，则不适合 参考链接：\n# How we built our multi-agent research system 应用实例：\nOpenAI Deep Research 为什么不要构建multi-agent agent通过context engineering维护可靠性（这里可靠性主要指任务完成能力），而子agent之间缺失的部分context会导致任务失败 - 原则1：共享context、以及完整的agent traces，而不只是消息 - 原则2：行为隐含了决策，而不同的决策会矛盾导致出错\n因此，multi-agent的结构最终只能简化为多个agent的串联，然而这在 长程任务中会导致context overflow的问题，需要agent之间进行context compression，但这很难： 可行的multi-agent：\nclaude code：子agent只搜索信息，不参与编程，保证不会出现子agent之间冲突的问题 多agent交互：通过子agent之间的讨论最终达成共识，解决冲突，但这目前很难 参考链接：\n# Don’t Build Multi-Agents 框架实现 langgraph 基于工具：构建handoffs工具，供agent调用 子agent共同连接到一个parent上，parent保存global的信息，每次子agent调用handoffs工具，会把message更新到global里，同时跳转到另一个子agent上 参考链接：\n# Building a multi-agent researcher with llms.txt # Understanding multi-agent handoffs A2A #todo a2a介绍\nswarm 参考链接：\nswarm autgen 参考链接：\nautogen 如何训练agent LLM训练技术学习手册\n参考链接：\n# Build Hour: Agent RFT 如何构建agent 构建可靠Agent的12个关键： 1. natural language to tool calls：将自然语言的需求转化为函数名、参数的能力 2. own your prompts：不依赖框架定义的prompt，自己写，方便调试迭代 3. own your context window：参考 Context Engineering学习手册 4. tools are structured outputs：大模型输出结构化的函数调用、函数执行、反馈结果给大模型 5. unify execution state and business state：执行和业务状态统一，减少复杂性，方便debug和恢复现场 6. launch/pause/resume：需要具备随时启动、暂停、继续的能力 7. contact humans with tool calls：总是通过工具调用的方式进行下一步（包括返回答案、澄清问题和调用工具），使得模型不至于需要在第一个token（“the” or “|JSON”）就明确下一步目标 8. own your control flow：自己定义运行逻辑，提高可控性，包括summarize/compaction、judge等 9. compact errors into context window：通过大模型的自愈能力来解决error 10. small, focused agents：把agent的功能聚焦，context可控、明确分工、方便测试和debug 11. tigger from anywhere, meet users where they are：在不同应用中出现，帮助用户（作者产品的广告） 12. make your agent a stateless reducer：agent尽可能是无状态的reducer，意味着每轮对话都是一个当前状态+用户输入到新状态的函数执行\n参考链接：\n# 12-Factor Agents: Patterns of reliable LLM applications LangChain # What is LangChain? # LangChain vs LangGraph: A Tale of Two Frameworks LangGraph 使用 LangGraph来搭建不同范式的workflow和Agent\t参考链接：\n# Workflows and agents # Building Effective Agents with LangGraph # LangGraph: Planning Agents Deep Agents 使用 deepagents来搭建Deep Agent\n自带各种工具及其对应中间件：planning、sub-agent delegation、filesystem 用户可以提供工具、指令以及sub-agent deepagents-quickstarts中包含了一个deep research的例子 参考链接：\n# Build a Research Agent with Deep Agents ADK Google在2025年发布的agent搭建开源框架，通过提供模块化、结构化的工具，帮助开发者更轻松地构建、评估和部署复杂的智能体系统\n#todo 试用adk 参考链接：\nADK是什么",
    "description": "Agent整体概括 Agent和workflow的主要区别在于，workflow的工作流是用户预定义好的，而Agent基于用户目标自己探索工作流。Agent的优势在于给予模型自由度，从而当模型的能力提升时，Agent的能力也会随着提升 Generative Agents通过在虚拟环境中多个agent的交互，证明拥有plan、reflect和memory能力的agent可以更好的作出符合人类共识的判断 Agent的四个核心部分：LLM + 规划 + 记忆 + 工具使用 参考链接：\n# Building effective agents A practical guide to building agents # LLM Powered Autonomous Agents Planning COT",
    "tags": [
      "技术笔记"
    ],
    "title": "Agent学习手册",
    "uri": "/hugo-blog/blogs/agent%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Context Rot 虽然大模型上下文窗口的上限不断增加，但任性的塞满上下文窗口其实反而会带来回复效果的损失，造成context rot现象 参考链接：\nContext Rot: How Increasing Input Tokens Impacts LLM Performance # How Long Contexts Fail Context Engineering 大模型回复的context应该包含以下几方面： system prompt：系统提示词，回复的高级指导 long-term/short-term memory：长短期，用户对话过程中对后续有用的关键信息 RAG：外部参考数据 tools：工具返回数据 structured output：回复的格式要求 user prompt：用户的上文 context engineering的工作主要包含以下四个方面： 写入：长期记忆（用户偏好的总结）、短期记忆（当前会话的摘要和状态） 选取：选取相关工具和知识、选取当前会话摘要和长期记忆 通过规则，比如指定读取某个memory文件 通过大模型，比如判断合适的工具 通过检索，比如RAG 压缩：基本分为compaction和summarization，经常同时使用 compaction：将信息offload到文件系统中，减少context长度，无损，可以回溯 summarization：直接通过大模型总结，容易造成信息损失 为什么需要压缩： # Don’t Build Multi-Agents 隔离：设定信息的边界，也可以理解为一种信息筛选 tool的调用相当于一种context隔离，只将tool需要的context传递 多agent相当于每个子agent对信息进行隔离，只关注自己需要处理的部分 参考链接： # Effective context engineering for AI agents # The New Skill in AI is Not Prompting, It’s Context Engineering # Context Engineering for Agents # Effective context engineering for AI agents # Build Hour: Agent Memory Patterns Manus中的Context Engineering 围绕KV缓存进行设计 考虑到大模型token成本在有缓存和无缓存情况下的价格相差巨大，且agent相比chatbot而言输入输出token比大得多（由于每一步都包含工具调用结果等），需要面向kv-cache缓存命中率设计context策略：\n保持前缀稳定，注意system prompt中不要包含时间戳 context只追加，注意一些json库会变动内部结构顺序 在需要时明确标记缓存断点，某些大模型厂商不支持自动前缀缓存，需要人工添加 遮蔽，而非移除 在agent任务迭代过程中，不要中途减少工具，这由于以下原因：\n工具定义一般在system prompt的上下，修改定义会导致前缀缓存失效 模型对于当前轮的观察如果基于不存在的工具，会导致下一步出现工具幻觉 更好的方式是对某些token的logits进行mask，从而达到杜绝工具使用的目的。一般模型厂商提供响应预填充选项：\n自动：模型可以选择调用或不调用函数，通过仅预填充回复前缀实现：\u003c|im_start|\u003eassistant 必需：模型必须调用函数，但选择不受约束。通过预填充到工具调用令牌实现：\u003c|im_start|\u003eassistant\u003ctool_call\u003e 指定：模型必须从特定子集中调用函数。通过预填充到函数名称的开头实现：\u003c|im_start|\u003eassistant\u003ctool_call\u003e{“name”: “browser_ 因此可以通过指定+mask指定工具的首token来完成对于对工具的禁用 使用文件系统作为上下文 固定窗口上下文具有以下三个痛点：\n观察结果可能非常庞大，尤其是当代理与网页或PDF等非结构化数据交互时，很容易溢出 模型性能往往会下降，超过一定的上下文长度后，即使技术上支持该窗口大小 长输入成本高昂，即使使用前缀缓存。你仍然需要为传输和预填充每个token付费 而文件系统大小不受限制，天然持久化，并且代理可以直接操作 通过复述操控注意力 Manus通过创建一个todo.md文件，并在任务进行过程中逐步更新它，勾选已完成的项目，来帮助agent保持对任务目标的注意力 保留错误的内容 当模型看到一个失败的行动，以及由此产生的观察结果或堆栈跟踪，它会隐式地更新其内部信念，即改变其先验，降低重复相同错误的可能性。因此不要将agent的错误痕迹擦除，保留在上下文中 不要被少样本示例所困 如果你的上下文充满了类似的过去行动-观察对，模型将倾向于遵循该模式，即使这不再是最优的。解决方法是增加few-shot的多样性，即不同的序列化模板、替代性措辞、顺序或格式上的微小噪音 参考链接：\n# Context Engineering for AI Agents: Lessons from Building Manus",
    "description": "Context Rot 虽然大模型上下文窗口的上限不断增加，但任性的塞满上下文窗口其实反而会带来回复效果的损失，造成context rot现象 参考链接：\nContext Rot: How Increasing Input Tokens Impacts LLM Performance # How Long Contexts Fail Context Engineering 大模型回复的context应该包含以下几方面： system prompt：系统提示词，回复的高级指导 long-term/short-term memory：长短期，用户对话过程中对后续有用的关键信息 RAG：外部参考数据 tools：工具返回数据 structured output：回复的格式要求 user prompt：用户的上文",
    "tags": [
      "技术笔记"
    ],
    "title": "Context Engineering学习手册",
    "uri": "/hugo-blog/blogs/context-engineering%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "RAG Retrieval-Augmented Generation 通过检索获取实时信息，补充模型预训练阶段缺失的知识 需要在两方面确保RAG能真正提升效果： retriever检索到的信息是高度相关且正确的 generator能辨别retriever提供信息的可靠性和正确性，或者抛弃检索信息 参考链接：\n../Answers/2025年RAG技术回顾与展望 CAG CAG（cache-augmented）通过将目标文档全文塞进prompt中，避免RAG中检索不精准的问题，目标文档通过kv-cache的方式预先计算，减少推理开销 关于CAG的存在意义以及其局限性： ../Answers/CAG的存在意义以及其局限性 参考链接：\n# RAG vs. CAG: Solving Knowledge Gaps in AI Models GraphRAG 索引阶段：对于每个chunk进行实体和关系的识别，对图进行社区发现算法，对于每一层社区进行llm摘要 检索阶段： global：主要针对概括性的问题，对每个社区（不同层级）进行llm判断，是否和query相关，排序top的部分进入prompt local：主要针对细节问题，先用向量检索实体，再对实体链接的社区摘要进行llm判断，是否和query相关，排序top的部分进入prompt 缺点： 无法很快更新知识数据，需要重跑社区发现算法和社区摘要生成 检索阶段的global方法需要计算每个摘要的相关度，也很耗费token 参考链接： From Local to Global: A GraphRAG Approach to Query-Focused Summarization # AI知识图谱 GraphRAG 是怎么回事？ graphrag neo4j系列视频 # Intro to GraphRAG — Zach Blumenfeld # GraphRAG: The Marriage of Knowledge Graphs and RAG # Practical GraphRAG: Making LLMs smarter with Knowledge Graphs # Agentic GraphRAG: AI’s Logical Edge LightRAG 对每个chunk中识别出的实体和链接，构造high-level和low-level的kv对，保存概括和细节信息 检索的时候将query也转成high-level和low-level，用向量匹配，匹配到的部分以及1-hop、2-hop进行打分排序，最终拼成prompt 索引阶段，相比GraphRAG需要跑社区检测算法，LightRAG只将新的节点添加进图即可，实时更新知识数据 参考链接：\nLIGHTRAG: SIMPLE AND FAST RETRIEVAL-AUGMENTED GENERATION Zep Zep = GraphRAG + 时序版本控制 + 工业级异步中间件\n三层子图：\nEpisode Subgraph：存储最原始的输入单元（消息、文本、JSON） Semantic Entity Subgraph：从情节（Episodes）中提取出的实体节点及其语义边（关系） Community Subgraph：对强相关的实体进行聚类形成的高阶摘要节点 两层存储：\n消息存储层：使用关系型数据库 (Postgres)，存储完整的对话原文、Token 计数、Session ID 知识存储层：使用图数据库 (Neo4j / Graphiti)，存储三层子图的所有节点和边 对话数据进入后，先存入消息存储，然后异步构建三个子图，子图中的内容都对应到消息存储中，包含了时间戳，保证新的事实替代旧的事实\n参考链接：\nZEP: A TEMPORAL KNOWLEDGE GRAPH ARCHITECTURE FOR AGENT MEMORY # Stop Using RAG as Memory Cognee 一个将非结构化数据转化为确定性图谱的框架 处理关系密集型的数据（如法律卷宗、医疗病历、企业内部文档） 在大模型应用中建立一套可维护、可演进的知识库，而不仅仅是临时的上下文补充 参考链接：\n# Cognee: Superior AI Memory \u0026 Knowledge For AI Agents! Greatly Beats ChatGPT! (Opensource) RAG评估 Ragas 基于大模型对检索质量和生成质量两方面进行评估： 检索质量： 召回：ground truth信息点中检索的覆盖比例，保证没有漏掉关键信息点 准确：检索中与question相关的比例，保证信息的纯净度 生成质量包括幻觉和相关性 支持自动生成测试集 通过大模型提取文档信息，构建临时的语义图 基于高信息文本快生成直接问题 基于语义图进行问题进化，包括多跳进化、推理进化、条件过滤进化、抽象进化 参考链接： Ragas: Automated Evaluation of Retrieval Augmented Generation Ragas Agentic RAG 相比传统的RAG只检索一次，然后拼到prompt里，Agentic RAG利用一个单独的agent来处理整体检索的行为，包括判断： 检索哪个数据源 使用什么工具 query是否需要改写 当前query是否足够进行检索，是否需要用户澄清 是否需要迭代式的检索 参考链接： # What is Agentic RAG? AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON AGENTIC RAG # Build a custom RAG agent with LangGraph",
    "description": "RAG Retrieval-Augmented Generation 通过检索获取实时信息，补充模型预训练阶段缺失的知识 需要在两方面确保RAG能真正提升效果： retriever检索到的信息是高度相关且正确的 generator能辨别retriever提供信息的可靠性和正确性，或者抛弃检索信息 参考链接：\n../Answers/2025年RAG技术回顾与展望 CAG CAG（cache-augmented）通过将目标文档全文塞进prompt中，避免RAG中检索不精准的问题，目标文档通过kv-cache的方式预先计算，减少推理开销 关于CAG的存在意义以及其局限性： ../Answers/CAG的存在意义以及其局限性 参考链接：\n# RAG vs. CAG: Solving Knowledge Gaps in AI Models GraphRAG",
    "tags": [
      "技术笔记"
    ],
    "title": "RAG学习手册",
    "uri": "/hugo-blog/blogs/rag%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "MCP model context protocol在2024年底由anthropic开源， 用于方便大模型agent获取和操作内部数据或者外部API接口 MCP规定了MCP client（agent）和MCP server上的tool、resource、prompt之间的交互协议 MCP中的分工： MCP server MCP server的实现，以python版本github的list_repo_issues为例： from mcp.server.fastmcp import FastMCP import httpx import os # 1. 初始化 FastMCP # name 会显示在 AI 客户端中 mcp = FastMCP(\"GitHub Manager\") # 从环境变量获取 Token GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\") # 2. 定义一个工具 (Tool) # FastMCP 会根据函数签名、类型提示和 Docstring 自动生成 MCP 所需的 Schema @mcp.tool() async def list_repo_issues(owner: str, repo: str) -\u003e str: \"\"\" 获取指定 GitHub 仓库的公开 Issue 列表。 :param owner: 仓库所有者 (例如 'psf') :param repo: 仓库名称 (例如 'requests') \"\"\" url = f\"https://api.github.com/repos/{owner}/{repo}/issues\" headers = { \"Authorization\": f\"token {GITHUB_TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\" } async with httpx.AsyncClient() as client: response = await client.get(url, headers=headers) response.raise_for_status() issues = response.json() # 格式化输出给 AI 看 results = [] for issue in issues[:10]: # 仅取前10个 results.append(f\"#{issue['number']}: {issue['title']}\") return \"\\n\".join(results) if results else \"没有找到打开的 Issue。\" if __name__ == \"__main__\": # 3. 启动 Server（默认使用 stdio 传输） mcp.run() MCP server的配置 { \"mcpServers\": { \"my_python_github\": { \"command\": \"python3\", \"args\": [\"/你的绝对路径/github_server.py\"], \"env\": { \"GITHUB_TOKEN\": \"你的_PERSONAL_ACCESS_TOKEN\" } } } } MCP Host（client） MCP Host会把各个的大模型工具调用json转换为统一的MCP格式调用json，并在MCP server中进行调用（server通常是在Host本地起的一个子进程），从而用户只要关心MCP上不同工具的配置参数即可，不同厂家的模型都可以无缝使用这些工具 MCP Host开始只是大模型厂商在做，逐渐演变成编辑器（IDE）、容器（docker）等加入，只要支持对json的翻译能力就可以 MCP Host初始化 会进行工具查询，与配置文件里的所有 MCP Server 进行“握手” 当你开启一个“新对话”并输入第一句话时，Host 会把缓存里的工具定义转换成模型能懂的格式，塞进system prompt 专业的host甚至在每一轮对话都会传入，以便模型维持记忆，并进行动态筛选+缓存（prompt caching） MCP Gateway 使用MCP网关可以帮我们减少在host上的mcp server的配置工作，简单说就是只用配置一个MCP server，即MCP网关，而MCP网关内部帮我们配置了多个MCP server 如果后续切换Host，或者你有多个Host，也不用重新设置一遍配置，或者在多个Host修改配置 这种网关比如docker desktop： 参考链接：\n# Introducing the Model Context Protocol # What is the Model Context Protocol (MCP)? # MCP是啥？技术原理是什么？一个视频搞懂MCP的一切。Windows系统配置MCP，Cursor,Cline 使用MCP‘ # 用过上百款编程MCP，只有这15个真正好用，Claude Code与Codex配置MCP详细教程 # you need to learn MCP RIGHT NOW!! (Model Context Protocol) Smithery Skills Claude发布，用于将一些重复的能力固化在md文件中，当用户问题与之相关时，自动读取Skill的md文件，加载到prompt中 skill的主要特点，渐进式披露： 元数据按需加载：根据用户query，模型会根据skills的描述选择合适的skill，之后用户确认使用后，再将skill的全部内容发给模型查看 reference：在过程中如果需要某个引用文件（一般内容较多，如规章制度、法律法规等），可以在skill.md中添加对应的查看条件，做到skill内部文件的按需加载 script：在过程中如果需要调用脚本程序，也可以在skil.md中添加对应的使用条件，自动触发脚本执行，注意模型不查看scirpt内容，不消耗token Claude自带的create-skill这个skill可以帮助我们创建自己的skill，通过几轮对话完善一个定制化的skill的md文件 与MCP不同，MCP用于获取外部数据和工具，而Skills用于指导大模型如何做某件事，比如规范、要求等 参考链接：\n# Claude Skills Explained - Step-by-Step Tutorial for Beginners # Equipping agents for the real world with Agent Skills # 停止构建智能体，开始构建技能：Anthropic Agent Skills的深度洞察与AI范式变革 # Agent Skill 从使用到原理，一次讲清",
    "description": "MCP model context protocol在2024年底由anthropic开源， 用于方便大模型agent获取和操作内部数据或者外部API接口 MCP规定了MCP client（agent）和MCP server上的tool、resource、prompt之间的交互协议 MCP中的分工： MCP server MCP server的实现，以python版本github的list_repo_issues为例： from mcp.server.fastmcp import FastMCP import httpx import os # 1. 初始化 FastMCP # name 会显示在 AI 客户端中 mcp = FastMCP(\"GitHub Manager\") # 从环境变量获取 Token GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\") # 2. 定义一个工具 (Tool) # FastMCP 会根据函数签名、类型提示和 Docstring 自动生成 MCP 所需的 Schema @mcp.tool() async def list_repo_issues(owner: str, repo: str) -\u003e str: \"\"\" 获取指定 GitHub 仓库的公开 Issue 列表。 :param owner: 仓库所有者 (例如 'psf') :param repo: 仓库名称 (例如 'requests') \"\"\" url = f\"https://api.github.com/repos/{owner}/{repo}/issues\" headers = { \"Authorization\": f\"token {GITHUB_TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\" } async with httpx.AsyncClient() as client: response = await client.get(url, headers=headers) response.raise_for_status() issues = response.json() # 格式化输出给 AI 看 results = [] for issue in issues[:10]: # 仅取前10个 results.append(f\"#{issue['number']}: {issue['title']}\") return \"\\n\".join(results) if results else \"没有找到打开的 Issue。\" if __name__ == \"__main__\": # 3. 启动 Server（默认使用 stdio 传输） mcp.run() MCP server的配置 { \"mcpServers\": { \"my_python_github\": { \"command\": \"python3\", \"args\": [\"/你的绝对路径/github_server.py\"], \"env\": { \"GITHUB_TOKEN\": \"你的_PERSONAL_ACCESS_TOKEN\" } } } } MCP Host（client） MCP Host会把各个的大模型工具调用json转换为统一的MCP格式调用json，并在MCP server中进行调用（server通常是在Host本地起的一个子进程），从而用户只要关心MCP上不同工具的配置参数即可，不同厂家的模型都可以无缝使用这些工具 MCP Host开始只是大模型厂商在做，逐渐演变成编辑器（IDE）、容器（docker）等加入，只要支持对json的翻译能力就可以 MCP Host初始化 会进行工具查询，与配置文件里的所有 MCP Server 进行“握手” 当你开启一个“新对话”并输入第一句话时，Host 会把缓存里的工具定义转换成模型能懂的格式，塞进system prompt 专业的host甚至在每一轮对话都会传入，以便模型维持记忆，并进行动态筛选+缓存（prompt caching） MCP Gateway 使用MCP网关可以帮我们减少在host上的mcp server的配置工作，简单说就是只用配置一个MCP server，即MCP网关，而MCP网关内部帮我们配置了多个MCP server 如果后续切换Host，或者你有多个Host，也不用重新设置一遍配置，或者在多个Host修改配置 这种网关比如docker desktop： 参考链接：",
    "tags": [
      "技术笔记"
    ],
    "title": "MCP和Skills",
    "uri": "/hugo-blog/blogs/mcp%E5%92%8Cskills/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 跟进大模型进展 跟进大模型进展 了解MCP和Skills： MCP和Skills 跟进RAG技术进展： RAG学习手册 了解Context Engineering： Context Engineering学习手册 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Planning Memory 知识 MCP统一模型调用外部接口的协议，从而让模型方和外部接口方共同努力实现自己到协议的转写，方便打通链路 Skills和workflow中的规划很像，只是用md文件表示，定义了统一的文件结构，方便模型理解workflow RAG演变到2025年主要采用图谱+向量的混合搜索方式（hybrid search），且通常采用agentic的方式进行图谱的构建 Context Engineering是agent如何选取对于当前轮对话最有用的context的技术 Agent=LLM + 规划 + 记忆 + 工具使用 人为定义好的规划，称为workflow，大模型在过程中实现自我规划，称为Agent 待办 跟进Agent技术进展",
    "description": "总结 跟进大模型进展 跟进大模型进展 了解MCP和Skills： MCP和Skills 跟进RAG技术进展： RAG学习手册 了解Context Engineering： Context Engineering学习手册 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Planning Memory 知识 MCP统一模型调用外部接口的协议，从而让模型方和外部接口方共同努力实现自己到协议的转写，方便打通链路 Skills和workflow中的规划很像，只是用md文件表示，定义了统一的文件结构，方便模型理解workflow RAG演变到2025年主要采用图谱+向量的混合搜索方式（hybrid search），且通常采用agentic的方式进行图谱的构建 Context Engineering是agent如何选取对于当前轮对话最有用的context的技术 Agent=LLM + 规划 + 记忆 + 工具使用 人为定义好的规划，称为workflow，大模型在过程中实现自我规划，称为Agent 待办 跟进Agent技术进展",
    "tags": [
      "周记"
    ],
    "title": "Week8 大模型进展",
    "uri": "/hugo-blog/weekly/week8/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Javascript",
    "uri": "/hugo-blog/tags/javascript/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "新建项目 npx create-react-app appName node_modules：保存依赖的库 public：保存静态文件 manifest.json：记录app的元数据，如名字，主题，字体等 robots.txt：设置User-agent、Disallow和Allow，提供网络交互routing规则 src：源代码 index.js：app启动入口，连接到index.html的root节点 App.js：具体app逻辑，可以理解为html的index.html 语法为JSX（Javascript XML），将javascript和html结合 采用function component，即App这个函数，返回一个“动态的html” export default，外部可以复用 App.test.js：测试文件 reportWebVitals.js：性能测试文件 package.json：记录关键信息，例如依赖、版本、启动脚本等 package-lock.json：记录依赖版本，保证协同开发版本一致 参考链接：\n# Master React JS in easy way 基本概念 Components 只返回一个元素：需要把要返回的部分包起来，比如\u003cdiv\u003e或者空的\u003c\u003e mount指添加组件到DOM，unmount指从DOM移除组件 props：用来给Component返回的元素加入属性，来实现不同的具体内容 component中： function Greeting(props) { return \u003ch1\u003e{props.text}\u003c/h1\u003e } 使用中： \u003cGreeting text={'yo'}/\u003e key props：用于区分component，可以用数字或str，一般在map函数中使用： {items.map((item =\u003e ( \u003cComponent key={item.id} /\u003e ))} propTypes：用来确保传入的prop的属性类型正确 array用PropTypes.arrayof object用PropTypes.shape({x: PropTypes.xxx, y: PropTypes.yyy}) Student.propTypes = { name: PropTypes.string, age: PropTypes.number, isStudent: PropTypes.bool, } defaultProp：用来填充prop的默认值 Rendering 利用虚拟DOM（VDOM）进行渲染：react做的三步 当state改变，更新VDOM 通过diffs检查改变 reconciliation：协调改变真实DOM Hook State hooks：useState/useReducer 记录状态，返回状态变量和更新函数 const [count, setCount] = useState(0) 实现受控组件（controlled components），提供数据驱动的能力，将UI和用户行为产生的数据关联在一起 function ControlledInput() { const [value, setValue] = useState('') return ( \u003cinput value={value} onChange={(e) =\u003e setValue(e.target.value)} /\u003e ) } 与原生JS实现的区别： 立即执行 vs 函数引用\n立即执行：onClick={func()}，还没点就运行了 函数引用：onClick={() =\u003e func()} 或 onClick={func} ，只有点的那下才运行 监听函数传入一定是函数引用，否则 多次调用更新函数，react会batch处理，比如三次setValue(value+1)，使用同样的value，最终结果还是value+1，而不是value+3\n可以使用updater function，react会将函数放入队列，顺序执行，是个好习惯 function increment() { setValue(v =\u003e v + 1) setValue(v =\u003e v + 1) setValue(v =\u003e v + 1) } Context hooks：useContext 避免prop需要层层传递的情况，直接通过context交给底层 在生产者组件ComponentA中： import {createContext} from 'react'; export const MyContext = createContext(); 在组件中包裹child \u003cMyContext.Provider value={value}\u003e \u003cChild /\u003e \u003c/MyContext.Provider 在消费者组件中： 导入MyContext import React, { useContext } from 'react'; import { MyContext } from './ComponentA; const value = useContext(MyContext); MyContext只能通过value（规定属性名称）传递一个对象，可以把想要传递的所有内容组成一个大的对象传递下去，也是通用做法 Reference hooks：useRef 和useState一样，都是用来保存数据，但是不希望关联到页面渲染， 相比useState在每次值变化时更新渲染，useRef不会 使用ref的current来存储DOM对象，对current进行操作，从而： 直接访问/交互html的DOM元素 处理focus、animation、transition 管理timer和interval Effect hooks：useEffect 在组件主逻辑运行时的side code，额外做一些事 用effect包起来，可以精确控制执行的条件，如： 组件重新渲染：useEffect(() =\u003e {}) 组件mount：useEffect(() =\u003e {}, [])， []代表空依赖，只在mount时生效 组件内状态变化：useEffect(() =\u003e {}, [value])，在mount和状态值变化时生效 在effect里面返回一个箭头函数，用于组件unmount时清理资源，如remove listener 一般用于： 事件监听：组件mount的写法避免每次渲染都添加新的listener DOM 操作 订阅实时更新 从API获取数据 unmount组件 Performance hooks：useMemo/useCallback Purity 保证component纯净，即相同的输入对应相同的输出 component只返回JSX 不要在render之前在component外部修改component里面的元素 Portal Suspense 加载图标：需要获取数据的时候，提供更好的UX Error Boundaries 通过添加ErrorBoundary的FallbackComponent来控制错误出现时的反应 CSS styling external 提供global作用域的style，适合小项目 class名称在大型项目中可能会重复，导致覆盖和难以管理 module 普通css文件是全局生效的，但如果css文件名为xxx.module.css，vite或者react会识别这个文件名，会自动给你的类名加一个“随机后缀”（哈希值） 缺点包括：导致动态类名写起来麻烦、使用第三方库时，需要:global跳出局部作用域等 inline 除了用module方式，还可以使用inline的方式，即把css样式直接写在component的jsx文件里 适用于简单样式 参考链接：\n# React Full Course for free ⚛️ # Every React Concept Explained in 12 Minutes # ALL React Hooks Explained in 12 Minutes",
    "description": "新建项目 npx create-react-app appName node_modules：保存依赖的库 public：保存静态文件 manifest.json：记录app的元数据，如名字，主题，字体等 robots.txt：设置User-agent、Disallow和Allow，提供网络交互routing规则 src：源代码 index.js：app启动入口，连接到index.html的root节点 App.js：具体app逻辑，可以理解为html的index.html 语法为JSX（Javascript XML），将javascript和html结合 采用function component，即App这个函数，返回一个“动态的html” export default，外部可以复用 App.test.js：测试文件 reportWebVitals.js：性能测试文件 package.json：记录关键信息，例如依赖、版本、启动脚本等 package-lock.json：记录依赖版本，保证协同开发版本一致 参考链接：\n# Master React JS in easy way 基本概念 Components 只返回一个元素：需要把要返回的部分包起来，比如\u003cdiv\u003e或者空的\u003c\u003e mount指添加组件到DOM，unmount指从DOM移除组件 props：用来给Component返回的元素加入属性，来实现不同的具体内容 component中： function Greeting(props) { return \u003ch1\u003e{props.text}\u003c/h1\u003e } 使用中： \u003cGreeting text={'yo'}/\u003e key props：用于区分component，可以用数字或str，一般在map函数中使用： {items.map((item =\u003e ( \u003cComponent key={item.id} /\u003e ))} propTypes：用来确保传入的prop的属性类型正确 array用PropTypes.arrayof object用PropTypes.shape({x: PropTypes.xxx, y: PropTypes.yyy}) Student.propTypes = { name: PropTypes.string, age: PropTypes.number, isStudent: PropTypes.bool, } defaultProp：用来填充prop的默认值 Rendering 利用虚拟DOM（VDOM）进行渲染：react做的三步 当state改变，更新VDOM 通过diffs检查改变 reconciliation：协调改变真实DOM Hook State hooks：useState/useReducer 记录状态，返回状态变量和更新函数 const [count, setCount] = useState(0) 实现受控组件（controlled components），提供数据驱动的能力，将UI和用户行为产生的数据关联在一起 function ControlledInput() { const [value, setValue] = useState('') return ( \u003cinput value={value} onChange={(e) =\u003e setValue(e.target.value)} /\u003e ) } 与原生JS实现的区别：",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "React学习手册",
    "uri": "/hugo-blog/blogs/react%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 利用原生JS完成tetris 学习react基础知识 利用原生JS完成tetris 从零开始构建Tetris React基础知识学习 React学习手册 知识 React是javascript的library，通过JSX（javascript XML）编写，提供了一种通过compenent复用样式的能力，支持基于数据驱动的页面更新 待办 跟进大模型进展",
    "description": "总结 利用原生JS完成tetris 学习react基础知识 利用原生JS完成tetris 从零开始构建Tetris React基础知识学习 React学习手册 知识 React是javascript的library，通过JSX（javascript XML）编写，提供了一种通过compenent复用样式的能力，支持基于数据驱动的页面更新 待办 跟进大模型进展",
    "tags": [
      "周记"
    ],
    "title": "Week7 React学习",
    "uri": "/hugo-blog/weekly/week7/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 在完成css、html、javascript的基础上，构建Tetris游戏，以巩固以上知识点 游戏布局 整体游戏布局分为两部分： 主画面：20 * 10 的grid sidebar：包含以下几部分： next：4 * 4 的grid提示下一个是什么 分数栏：包含当前分数和最高分数 按钮栏：包含暂停（pause）和开始（play） Game Loop 用户进入界面后，游戏状态为“ready” 点击play按钮开始Game Loop，游戏状态进入“play” 每50帧，block向下一格 如果向下位置已经有颜色，则锁住当前block的颜色，新建block，同时清理整行 如果当前block无法新建，即新建位置之前被填充颜色，则失败 游戏过程中点击pause按钮暂停Game Loop，暂停所有keydown事件监听，游戏状态进入“pause” 用户点击play按钮继续Game Loop，游戏状态进入“play” 游戏失败后，游戏状态变为“end”，弹出对话框，显示分数，play按钮文本变为replay 用户点击replay按钮重新初始化，开始Game Loop，游戏状态进入“play” 技术细节 Grid背景 主画面的20 * 10的grid的显示，需要在scene中新建200个div，且每个div之间有gap，scene本身的背景颜色设为深色，div的背景颜色设为白色，这样gap会显示为深色的线 #scene \u003e div { background-color: white; } #scene { display: grid; grid-template-columns: repeat(10, 1fr); grid-template-rows: repeat(20, 1fr); gap: 1px; background-color: #999; /* 网格线颜色 */ border: 3px solid blue; margin: 10px; position: relative; } 移动和旋转 游戏过程中只有一个block是active的，也就是用户控制的 block的状态包含形状（index）、旋转（四种方向）和位置 let blockState = { index: 0, rotation: 0, x: 1, y: 4, } 通过block的index以及x、y，结合旋转，可以确定整个block的所有div 每次旋转90度：[x, y] = [y, -x] const TETROMINOES = [ // I 块 (直线) [ [-1, 0], [0, 0], [1, 0], [2, 0] ], // O 块 (方形) [ [0, 0], [-1, 0], [-1, 1], [0, 1] ], // L 块 [ [-1, 0], [0, 0], [1, 0], [1, 1] ], // J 块 [ [-1, 0], [0, 0], [1, 0], [-1, 1] ], // T 块 [ [-1, 0], [0, 0], [1, 0], [0, 1] ], // S 块 [ [-1, 0], [0, 0], [0, 1], [1, 1] ], // Z 块 [ [0, 0], [1, 0], [-1, 1], [0, 1] ], ]; 当block在边缘处旋转时，有超出边框的可能，这时候需要向里移动一格 新建Block 新建Block的时候，需要将之前设定的nextBlockIndex作为新的blockIndex，同时随机产生一个新的nextBlockIndex，根据这两个index渲染next的grid和scene的grid 如果发现新建Block的div上已经填充颜色，说明scene满了，游戏失败，返回false，否则返回true；Game Loop里通过这个返回值跳出 // 七种模块在next中的index const nextBlockIndices = [ [2, 6, 10, 14], // I [5, 6, 9, 10], // O [1, 5, 9, 10], // L [1, 2, 5, 9], // J [1, 5, 6, 9], // T [1, 5, 6, 10], // S [2, 5, 6, 9], // Z ] function NewBlock() { blockIndex = nextBlockIndex; blockState = { index: blockIndex, rotation: 0, x: 1, y: 4, }; let newBlockIndices = GetBlockAllIndices(blockState); for (let [newX, newY] of newBlockIndices) { if (blockColors[newX][newY] != \"white\") { return false; } } nextBlockIndex = Math.floor(Math.random() * 7); for (let i=0; i \u003c 16; i++) { if (nextBlockIndices[nextBlockIndex].includes(i)) { nextBlocks[i].style.background = index2Color[nextBlockIndex]; } else { nextBlocks[i].style.background = 'white'; } } console.log(`NewBlock: ${blockIndex}, ${nextBlockIndex}`); return true; } Ghost Block 为提升用户体验，为当前block预期掉落位置Ghost Block加border，方便用户通过空格键快速掉落block // 画预期掉落block (Ghost Block) // 1. 计算能掉落多远 let moveX = 0; while (canMoveBlock(moveX + 1, 0)) { // 注意这里要探测 +1 的位置 moveX++; } // 2. 只有当能移动时才画 ghost if (moveX \u003e 0) { let dropBlockState = {...blockState}; dropBlockState.x += moveX; let dropBlockIndices = GetBlockAllIndices(dropBlockState); for (let [x, y] of dropBlockIndices) { let dropIndex = x * colNum + y; // 确保不覆盖已经存在的方块颜色（虽然 ghost 通常在空白处，但为了保险） if (blockColors[x][y] === 'white') { sceneBlocks[dropIndex].style.border = `1px dashed ${index2Color[blockState.index]}`; } } }",
    "description": "背景 在完成css、html、javascript的基础上，构建Tetris游戏，以巩固以上知识点 游戏布局 整体游戏布局分为两部分： 主画面：20 * 10 的grid sidebar：包含以下几部分： next：4 * 4 的grid提示下一个是什么 分数栏：包含当前分数和最高分数 按钮栏：包含暂停（pause）和开始（play） Game Loop 用户进入界面后，游戏状态为“ready” 点击play按钮开始Game Loop，游戏状态进入“play” 每50帧，block向下一格 如果向下位置已经有颜色，则锁住当前block的颜色，新建block，同时清理整行 如果当前block无法新建，即新建位置之前被填充颜色，则失败 游戏过程中点击pause按钮暂停Game Loop，暂停所有keydown事件监听，游戏状态进入“pause” 用户点击play按钮继续Game Loop，游戏状态进入“play” 游戏失败后，游戏状态变为“end”，弹出对话框，显示分数，play按钮文本变为replay 用户点击replay按钮重新初始化，开始Game Loop，游戏状态进入“play” 技术细节 Grid背景 主画面的20 * 10的grid的显示，需要在scene中新建200个div，且每个div之间有gap，scene本身的背景颜色设为深色，div的背景颜色设为白色，这样gap会显示为深色的线 #scene \u003e div { background-color: white; } #scene { display: grid; grid-template-columns: repeat(10, 1fr); grid-template-rows: repeat(20, 1fr); gap: 1px; background-color: #999; /* 网格线颜色 */ border: 3px solid blue; margin: 10px; position: relative; } 移动和旋转 游戏过程中只有一个block是active的，也就是用户控制的 block的状态包含形状（index）、旋转（四种方向）和位置 let blockState = { index: 0, rotation: 0, x: 1, y: 4, } 通过block的index以及x、y，结合旋转，可以确定整个block的所有div 每次旋转90度：[x, y] = [y, -x] const TETROMINOES = [ // I 块 (直线) [ [-1, 0], [0, 0], [1, 0], [2, 0] ], // O 块 (方形) [ [0, 0], [-1, 0], [-1, 1], [0, 1] ], // L 块 [ [-1, 0], [0, 0], [1, 0], [1, 1] ], // J 块 [ [-1, 0], [0, 0], [1, 0], [-1, 1] ], // T 块 [ [-1, 0], [0, 0], [1, 0], [0, 1] ], // S 块 [ [-1, 0], [0, 0], [0, 1], [1, 1] ], // Z 块 [ [0, 0], [1, 0], [-1, 1], [0, 1] ], ]; 当block在边缘处旋转时，有超出边框的可能，这时候需要向里移动一格 新建Block 新建Block的时候，需要将之前设定的nextBlockIndex作为新的blockIndex，同时随机产生一个新的nextBlockIndex，根据这两个index渲染next的grid和scene的grid 如果发现新建Block的div上已经填充颜色，说明scene满了，游戏失败，返回false，否则返回true；Game Loop里通过这个返回值跳出 // 七种模块在next中的index const nextBlockIndices = [ [2, 6, 10, 14], // I [5, 6, 9, 10], // O [1, 5, 9, 10], // L [1, 2, 5, 9], // J [1, 5, 6, 9], // T [1, 5, 6, 10], // S [2, 5, 6, 9], // Z ] function NewBlock() { blockIndex = nextBlockIndex; blockState = { index: blockIndex, rotation: 0, x: 1, y: 4, }; let newBlockIndices = GetBlockAllIndices(blockState); for (let [newX, newY] of newBlockIndices) { if (blockColors[newX][newY] != \"white\") { return false; } } nextBlockIndex = Math.floor(Math.random() * 7); for (let i=0; i \u003c 16; i++) { if (nextBlockIndices[nextBlockIndex].includes(i)) { nextBlocks[i].style.background = index2Color[nextBlockIndex]; } else { nextBlocks[i].style.background = 'white'; } } console.log(`NewBlock: ${blockIndex}, ${nextBlockIndex}`); return true; } Ghost Block 为提升用户体验，为当前block预期掉落位置Ghost Block加border，方便用户通过空格键快速掉落block // 画预期掉落block (Ghost Block) // 1. 计算能掉落多远 let moveX = 0; while (canMoveBlock(moveX + 1, 0)) { // 注意这里要探测 +1 的位置 moveX++; } // 2. 只有当能移动时才画 ghost if (moveX \u003e 0) { let dropBlockState = {...blockState}; dropBlockState.x += moveX; let dropBlockIndices = GetBlockAllIndices(dropBlockState); for (let [x, y] of dropBlockIndices) { let dropIndex = x * colNum + y; // 确保不覆盖已经存在的方块颜色（虽然 ghost 通常在空白处，但为了保险） if (blockColors[x][y] === 'white') { sceneBlocks[dropIndex].style.border = `1px dashed ${index2Color[blockState.index]}`; } } }",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "从零开始构建Tetris",
    "uri": "/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAtetris/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: CSS",
    "uri": "/hugo-blog/tags/css/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础用法 id：采用id的方式（比如\u003cp id=“p1”\u003e），可以用#（比如#p1） class：给元素在html里加上class，在css里面可以针对class加格式，比如.class combinators descendant：使用“ ”，对所有子节点加格式，比如.class p child：使用“\u003e” ，对儿子节点加格式（不包括孙子及更远下属） general sibling：使用“～”，兄弟节点 adjacent sibling：使用“+”，相邻兄弟节点 伪class：定义元素在特殊状态时的格式，比如： 悬浮：hover（li:hover，li:not(:hover)） 选中：active 某个：nth-child（li:nth-child(2)，li:nth-child(even)） 伪元素：对某一类元素加细粒度格式（比如p的第一行），可以用::（比如p::first-line、p::selection） span或者div（称为container）用来分组，从而对各组或整体进行排版 span是inline的，只对内容生效，div是block的，对整体block生效 通过nested layout技术组合横向或竖向（display设为block/inline-block）的div，得到几乎任意排版效果 box model：padding就是字和border的距离，margin就是border和其他元素的距离 position： static：元素原本位置，不受left、right、top、bottom影响，也不作为上级给absolute做基准定位 relative：元素原本位置加上left、right、top、bottom的影响，多用于flex的下级元素 fixed：基于窗口（即browser view）作为基准定位的位置 absolute：基于上级（如果没有上级，即page）作为基准定位的位置 sticky：跟着scroll走 z-index：在html中的出现顺序决定了元素的默认渲染顺序，即后出现的在上面。可以通过设置z-index解决谁覆盖谁的问题（越大越靠上） grid：可以实现行列m x n布局，m和n也可以根据页面大小自适应调整 flexbox：可以方便地对container内部的元素进行动态的水平和垂直布局 可以解决两个div之间由于html换行导致的空格间隙 相比grid先定义行列再填充内容的方式，flexbox根据内容自动调整行列大小 参考链接： # Learn Flexbox CSS in 8 minutes transform：可以使对象旋转、缩放、变形等，会使用gpu加速，做动画场景时优先考虑 aspect-ratio：保持元素的比例，例如16:9 @media：考虑显示器的大小，实现相对应的不同样式，比如横向排版在宽度变小后改为纵向排版 @keyframes：利用transform、opacity、background-color等实现简单的动画效果 var：变量，实现属性的重复使用。变量名一般在:root伪作用域中定义，以双破折号–开头，通过var()函数来引用定义的值 :root { --primary-color: #3498db; /* 蓝色 */ --font-size-base: 16px; } body { background-color: var(--primary-color); font-size: var(--font-size-base); } calc：可以结合变量进行赋值操作，如animation-delay: calc(var(X)) * 100ms;",
    "description": "基础用法 id：采用id的方式（比如\u003cp id=“p1”\u003e），可以用#（比如#p1） class：给元素在html里加上class，在css里面可以针对class加格式，比如.class combinators descendant：使用“ ”，对所有子节点加格式，比如.class p child：使用“\u003e” ，对儿子节点加格式（不包括孙子及更远下属） general sibling：使用“～”，兄弟节点 adjacent sibling：使用“+”，相邻兄弟节点 伪class：定义元素在特殊状态时的格式，比如： 悬浮：hover（li:hover，li:not(:hover)） 选中：active 某个：nth-child（li:nth-child(2)，li:nth-child(even)） 伪元素：对某一类元素加细粒度格式（比如p的第一行），可以用::（比如p::first-line、p::selection） span或者div（称为container）用来分组，从而对各组或整体进行排版 span是inline的，只对内容生效，div是block的，对整体block生效 通过nested layout技术组合横向或竖向（display设为block/inline-block）的div，得到几乎任意排版效果 box model：padding就是字和border的距离，margin就是border和其他元素的距离 position： static：元素原本位置，不受left、right、top、bottom影响，也不作为上级给absolute做基准定位 relative：元素原本位置加上left、right、top、bottom的影响，多用于flex的下级元素 fixed：基于窗口（即browser view）作为基准定位的位置 absolute：基于上级（如果没有上级，即page）作为基准定位的位置 sticky：跟着scroll走 z-index：在html中的出现顺序决定了元素的默认渲染顺序，即后出现的在上面。可以通过设置z-index解决谁覆盖谁的问题（越大越靠上） grid：可以实现行列m x n布局，m和n也可以根据页面大小自适应调整 flexbox：可以方便地对container内部的元素进行动态的水平和垂直布局 可以解决两个div之间由于html换行导致的空格间隙 相比grid先定义行列再填充内容的方式，flexbox根据内容自动调整行列大小 参考链接： # Learn Flexbox CSS in 8 minutes transform：可以使对象旋转、缩放、变形等，会使用gpu加速，做动画场景时优先考虑 aspect-ratio：保持元素的比例，例如16:9 @media：考虑显示器的大小，实现相对应的不同样式，比如横向排版在宽度变小后改为纵向排版 @keyframes：利用transform、opacity、background-color等实现简单的动画效果 var：变量，实现属性的重复使用。变量名一般在:root伪作用域中定义，以双破折号–开头，通过var()函数来引用定义的值 :root { --primary-color: #3498db; /* 蓝色 */ --font-size-base: 16px; } body { background-color: var(--primary-color); font-size: var(--font-size-base); } calc：可以结合变量进行赋值操作，如animation-delay: calc(var(X)) * 100ms;",
    "tags": [
      "技术笔记",
      "CSS"
    ],
    "title": "CSS学习手册",
    "uri": "/hugo-blog/blogs/css%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: HTML",
    "uri": "/hugo-blog/tags/html/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础概念 HTML本质上 就是和markdown 一样的标记语言，用于给文本添加格式。\nmarkdown的设计目标是对应markup做的简化版本\ndata-* ../Answers/HTML中的data-* Canvas canvas创建 \u003ccanvas width=\"300\" height=\"200\"\u003e\u003c/canvas\u003e context 设置使用2D canvas，获取canvas的context，后续操作都是通过context\nconst ctx = canvas.getContext(\"2d\"); 绘制API 矩形 ctx.fillRect(x, y, width, height); 直线 ctx.beginPath(); ctx.moveTo(x1, y1); ctx.lineTo(x2, y2); ctx.stroke(); 图像 参数包括（可以分别选择前3、5、9个参数）： image、paste坐标（x1、y1）、paste大小（w1、h1）、原图坐标（x2、y2）、原图大小（w2、h2）\nconst img = new Image(); img.src = 'xxx.png'; ctx.drawImage(img, x1, y1, w1, h1, x2, y2, w2, h2); 参考链接：\n# How to Draw Images to HTML Canvas (JavaScript Tutorial) 更多链接： HTML5 Canvas Tutorials for Beginners # Canvas HTML5 JavaScript Full Tutorial # HTML Canvas DEEP DIVE",
    "description": "基础概念 HTML本质上 就是和markdown 一样的标记语言，用于给文本添加格式。\nmarkdown的设计目标是对应markup做的简化版本\ndata-* ../Answers/HTML中的data-* Canvas canvas创建 \u003ccanvas width=\"300\" height=\"200\"\u003e\u003c/canvas\u003e context 设置使用2D canvas，获取canvas的context，后续操作都是通过context",
    "tags": [
      "技术笔记",
      "HTML"
    ],
    "title": "HTML学习手册",
    "uri": "/hugo-blog/blogs/html%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "npm npm init在项目中来创建package.json –yes（-y）直接默认 package.json的作用： 管理项目依赖 scripts中支持脚本运行初始化构建项目 package-lock.json：由于不同时间npm install的包版本不一致，需要用这个文件来固定版本，保证不同人安装同样的版本 npm install安装依赖，创建node_modules目录存放 –save（-S）可以自动在package.json中记录 –save-dev（-D）表示只用于develop，不用于production -g：全局（global）安装，不会出现在package.json中，可以命令行执行，例如live-server @X.X.X：安装指定版本 ^X.X.X指保持大版本，小版本和patch更新到最新 ~X.X.X指保持大版本和小版本，patch更新到最新 *指更新到最新 npm list列出项目依赖 –depth 0，不显示子依赖，1，显示一级子依赖 –global true，显示全局安装 npm update更新依赖到指定的最新版本 npm prune删除package.json中不存在的已安装依赖 npm run运行package.json中scripts指定的命令 参考链接：\nnpm Tutorial for Beginners # NPM Full Course For Beginners - Learn NPM fundamentals and basics Yarn yarn add来安装包 yarn.lock对应package-lock.json的作用 yarn set version berry：设置为v2版本 v2版本支持pnp（plug and play）：Yarn 在您的本地文件系统上维护了一个全局缓存目录，所有通过 Yarn 下载的包都会被存储在这个目录中，且是zip压缩的，从而实现离线安装 参考链接：\n# Yarn Package Manager Crash Course pnpm 速度快 node modules中的包文件基于链接，避免重复的io操作 支持monorepos 避免flat结构的node modules带来的幻影依赖问题 参考链接：\n# What Is pnpm? # Why I Switched From NPM/Yarn to PNPM And Why You Should Too! 一些概念 幻影依赖 幻影依赖指的是您的项目代码中直接使用了某个包，但该包并没有在您的package.json文件的dependencies或devDependencies中明确声明。 比如你的包A依赖B，B依赖C，安装包A的依赖的时候会把B和C都安装到node modules中，是你在开发A时可以直接引用C，但是其实package.json中没有显式写出C。 这会导致，当后续B不再依赖C的时候，node moduels中将不再有C，你的包A中的对C的引用会突然失效。 用户项目 用户项目就是指最终使用您的代码，并直接在package.json中声明了对您的包依赖的那个应用程序或库。peerDependency就是指 peer Dependency 对比 处理 Peer Dependency 的方式是包管理工具（npm、Yarn、pnpm）之间差异最大的地方之一： 对于幻影依赖： 参考链接：\n# How JavaScript package managers work: npm vs. yarn vs. pnpm vs. npx Bun Bun是一个Javascript的运行时工具，包括前端、后端 包含了bundler、transpiler、任务执行和npm客户端的综合体 参考链接： ../Answers/Javascript中的bundler和transpiler 替代nodejs和npm，并兼容他们 基本操作 watch mode： bun –watch index.ts，监控改动反应到页面，类似node的nodemon bun –hot index.ts，相比watch不用手动reload页面 .env：存储环境变量，process.env.XXX或者bun.env.XXX bunx：不用安装直接运行，对应npx 支持sqlite 文件读写： 写： const data = 'I love Javascript'; await Bun.write('output.txt', data); 读： const file = await Bun.file('output.txt'); console.log(await file.text()); 测试： import { describe, expect, test, beforeAll } from 'bun:test' beforeAll(() =\u003e { // setup tests }); describe('math', () =\u003e { test('addition', () =\u003e { expect(2 + 2).toBe(4); }) }) bundler：将代码打包成可使用的js文件 bun build ./src/index.ts –outfile=./dist/bundle.js 同时支持watch mode 参考链接： Bun 1.0 # Bun Crash Course | JavaScript Runtime, Bundler \u0026 Transpiler Bun 1.3 bun index.html可以直接起服务 数据库支持增加redis Bun.secrets包裹的部分可以存在keychain中（macOS），提升安全性",
    "description": "npm npm init在项目中来创建package.json –yes（-y）直接默认 package.json的作用： 管理项目依赖 scripts中支持脚本运行初始化构建项目 package-lock.json：由于不同时间npm install的包版本不一致，需要用这个文件来固定版本，保证不同人安装同样的版本 npm install安装依赖，创建node_modules目录存放 –save（-S）可以自动在package.json中记录 –save-dev（-D）表示只用于develop，不用于production -g：全局（global）安装，不会出现在package.json中，可以命令行执行，例如live-server @X.X.X：安装指定版本 ^X.X.X指保持大版本，小版本和patch更新到最新 ~X.X.X指保持大版本和小版本，patch更新到最新 *指更新到最新 npm list列出项目依赖 –depth 0，不显示子依赖，1，显示一级子依赖 –global true，显示全局安装 npm update更新依赖到指定的最新版本 npm prune删除package.json中不存在的已安装依赖 npm run运行package.json中scripts指定的命令 参考链接：\nnpm Tutorial for Beginners # NPM Full Course For Beginners - Learn NPM fundamentals and basics Yarn yarn add来安装包 yarn.lock对应package-lock.json的作用 yarn set version berry：设置为v2版本 v2版本支持pnp（plug and play）：Yarn 在您的本地文件系统上维护了一个全局缓存目录，所有通过 Yarn 下载的包都会被存储在这个目录中，且是zip压缩的，从而实现离线安装 参考链接：",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "Javascript包管理",
    "uri": "/hugo-blog/blogs/javascript%E5%8C%85%E7%AE%A1%E7%90%86/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础用法 console.log：打印变量，可以在页面中inspect看到，或者使用node直接界面打印 对于多个{}的打印可以console.log({foo, bar, baz})可以看到每个map的名字 可以console.table来对结构化数据打印 console.time(XXX)和console.timeEnd(XXX)可以用来计时 console.trace可以追踪执行代码位置 ``：反引号表示模版字面量，类似formatted string 标签模版字面量：函数后面紧接着模版字面量 foo(strs, …values)，如foo`this is ${apple.id}`，strs接收被${}分开的部分，values接收${}的部分，从而自定义字符串解析和输出 可以作为DSL使用，如 ../Answers/标签模版字面量在DSL中的例子 ===：三个等号是全等，即对象类型和值均相等，两个等号只代表值相等 …：spread标识 可变参数，比如function foo (a, b, …c)，c获取剩余可变参数 将两个{}合并在一起，const ab = {…a, …b}; 给array添加元素，a = […a, “apple”, “banana”]; typeof：类型判断，如typeof xxx !== ‘function’ for循环： 对array（可迭代对象，包括map）可以用of for(ele of elements) {...} 对object属性可以用in for(property in element) {...} forEach函数接受的callback可以最多包含element、index、array三个参数 map函数相比forEach的区别：返回新的array；类似的还有filter reduce函数接受accumulator和element两个参数，最终返回一个element arrow function：一种函数定义的简化：() =\u003e … ()里面填参数，…填函数实现，多用于简单的one-liner fruits.forEach(fruit =\u003e console.log(fruit.calories)); this：使用当前对象作为this arrow function中的this的指向是代码位置确定的（词法定义域，Lexical Scope），而不是他被调用时确定的，因此在回调函数函数中可以绑定到当前对象（如果是普通函数作为回调函数，this在运行时才确定，已经和对象失联，只会绑定到global object） 可以用来实现constructor（ES6新特性支持class） super：使用父类方法 在子类中定义constructor时要首先使用父类的constructor，即super(xxx) setter和getter：用set和get关键字函数，定义class的私有属性的读写 外部只通过setter和getter读写私有属性，逻辑可控（比如类型检查） destructuring： 通过[]来解构array内容，例如swap a，b：[a,b] = [b,a] const elements = ['a', 'b', 'c', 'd', 'e']; // a = 'a', b = 'b', c = 'c', r = ['d', 'e'] const [a, b, c, ...r] = elements; 通过{}来解构object内容，可以在function的参数使用这个方式 function displayPerson({firstname, lastname}) { console.log(firstname); console.log(lastname); } sort：默认情况按lexicographic（字母+数字+符号）排序，即1、10、2、3… 可以添加cmp：numbers.sort((a,b) =\u003e a - b)，来进行数字排序 string属性不能用减法，转成NAN后都相等，需要a.name.localeCompare(b.name) shuffle：没有内置的shuffle，需要自己实现Fisher-Yates算法 setTimeout：等待一段时间（毫秒）后执行函数，可以通过clearTimeout取消 异常捕获： 使用try、catch、finally拦截异常 使用throw new Error()抛出异常 修改html： 添加三部曲：构建element、添加属性、插入DOM（基于父节点） 删除：removeChild，基于父节点 事件监听：eventListener 监听click、mouseover、mouseout、keydown、keyup事件 .addEventListener(event, callback) 可以通过DOMContentLoaded事件来等待dom加载完成后再加载图片 classList：通过api访问className，来修改元素的css类，相当于通过js给页面动态加样式 支持add、remove、toggle、replace、contains操作 JSON：stringify和parse实现js obect和json string的相互转换 fetch：异步函数，通过路径（本地或远程连接）读取数据 事件委托 ../Answers/Javascript中的事件委托 Module module：代码通过模块载入，类似python的import 在html的script标签里加上type=“module”，将index.js当作module载入 变量和函数前需要加上export关键字 在index.js头部加上import {变量名、函数名} from ‘文件路径’ 异步 setTimeout就是一个异步函数的例子，不阻碍主线程 一般通过callback、promise、async/await实现 当使用callback来串联异步函数时，会出现callback hell现象，即callback嵌套过度让代码可读性变差，可以通过promise或者async/await解决 promise：用来管理异步操作的对象 异步函数foo()返回promise对象，new Promise((resolve, reject)) =\u003e {异步逻辑} 用.then来承接异步函数，即foo().then(value =\u003e {…})，then里面写resolve处理逻辑 resolve函数可以返回另一个promise，用来串联下一个异步函数 用.catch来承接异步函数，即foo().catch(error =\u003e {…})，catch里面写reject处理逻辑 reject函数指定失败的逻辑，对整体异步链生效，无法针对其中的某个异步函数单独指定reject 如果想对某个异步函数实现单独失败处理，有如下两种方案： 方法一：.then中加入(error =\u003e {…})的部分 方法二：在异步函数中的reject参数中给出类型，在后续统一的catch中按类型实现单独逻辑 async/await：用同步的方式写异步 async让一个函数返回promise 是个语法糖，自动将函数返回结果包装在Promise.resolve()中 await让一个异步函数等待一个promise 通过写一个async关键词的函数来包含多个异步函数，每个异步函数用await来等待结果，用try/catch来捕获异步过程中的error，可以将上述promise的链式调用改写为同步风格的函数： function bar() { return new Promise((...)) } function fuz() { return new Promise((...)) } async function foo() { xxx = await bar(); yyy = await fuz(); } DOM的概念 DOM：Document Object Model 浏览器加载html构建DOM，将元素以树形结构展示 javascript可以通过DOM动态改变网页的内容、结构和样式 动态（live）获取，指查询后的改变能实时更新，类似于引用： getElementById：精准获取element getElementsClassName返回html collection，注意不等同于array，比如不支持forEach操作，可以用Array.from(XXX)来转化为array getElementsByTagName返回所有tag（如h2）下面的html collection 静态（static）获取，指查一次，子元素不再改变，类似于快照、拷贝： querySelector/querySelectorAll通过类似css的获取方式（./#） querySelector获取第一个element querySelectorAll获取nodelist，一般用forEach遍历 可以用console.dir(document)来显示结构 ES6新特性 let和const替代var： 明确的块级作用域， 避免变量提升（hoisting），鼓励先声明后使用 防止重复声明（var可以重复声明，后者覆盖前者） 引入const，var不具有const的属性 this：指向对象本身 只有在对象调用的时候才指向对象，否则指向global object（window） 可以通过bind将函数手动绑定到对象上来正确使用this 异步函数的回调函数中的this，不会自动绑定到对象上，需要使用arrow函数或者手动绑定 const person = { name: 'Alice', talk() { // 箭头函数继承了外部 talk() 的 this (即 person) setTimeout(() =\u003e { console.log(\"this\", this); // 输出: person }, 1000); } }; person.talk(); 解构： 同名结构，如果需要重命名，可以 address = { \"a\": 1, \"b\": 2 } const {a: e, b: f} = address // 这样 e 的值才是 address.a (即 1) …： 可以展开array，例如a.concat(b)等价于[…a, …b]，展开object，例如{…a, …b} 同样的可以用来clone array class： 预留constructor关键字作为构造函数名，类似之前的首字母大写的同名函数 继承：class b extends a { … }，需要在constructor中调用super() module： 对外部需要调用的class/function加上export关键字 named：export class a { … } default：export default class a { … } 对内部需要使用的部分进行import， named：import { a } from ‘文件路径’ default：import a from ‘文件路径’ 命名（named）导出和默认（default）导出： 命名导出只能有一个，而命名导出可以多个，import a, { b, c, d } from ‘文件路径’ 在html文件中需要标记\u003cscript type=‘module’ src=‘index.js’\u003e\u003c/script\u003e 面向对象 OOP的四大基本原则： encapsulation（封装）：对象的内部函数可以将属性用this关键词使用 abstraction（抽象）：定义接口，隐藏内部逻辑和变量 inheritance（继承）：减少重复代码 polymorphism（多态）：不同的子类对于相同函数签名的实现 构造函数： 函数名首字母大写，内部属性用this表示，隐式返回this 调用使用new关键字，否则当作普通函数对待 对象的属性可以随时增减，因为本质就是一个{} 可以用obj[‘xxx’] = yyy来新增属性（正常是obj.xxx = yyy），这样属性名可以不用写死 delete关键字删除属性 基本类型按值拷贝，对象（{}、函数、Array）按引用拷贝 函数内用let定义私有属性，防止外部使用 getter/setter： 通过Object.defineProperty来实现 Object.defineProperty(this, 'xxx', { get: function() { return xxx; }, set: function(value) { if (value is valid) xxx = value; } }) 或者直接定义set，get class Example { constructor(value) { this._value = value; // 约定：内部存储属性 } // 公共接口：控制对属性的访问 get value() { return this._value; } set value(newValue) { this._value = newValue; } } 扩展阅读 javascript游戏编程 Javascript Game Development Masterclass 2022",
    "description": "基础用法 console.log：打印变量，可以在页面中inspect看到，或者使用node直接界面打印 对于多个{}的打印可以console.log({foo, bar, baz})可以看到每个map的名字 可以console.table来对结构化数据打印 console.time(XXX)和console.timeEnd(XXX)可以用来计时 console.trace可以追踪执行代码位置 ``：反引号表示模版字面量，类似formatted string 标签模版字面量：函数后面紧接着模版字面量 foo(strs, …values)，如foo`this is ${apple.id}`，strs接收被${}分开的部分，values接收${}的部分，从而自定义字符串解析和输出 可以作为DSL使用，如 ../Answers/标签模版字面量在DSL中的例子 ===：三个等号是全等，即对象类型和值均相等，两个等号只代表值相等 …：spread标识 可变参数，比如function foo (a, b, …c)，c获取剩余可变参数 将两个{}合并在一起，const ab = {…a, …b}; 给array添加元素，a = […a, “apple”, “banana”]; typeof：类型判断，如typeof xxx !== ‘function’ for循环： 对array（可迭代对象，包括map）可以用of for(ele of elements) {...} 对object属性可以用in for(property in element) {...} forEach函数接受的callback可以最多包含element、index、array三个参数 map函数相比forEach的区别：返回新的array；类似的还有filter reduce函数接受accumulator和element两个参数，最终返回一个element arrow function：一种函数定义的简化：() =\u003e … ()里面填参数，…填函数实现，多用于简单的one-liner fruits.forEach(fruit =\u003e console.log(fruit.calories)); this：使用当前对象作为this arrow function中的this的指向是代码位置确定的（词法定义域，Lexical Scope），而不是他被调用时确定的，因此在回调函数函数中可以绑定到当前对象（如果是普通函数作为回调函数，this在运行时才确定，已经和对象失联，只会绑定到global object） 可以用来实现constructor（ES6新特性支持class） super：使用父类方法 在子类中定义constructor时要首先使用父类的constructor，即super(xxx) setter和getter：用set和get关键字函数，定义class的私有属性的读写 外部只通过setter和getter读写私有属性，逻辑可控（比如类型检查） destructuring： 通过[]来解构array内容，例如swap a，b：[a,b] = [b,a] const elements = ['a', 'b', 'c', 'd', 'e']; // a = 'a', b = 'b', c = 'c', r = ['d', 'e'] const [a, b, c, ...r] = elements; 通过{}来解构object内容，可以在function的参数使用这个方式 function displayPerson({firstname, lastname}) { console.log(firstname); console.log(lastname); } sort：默认情况按lexicographic（字母+数字+符号）排序，即1、10、2、3… 可以添加cmp：numbers.sort((a,b) =\u003e a - b)，来进行数字排序 string属性不能用减法，转成NAN后都相等，需要a.name.localeCompare(b.name) shuffle：没有内置的shuffle，需要自己实现Fisher-Yates算法 setTimeout：等待一段时间（毫秒）后执行函数，可以通过clearTimeout取消 异常捕获： 使用try、catch、finally拦截异常 使用throw new Error()抛出异常 修改html： 添加三部曲：构建element、添加属性、插入DOM（基于父节点） 删除：removeChild，基于父节点 事件监听：eventListener 监听click、mouseover、mouseout、keydown、keyup事件 .addEventListener(event, callback) 可以通过DOMContentLoaded事件来等待dom加载完成后再加载图片 classList：通过api访问className，来修改元素的css类，相当于通过js给页面动态加样式 支持add、remove、toggle、replace、contains操作 JSON：stringify和parse实现js obect和json string的相互转换 fetch：异步函数，通过路径（本地或远程连接）读取数据 事件委托 ../Answers/Javascript中的事件委托 Module module：代码通过模块载入，类似python的import 在html的script标签里加上type=“module”，将index.js当作module载入 变量和函数前需要加上export关键字 在index.js头部加上import {变量名、函数名} from ‘文件路径’ 异步 setTimeout就是一个异步函数的例子，不阻碍主线程 一般通过callback、promise、async/await实现 当使用callback来串联异步函数时，会出现callback hell现象，即callback嵌套过度让代码可读性变差，可以通过promise或者async/await解决 promise：用来管理异步操作的对象 异步函数foo()返回promise对象，new Promise((resolve, reject)) =\u003e {异步逻辑} 用.then来承接异步函数，即foo().then(value =\u003e {…})，then里面写resolve处理逻辑 resolve函数可以返回另一个promise，用来串联下一个异步函数 用.catch来承接异步函数，即foo().catch(error =\u003e {…})，catch里面写reject处理逻辑 reject函数指定失败的逻辑，对整体异步链生效，无法针对其中的某个异步函数单独指定reject 如果想对某个异步函数实现单独失败处理，有如下两种方案： 方法一：.then中加入(error =\u003e {…})的部分 方法二：在异步函数中的reject参数中给出类型，在后续统一的catch中按类型实现单独逻辑 async/await：用同步的方式写异步 async让一个函数返回promise 是个语法糖，自动将函数返回结果包装在Promise.resolve()中 await让一个异步函数等待一个promise 通过写一个async关键词的函数来包含多个异步函数，每个异步函数用await来等待结果，用try/catch来捕获异步过程中的error，可以将上述promise的链式调用改写为同步风格的函数： function bar() { return new Promise((...)) } function fuz() { return new Promise((...)) } async function foo() { xxx = await bar(); yyy = await fuz(); } DOM的概念 DOM：Document Object Model 浏览器加载html构建DOM，将元素以树形结构展示 javascript可以通过DOM动态改变网页的内容、结构和样式 动态（live）获取，指查询后的改变能实时更新，类似于引用： getElementById：精准获取element getElementsClassName返回html collection，注意不等同于array，比如不支持forEach操作，可以用Array.from(XXX)来转化为array getElementsByTagName返回所有tag（如h2）下面的html collection 静态（static）获取，指查一次，子元素不再改变，类似于快照、拷贝： querySelector/querySelectorAll通过类似css的获取方式（./#） querySelector获取第一个element querySelectorAll获取nodelist，一般用forEach遍历 可以用console.dir(document)来显示结构 ES6新特性 let和const替代var： 明确的块级作用域， 避免变量提升（hoisting），鼓励先声明后使用 防止重复声明（var可以重复声明，后者覆盖前者） 引入const，var不具有const的属性 this：指向对象本身 只有在对象调用的时候才指向对象，否则指向global object（window） 可以通过bind将函数手动绑定到对象上来正确使用this 异步函数的回调函数中的this，不会自动绑定到对象上，需要使用arrow函数或者手动绑定 const person = { name: 'Alice', talk() { // 箭头函数继承了外部 talk() 的 this (即 person) setTimeout(() =\u003e { console.log(\"this\", this); // 输出: person }, 1000); } }; person.talk(); 解构： 同名结构，如果需要重命名，可以 address = { \"a\": 1, \"b\": 2 } const {a: e, b: f} = address // 这样 e 的值才是 address.a (即 1) …： 可以展开array，例如a.concat(b)等价于[…a, …b]，展开object，例如{…a, …b} 同样的可以用来clone array class： 预留constructor关键字作为构造函数名，类似之前的首字母大写的同名函数 继承：class b extends a { … }，需要在constructor中调用super() module： 对外部需要调用的class/function加上export关键字 named：export class a { … } default：export default class a { … } 对内部需要使用的部分进行import， named：import { a } from ‘文件路径’ default：import a from ‘文件路径’ 命名（named）导出和默认（default）导出： 命名导出只能有一个，而命名导出可以多个，import a, { b, c, d } from ‘文件路径’ 在html文件中需要标记\u003cscript type=‘module’ src=‘index.js’\u003e\u003c/script\u003e 面向对象 OOP的四大基本原则： encapsulation（封装）：对象的内部函数可以将属性用this关键词使用 abstraction（抽象）：定义接口，隐藏内部逻辑和变量 inheritance（继承）：减少重复代码 polymorphism（多态）：不同的子类对于相同函数签名的实现 构造函数： 函数名首字母大写，内部属性用this表示，隐式返回this 调用使用new关键字，否则当作普通函数对待 对象的属性可以随时增减，因为本质就是一个{} 可以用obj[‘xxx’] = yyy来新增属性（正常是obj.xxx = yyy），这样属性名可以不用写死 delete关键字删除属性 基本类型按值拷贝，对象（{}、函数、Array）按引用拷贝 函数内用let定义私有属性，防止外部使用 getter/setter： 通过Object.defineProperty来实现 Object.defineProperty(this, 'xxx', { get: function() { return xxx; }, set: function(value) { if (value is valid) xxx = value; } }) 或者直接定义set，get class Example { constructor(value) { this._value = value; // 约定：内部存储属性 } // 公共接口：控制对属性的访问 get value() { return this._value; } set value(newValue) { this._value = newValue; } } 扩展阅读 javascript游戏编程 Javascript Game Development Masterclass 2022",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "Javascript学习手册",
    "uri": "/hugo-blog/blogs/javascript%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "官网链接",
    "description": "官网链接",
    "tags": [],
    "title": "像素工具Asperite",
    "uri": "/hugo-blog/blogs/%E5%83%8F%E7%B4%A0%E5%B7%A5%E5%85%B7asperite/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Game Loop Game Loop的结构通常是以下三步的循环： Process Input Update Game State Draw Game 参考链接： Game Programming Patterns # Game States and Game Loops # Getting The Game Loop Right 扩展阅读",
    "description": "Game Loop Game Loop的结构通常是以下三步的循环： Process Input Update Game State Draw Game 参考链接： Game Programming Patterns # Game States and Game Loops # Getting The Game Loop Right 扩展阅读",
    "tags": [
      "技术笔记"
    ],
    "title": "游戏编程基本概念",
    "uri": "/hugo-blog/blogs/%E6%B8%B8%E6%88%8F%E7%BC%96%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 Javascript进阶学习 利用原生JS搭建Flappy Bird 了解Javascript的包管理器 Javascript面向对象编程 Javascript学习手册-面向对象 课程链接： # Object-oriented Programming in JavaScript: Made Super Simple | Mosh 学习Javascript ES6新特性 Javascript学习手册-ES6新特性 课程链接： # ES6 Tutorial: Learn Modern JavaScript in 1 Hour 开发Flappy Bird 从零开始构建Flappy Bird 了解Javascript的包管理器 Javascript包管理 知识 Javascript ES6 2015年发布，主要引入let、const、class、module等 Javascript的包管理器有npm、yarn、pnpm等 Bun作为nodejs的运行时替代，也包含包管理器，可以完成前后端整体链路 待办 利用原生JS搭建俄罗斯方块 学习React框架基础知识",
    "description": "总结 Javascript进阶学习 利用原生JS搭建Flappy Bird 了解Javascript的包管理器 Javascript面向对象编程 Javascript学习手册-面向对象 课程链接： # Object-oriented Programming in JavaScript: Made Super Simple | Mosh 学习Javascript ES6新特性 Javascript学习手册-ES6新特性 课程链接： # ES6 Tutorial: Learn Modern JavaScript in 1 Hour 开发Flappy Bird 从零开始构建Flappy Bird 了解Javascript的包管理器 Javascript包管理 知识 Javascript ES6 2015年发布，主要引入let、const、class、module等 Javascript的包管理器有npm、yarn、pnpm等 Bun作为nodejs的运行时替代，也包含包管理器，可以完成前后端整体链路 待办 利用原生JS搭建俄罗斯方块 学习React框架基础知识",
    "tags": [
      "周记"
    ],
    "title": "Week6 Flappy Bird",
    "uri": "/hugo-blog/weekly/week6/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 在完成css、html、javascript的基础上，构建Flappy Bird游戏，以巩固以上知识点 预备知识： HTML学习手册-Canvas 游戏编程基本概念-Game Loop 在线试玩 Flappy Bird Game Loop Flappy Bird的游戏逻辑设计 home：主页，显示标题和两个button（start、score），目前只实现了start ready：主页点击start进入ready页面，给出游戏提示“tap” play：在ready页面tap后进入游戏页面，包含： 初始化：小鸟、pipe和实时分数 监听用户tap： 没有tap的话有向下加速度，模拟重力 有tap行为则直接给一个向上的速度 记分：小鸟每次经过一个pipe，score加1，score超过best则覆盖best 结束：小鸟和pipe的碰撞检测，碰到上下边缘或者pipe则结束，进入结束页面 over： 结束页面显示记分牌，显示奖牌（是best则金牌，不然白牌），显示score和best 给出ok和menu按钮，其中ok回到ready页，menu回到home页 技术细节 精灵图集 Sprite 精灵图集将游戏中所有的小图片（如小鸟、管道、背景、按钮等）集中在一个大文件里 在网页中获取的时候，可以使用CSS Sprites技术：使用 background-image 和 background-position 两个属性来在网页元素中显示精灵图集上的某一个图标或图片 可以使用 Sprite Cow对精灵图集进行分割处理，得到每个图标的position 注意图标推荐放在div或者span中，而不是img中 如何实现游戏基本逻辑 利用requestAnimationFrame（简单来说，rAF告诉浏览器：“我要执行一个动画，请在下一次重绘（Repaint）之前调用我的回调函数。”），将Game Loop函数作为callback，从而不断刷新游戏状态，实现元素的移动，以及整体Game Loop function GameLoop() { //移动小鸟 ... // 更新游戏分数 updateGameScore(); // 更新pipes的状态 updatePipes(); // 边界碰撞检测 if (birdPosY \u003c 0 || birdPosY + 22 \u003e 400) { console.log(\"Hit Boundary\"); game_state = \"ended\"; } // pipe碰撞检测 ... if (game_state === \"ended\") { ... console.log(\"Game Over!\"); return; } requestAnimationFrame(GameLoop); } 如何实现背景的移动 css中将背景设置为精灵图集中的一部分，且repeat-x，水平循环 #scene { position: relative; /* 改为相对定位，由 flex 控制居中 */ border: 5px solid red; box-sizing: border-box; /* 让 border 包含在宽高内 */ display: flex; align-items: center; justify-content: center; flex-direction: column; background: url('../images/flappy-bird-sprite.png') repeat-x 0 0; image-rendering: pixelated; width: 225px; /* 原始宽度 */ height: 400px; /* 原始高度 */ overflow: hidden; /* 确保内容不溢出游戏画面 */ } 在js中设置scene的backgroundPostion可以整体移动 scene.style.backgroundPosition = `-${backgroundX}px 0px`; 如何实现小鸟的移动 水平方向上不移动，固定在屏幕30%位置上 #bird { position: absolute; background: url('../images/flappy-bird-sprite.png') no-repeat -179px -513px; width: 28px; height: 22px; top: 200px; left: 30%; transform: translateX(-50%); } 垂直方向上，根据游戏逻辑，确定位置后，通过top来实现移动 // 重力加速度下降 birdSpeedY += 0.1; birdPosY = birdPosY + birdSpeedY; bird.style.top = `${birdPosY}px`; 如何实现小鸟扇动翅膀 对于小鸟这个div，根据时间不断切换精灵图集中的backgroundPosition，从而切换不同图片 // 小鸟扇翅膀动画 birdFrameTimer++; if (birdFrameTimer % birdWingRate === 0) { // 每10帧切换一次图片，数字越小越快 birdFrameIndex = (birdFrameIndex + 1) % birdFrames.length; bird.style.backgroundPosition = birdFrames[birdFrameIndex]; if (birdFrameTimer === birdWingRate) { birdFrameTimer = 0; } } 如何实现管道的平移 在html中写三组pipe元素 在js中通过transform水平移动，如果移动超过屏幕，则从最左侧重置到最右侧 // 如果移出屏幕左侧，移动到最右侧 if (pipe.x \u003c -pipeWidth) { // 找到当前最右边的管道的 x 坐标 let maxX = Math.max(...pipesData.map(p =\u003e p.x)); pipe.x = maxX + pipeDistance; // 重新随机高度 pipe.y = Math.floor(Math.random() * (300 - 100)) + 100; pipe.scored = false; } 如何实现页面切换 在html中写在一起 \u003cdiv id=\"scene\"\u003e \u003c!-- 首页场景 --\u003e \u003cdiv id=\"scene_home\"\u003e ... \u003c/div\u003e \u003c!-- 游戏准备场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_ready\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_play\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏结束场景（默认隐藏）--\u003e \u003cdiv id=\"scene_over\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c/div\u003e 然后通过在js中设定某一个部分（div）的display是否为none，来切换显示不同部分 document.getElementById(\"ok_button\").addEventListener('click', function(){ // 隐藏首页场景 document.getElementById(\"scene_over\").style.display = 'none'; // 隐藏游戏层 document.getElementById(\"scene_play\").style.display = 'none'; // 显示游戏场景 document.getElementById(\"scene_ready\").style.display = 'flex'; // 游戏状态进入ready game_state = \"ready\"; }); 代码仓库 github链接： flappy-bird",
    "description": "背景 在完成css、html、javascript的基础上，构建Flappy Bird游戏，以巩固以上知识点 预备知识： HTML学习手册-Canvas 游戏编程基本概念-Game Loop 在线试玩 Flappy Bird Game Loop Flappy Bird的游戏逻辑设计 home：主页，显示标题和两个button（start、score），目前只实现了start ready：主页点击start进入ready页面，给出游戏提示“tap” play：在ready页面tap后进入游戏页面，包含： 初始化：小鸟、pipe和实时分数 监听用户tap： 没有tap的话有向下加速度，模拟重力 有tap行为则直接给一个向上的速度 记分：小鸟每次经过一个pipe，score加1，score超过best则覆盖best 结束：小鸟和pipe的碰撞检测，碰到上下边缘或者pipe则结束，进入结束页面 over： 结束页面显示记分牌，显示奖牌（是best则金牌，不然白牌），显示score和best 给出ok和menu按钮，其中ok回到ready页，menu回到home页 技术细节 精灵图集 Sprite 精灵图集将游戏中所有的小图片（如小鸟、管道、背景、按钮等）集中在一个大文件里 在网页中获取的时候，可以使用CSS Sprites技术：使用 background-image 和 background-position 两个属性来在网页元素中显示精灵图集上的某一个图标或图片 可以使用 Sprite Cow对精灵图集进行分割处理，得到每个图标的position 注意图标推荐放在div或者span中，而不是img中 如何实现游戏基本逻辑 利用requestAnimationFrame（简单来说，rAF告诉浏览器：“我要执行一个动画，请在下一次重绘（Repaint）之前调用我的回调函数。”），将Game Loop函数作为callback，从而不断刷新游戏状态，实现元素的移动，以及整体Game Loop function GameLoop() { //移动小鸟 ... // 更新游戏分数 updateGameScore(); // 更新pipes的状态 updatePipes(); // 边界碰撞检测 if (birdPosY \u003c 0 || birdPosY + 22 \u003e 400) { console.log(\"Hit Boundary\"); game_state = \"ended\"; } // pipe碰撞检测 ... if (game_state === \"ended\") { ... console.log(\"Game Over!\"); return; } requestAnimationFrame(GameLoop); } 如何实现背景的移动 css中将背景设置为精灵图集中的一部分，且repeat-x，水平循环 #scene { position: relative; /* 改为相对定位，由 flex 控制居中 */ border: 5px solid red; box-sizing: border-box; /* 让 border 包含在宽高内 */ display: flex; align-items: center; justify-content: center; flex-direction: column; background: url('../images/flappy-bird-sprite.png') repeat-x 0 0; image-rendering: pixelated; width: 225px; /* 原始宽度 */ height: 400px; /* 原始高度 */ overflow: hidden; /* 确保内容不溢出游戏画面 */ } 在js中设置scene的backgroundPostion可以整体移动 scene.style.backgroundPosition = `-${backgroundX}px 0px`; 如何实现小鸟的移动 水平方向上不移动，固定在屏幕30%位置上 #bird { position: absolute; background: url('../images/flappy-bird-sprite.png') no-repeat -179px -513px; width: 28px; height: 22px; top: 200px; left: 30%; transform: translateX(-50%); } 垂直方向上，根据游戏逻辑，确定位置后，通过top来实现移动 // 重力加速度下降 birdSpeedY += 0.1; birdPosY = birdPosY + birdSpeedY; bird.style.top = `${birdPosY}px`; 如何实现小鸟扇动翅膀 对于小鸟这个div，根据时间不断切换精灵图集中的backgroundPosition，从而切换不同图片 // 小鸟扇翅膀动画 birdFrameTimer++; if (birdFrameTimer % birdWingRate === 0) { // 每10帧切换一次图片，数字越小越快 birdFrameIndex = (birdFrameIndex + 1) % birdFrames.length; bird.style.backgroundPosition = birdFrames[birdFrameIndex]; if (birdFrameTimer === birdWingRate) { birdFrameTimer = 0; } } 如何实现管道的平移 在html中写三组pipe元素 在js中通过transform水平移动，如果移动超过屏幕，则从最左侧重置到最右侧 // 如果移出屏幕左侧，移动到最右侧 if (pipe.x \u003c -pipeWidth) { // 找到当前最右边的管道的 x 坐标 let maxX = Math.max(...pipesData.map(p =\u003e p.x)); pipe.x = maxX + pipeDistance; // 重新随机高度 pipe.y = Math.floor(Math.random() * (300 - 100)) + 100; pipe.scored = false; } 如何实现页面切换 在html中写在一起 \u003cdiv id=\"scene\"\u003e \u003c!-- 首页场景 --\u003e \u003cdiv id=\"scene_home\"\u003e ... \u003c/div\u003e \u003c!-- 游戏准备场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_ready\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_play\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏结束场景（默认隐藏）--\u003e \u003cdiv id=\"scene_over\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c/div\u003e 然后通过在js中设定某一个部分（div）的display是否为none，来切换显示不同部分 document.getElementById(\"ok_button\").addEventListener('click', function(){ // 隐藏首页场景 document.getElementById(\"scene_over\").style.display = 'none'; // 隐藏游戏层 document.getElementById(\"scene_play\").style.display = 'none'; // 显示游戏场景 document.getElementById(\"scene_ready\").style.display = 'flex'; // 游戏状态进入ready game_state = \"ready\"; }); 代码仓库 github链接： flappy-bird",
    "tags": [
      "技术笔记"
    ],
    "title": "从零开始构建Flappy Bird",
    "uri": "/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAflappy-bird/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 完成neovim环境搭建 熟练vim操作 完成Javascript基础知识学习 neovim配置 学习Lua的基础知识： Lua学习手册 配置Neovim环境，安装所需插件： Vim和NeoVim 学习javascript基础知识 Javascript学习手册-基础用法 课程链接 # JavaScript Full Course for free # JavaScript Tutorial Full Course - Beginner to Pro 知识 NeoVim是Vim的新版本重构 通过LazyNvim插件配置 支持语法高亮、语义补全、代码跳转、git等几乎全部所需功能 基本替代vscode，熟练掌握vim操作后，效率提升 html负责内容、css负责样式、javascript负责交互 通过nodejs的live-server可以同步更新目录内的网页状态，类似vscode里的go live插件 node开启javascript命令行 待办 了解Javscript进阶知识：ES6+新特性 Flappy Bird开发",
    "description": "总结 完成neovim环境搭建 熟练vim操作 完成Javascript基础知识学习 neovim配置 学习Lua的基础知识： Lua学习手册 配置Neovim环境，安装所需插件： Vim和NeoVim 学习javascript基础知识 Javascript学习手册-基础用法 课程链接 # JavaScript Full Course for free # JavaScript Tutorial Full Course - Beginner to Pro 知识 NeoVim是Vim的新版本重构 通过LazyNvim插件配置 支持语法高亮、语义补全、代码跳转、git等几乎全部所需功能 基本替代vscode，熟练掌握vim操作后，效率提升 html负责内容、css负责样式、javascript负责交互 通过nodejs的live-server可以同步更新目录内的网页状态，类似vscode里的go live插件 node开启javascript命令行 待办 了解Javscript进阶知识：ES6+新特性 Flappy Bird开发",
    "tags": [
      "周记"
    ],
    "title": "Week5 Javascript学习",
    "uri": "/hugo-blog/weekly/week5/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 优化本地开发环境 tmux配置 Tmux使用教程 aerospace配置 Aerospace使用教程 iterm2配置 Terminal配置 知识 通过tmux解决同一个shell下多开进程的操作方式 tmux分为session、window、pane，每个session下可以快速切换window和pane 通过aerospace可以通过键盘快速切换操作页面，如chrome、terminal、obsidian之间 linux上使用i3，macOS上使用aerospace terminal配置主要包含以下几步 安装terminal 安装color themes 安装字体：支持定制化图标 安装插件：powerlevel10k（未使用）、zsh的语义补全和语法高亮 待办 配置neovim环境 熟练vim操作",
    "description": "总结 优化本地开发环境 tmux配置 Tmux使用教程 aerospace配置 Aerospace使用教程 iterm2配置 Terminal配置 知识 通过tmux解决同一个shell下多开进程的操作方式 tmux分为session、window、pane，每个session下可以快速切换window和pane 通过aerospace可以通过键盘快速切换操作页面，如chrome、terminal、obsidian之间 linux上使用i3，macOS上使用aerospace terminal配置主要包含以下几步 安装terminal 安装color themes 安装字体：支持定制化图标 安装插件：powerlevel10k（未使用）、zsh的语义补全和语法高亮 待办 配置neovim环境 熟练vim操作",
    "tags": [
      "周记"
    ],
    "title": "Week4 开发环境优化",
    "uri": "/hugo-blog/weekly/week4/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 Aerospace是linux系统上i3窗口管理器应用的macOS系统上的替代应用\n支持设置多个workspace，在workspace内管理（平铺或者移动）窗口 支持给每个workspace设定快捷键，进行快速切入 使用说明 配置文件：~/.config/aerospace/aerospace.toml\nmain mode： 设置workspace快捷键：alt+数字/字母 切换窗口：alt+hjkl 移动窗口：alt+shift+hjkl 放大/缩小窗口：alt+shift+（=-） 切换横竖分割：alt+/ 切换横竖全屏：alt+, service mode： reload config：alt+shift+; 相关链接：\n# Aerospace Is The Best Tiling Window Manager I’ve Tried On macOS",
    "description": "背景 Aerospace是linux系统上i3窗口管理器应用的macOS系统上的替代应用\n支持设置多个workspace，在workspace内管理（平铺或者移动）窗口 支持给每个workspace设定快捷键，进行快速切入 使用说明 配置文件：~/.config/aerospace/aerospace.toml\nmain mode： 设置workspace快捷键：alt+数字/字母 切换窗口：alt+hjkl 移动窗口：alt+shift+hjkl 放大/缩小窗口：alt+shift+（=-） 切换横竖分割：alt+/ 切换横竖全屏：alt+, service mode： reload config：alt+shift+; 相关链接：",
    "tags": [],
    "title": "Aerospace使用教程",
    "uri": "/hugo-blog/blogs/aerospace%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "参考链接：\nLearn X in Y minutes # Lua, the simplest language to learn",
    "description": "参考链接：\nLearn X in Y minutes # Lua, the simplest language to learn",
    "tags": [
      "技术笔记"
    ],
    "title": "Lua学习手册",
    "uri": "/hugo-blog/blogs/lua%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "iTerm2 color themes 下载iterm2的色彩主题，可以选择以下颜色组合，搭配neovim使用\n# Iterm2-color-schemes Oh My Zsh 安装oh my zsh，一键配置zsh的.zshrc\ngithub链接 fonts 下载字体以支持一些开发相关的图标\nnerd fonts 插件 语法高亮： zsh-syntax-highlighting 语义补全： zsh-autosuggestions fzf：\nctrl + f：进行目录下的文件查找（目前扩展为在home目录下的dir） ctrl + t：进行目录下的目录查找（同上） ctrl + r：显示history **：会根据当前命令前缀查找候选 cd **：列出目录，仅限当前目录下，和上面的生效区域不同 export **：列出环境变量 ssh **：列出最近访问过的hostname kill -9 **：列出进程 参考链接：\n# How to setup your Mac Terminal to be beautiful",
    "description": "iTerm2 color themes 下载iterm2的色彩主题，可以选择以下颜色组合，搭配neovim使用\n# Iterm2-color-schemes Oh My Zsh 安装oh my zsh，一键配置zsh的.zshrc\ngithub链接 fonts 下载字体以支持一些开发相关的图标",
    "tags": [],
    "title": "Terminal配置",
    "uri": "/hugo-blog/blogs/terminal%E9%85%8D%E7%BD%AE/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "pip 一般通过freeze的命令记录目前全部环境依赖\npip freeze \u003e requirements.txt 其他人可以通过pip的方式一键安装全部环境以来\npip install -r requirements.txt 缺陷 pip install会将库安装到全局目录，多用户共享，导致版本依赖和兼容性问题 pip freeze的问题在于无法明确区分哪些是项目的直接依赖，只是一股脑的记录 pip uninstall后，对应的间接依赖不会被卸载掉 venv 在项目目录里执行，创建.venv虚拟python环境，名称推荐叫.venv，因为vscode等可以自动识别\npython3 -m venv .venv 通过active激活环境，并通过deactivate关闭环境\n// 激活环境 source .venv/bin/activate // 关闭环境 deactivate pyproject.toml 目标就是统一不同的配置文件，把所有与项目构建（打包、依赖管理）和工具配置相关的设置都放在这一个文件里\n完整示例：\n# 指定构建项目的工具 [build-system] requires = [\"setuptools\u003e=45\", \"wheel\"] build-backend = \"setuptools.build_meta\" [project] name = \"my-cli-tool\" version = \"0.1.0\" description = \"一个强大的命令行工具\" authors = [{name = \"王五\", email = \"wangwu@example.com\"}] readme = \"README.md\" license = {text = \"MIT\"} requires-python = \"\u003e=3.8\" # 运行时依赖 dependencies = [ \"requests\u003e=2.25.1\", \"rich\u003e=10.0.0\", ] classifiers = [ \"License :: OSI Approved :: MIT License\", \"Programming Language :: Python :: 3\", ] # 可选依赖 [project.optional-dependencies] dev = [\"pytest\", \"black\", \"flake8\"] # 命令行运行模块函数 [project.scripts] my-tool = \"my_tool.main:cli\" # 工具配置参数 [tool.black] line-length = 88 [tool.pytest.ini_options] addopts = \"-v\" testpaths = [\"tests\"] 通过以下命令可以一键安装\npip install -e . 缺陷 通过venv和pyproject.toml的方式管理和安装库，导致每次安装新库，需要查找对应的版本号，并手动添加到toml配置文件中 UV 为用户封装了管理项目的库安装和配置过程 安装uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh 通过uv add命令替代pip install\nuv add flask 可以对目录下的pyproject.toml进行自动更新 检查并自动创建.venv虚拟环境 将库和所有依赖均安装到.venv环境中 通过sync命令搭建虚拟环境并安装项目全部依赖\nuv sync 通过run命令可以自动找到venv环境，并省略手动activate环境这步，直接运行代码\n# 直接运行 uv run main.py # 使用特定目录作为虚拟环境 uv --python 3.11 run script.py # 使用系统Python uv --system run script.py # 指定虚拟环境路径 uv --with-venv /path/to/venv run script.py 通过tool install命令可以安装虚拟环境外的工具库，整个系统可用\nuv tool install ruff 通过build命令可以进行项目打包成whl文件\nuv build 参考链接：\n# 从pip到uv：一口气梳理现代Python项目管理全流程！ # 用uv管理Python的一切！",
    "description": "pip 一般通过freeze的命令记录目前全部环境依赖\npip freeze \u003e requirements.txt 其他人可以通过pip的方式一键安装全部环境以来\npip install -r requirements.txt 缺陷 pip install会将库安装到全局目录，多用户共享，导致版本依赖和兼容性问题 pip freeze的问题在于无法明确区分哪些是项目的直接依赖，只是一股脑的记录 pip uninstall后，对应的间接依赖不会被卸载掉 venv 在项目目录里执行，创建.venv虚拟python环境，名称推荐叫.venv，因为vscode等可以自动识别\npython3 -m venv .venv 通过active激活环境，并通过deactivate关闭环境\n// 激活环境 source .venv/bin/activate // 关闭环境 deactivate pyproject.toml 目标就是统一不同的配置文件，把所有与项目构建（打包、依赖管理）和工具配置相关的设置都放在这一个文件里",
    "tags": [
      "技术笔记"
    ],
    "title": "Python项目管理",
    "uri": "/hugo-blog/blogs/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Vim Vim分为两个概念，编辑器和motion\n编辑器指ui，功能提供的模块，在neovim中进行插件化配置优化 motion指vim的基础操作命令，这部分在neovim中通用 操作命令 删除：d 修改：c 替换：r 撤销：u 重做：ctrl+r 移动： 按字符移动：上：k；下：j；左：h；右：l 行数+上下：例如向上8行是8k 按词移动，向前到词首：b；向后到下一个词首：w，向后到词尾：e 到行首：0，到行首非空字符：_ ，到行尾：$ 到指定字符： 到下一个指定字符：f，例如ft到下一个t；F到上一个 到下一个指定字符前：t，例如ta到下一个a前；T到上一个 重复到下一个：; 重复到上一个：, 按paragraph移动：上：{；下：}; 按页移动：ctrl+u：向上半页；ctrl+d：向下半页 进入insert mode： 到下一个字符：a 到最后一个字符后面：A 到第一个字符前面：I 新建下一行：o 新建上一行：O 手动缩进：向左：\u003c；向右：\u003e； 可以同时缩进多行，在首行\u003e4j：包括首行一共缩进5行 搜索： /：搜索下一个；？：默认搜索上一个 *：搜索下一个当前光标对应单词；#：搜索上一个当前光标对应单词 光标居中（中间行）：zz text object：动作（v/y/d）+选中类型（i/a）+object类型（w/W/s/p） 例如：viw（选中当前单词）；viW（选中当前连续字符） 将下一行拼接到当前行，用空格分割： J 重复上一次操作：. 重要概念 Text Object 参考链接：\n# Vim As Your Editor # Vim中的重要概念 Text Object NeoVim neovim内置lua引擎，可以用lua编写插件和配置，易读且功能强大 配置文件位置：~/.config/nvim/init.lua 重要概念 LSP 代码编辑过程中，需要和ls（language server）进行交互，来获取当前代码的状态（定义、引用、诊断等），两者之间的交互基于LSP（language server protocol）\nlspconfig 每次编辑器启动ls的时候，需要给出配置（包括什么时候启动、当前语言等），这时候就需要lspconfig插件： nvim-lspconfig\n对每一种语言安装对应的插件，来支持对应语言的lsp # LSP in Neovim (with like 3 lines of code) nvim在0.11之后可以更方便的安装配置lsp # How to Setup Neovim LSP Like A Pro in 2025 (v0.11+) 插件 可以使用lazy.nvim来帮助安装插件，其中重要的包括：\nneo-tree 提供左侧文件目录，可以浏览目录结构，进行文件的增删改 通过ctrl-w + 方向键可以切换目录和文件窗口 在目录中文件上按t可以在新的tab中打开 bufferline 管理buffer的工具，可以方便在buffer之间切换： 上一个buffer：[b 下一个buffer：]b comment 注释toggle： 对于insert模式下，使用gc 对于visual模式下，使用gcc telescope 方便在目录内查找文件，字符串等 leader + ff（find files） 进行文件查找 leader + fg（live grep） 进行关键词查找 treesitter 基于语言将文本进行结构化理解 实现语法高亮，支持text object、incremental selection等能力 text object支持diw（internal delete word）、vap（visual around paragraph）等操作 incremental selection：ctrl+space nvim-cmp 基于treesitter的理解，实现代码补全 另外autopair实现引号等配对补全，autotag实现html标签配对补全 mason mason是一个管理lsp下载、安装的工具 通过ui完成lsp、包括linter、formatter的安装 mason-lspconfig可以帮助nvim找到nvim-lspconfig的配置，传给lsp 最终lsp实现代码的定义、引用等跳转，并可以进行代码错误识别，给出诊断和code action 定义跳转：gd 引用跳转：gR 显示诊断：leader+d（当前行），leader+D（当前文件） lazygit 在nvim中进行git操作，支持add、commit、push等 减少从nvim出来进入命令行git的操作 参考链接： # Lazygit - The Best Way To Use Git On The Terminal \u0026 Neovim 参考链接：\n# The Only Video You Need to Get Started with Neovim # NeoVim 从平凡到非凡 # How I Setup Neovim To Make It AMAZING in 2024: The Ultimate Guide",
    "description": "Vim Vim分为两个概念，编辑器和motion\n编辑器指ui，功能提供的模块，在neovim中进行插件化配置优化 motion指vim的基础操作命令，这部分在neovim中通用 操作命令 删除：d 修改：c 替换：r 撤销：u 重做：ctrl+r 移动： 按字符移动：上：k；下：j；左：h；右：l 行数+上下：例如向上8行是8k 按词移动，向前到词首：b；向后到下一个词首：w，向后到词尾：e 到行首：0，到行首非空字符：_ ，到行尾：$ 到指定字符： 到下一个指定字符：f，例如ft到下一个t；F到上一个 到下一个指定字符前：t，例如ta到下一个a前；T到上一个 重复到下一个：; 重复到上一个：, 按paragraph移动：上：{；下：}; 按页移动：ctrl+u：向上半页；ctrl+d：向下半页 进入insert mode： 到下一个字符：a 到最后一个字符后面：A 到第一个字符前面：I 新建下一行：o 新建上一行：O 手动缩进：向左：\u003c；向右：\u003e； 可以同时缩进多行，在首行\u003e4j：包括首行一共缩进5行 搜索： /：搜索下一个；？：默认搜索上一个 *：搜索下一个当前光标对应单词；#：搜索上一个当前光标对应单词 光标居中（中间行）：zz text object：动作（v/y/d）+选中类型（i/a）+object类型（w/W/s/p） 例如：viw（选中当前单词）；viW（选中当前连续字符） 将下一行拼接到当前行，用空格分割： J 重复上一次操作：. 重要概念 Text Object",
    "tags": [],
    "title": "Vim和NeoVim",
    "uri": "/hugo-blog/blogs/vim%E5%92%8Cneovim/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 全称terminal multiplexer 一个session可以包含多个子进程window，可以切换显示多个子进程 一个window又可以分割成多个分屏pane，方便编辑和后台运行任务 一个session保存了多个子进程信息，关闭terminal后可以重新恢复session 可以使用tmux运行长时间任务，后台运行防止关闭terminal导致杀死进程 可以在不同session、window和pane之间切换 使用说明 命令行操作 tmux：自动创建session tmux new -s 名字：创建命名session tmux ls：列出tmux全部session tmux a -t 数字：进入指定session，如果只有一个session，可以省略-t参数 tmux kill-session -t 数字或名字：退出指定session tmux has-session -t 数字或名字：是否运行着指定session 内部操作 前缀（ctrl+s）+：\nc：新建window 数字：切换window编号 d：退出当前session，但不杀死（后台运行） n/p：切换到下一个/上一个window $：对当前session重命名 ,：对当前window重命名 \u0026：杀死当前session w：查看当前整体window层级结构 /：显示层级结构后，输入斜杠，进行进一步搜索 %：向右分屏 “：向下分屏 方向键：切换分屏 z：全屏/恢复当前分屏 x：关闭分屏（exit），当window的全部分屏退出后，自动关闭session { } ctrl+up ctrl+down：将当前pane移动到左/右/上/下 配置文件 在home目录新建~/.tmux.conf文件：\nset -g mouse on：开启鼠标支持（调整分屏大小） set -g prefix C-s：tmux默认前缀改为control+s bind | split-window -h -c “#{pane_current_path}\"：水平分屏到cwd bind - split-window -v -c “#{pane_current_path}\"：垂直分屏到cwd 启动脚本示例 #!/bin/bash # tmux 开发环境启动脚本 # 检查是否已经在 tmux 会话中 if [ -n \"$TMUX\" ]; then echo \"Error: Already in a tmux session\" exit 1 fi # 检查会话是否已存在 tmux has-session -t dev 2\u003e/dev/null if [ $? != 0 ]; then # 创建新会话 tmux new-session -d -s dev -n \"editor\" # 第一个窗口：代码编辑器 tmux send-keys -t dev:1 \"cd ~/projects/myapp\" Enter tmux send-keys -t dev:1 \"vim\" Enter # 创建第二个窗口：服务器 tmux new-window -t dev:2 -n \"server\" tmux send-keys -t dev:2 \"cd ~/projects/myapp\" Enter tmux send-keys -t dev:2 \"npm run dev\" Enter # 创建第三个窗口：数据库 tmux new-window -t dev:3 -n \"database\" tmux send-keys -t dev:3 \"docker ps\" Enter tmux send-keys -t dev:3 \"docker exec -it postgres psql -U user mydb\" Enter # 创建第四个窗口：日志 tmux new-window -t dev:4 -n \"logs\" tmux send-keys -t dev:4 \"cd ~/projects/myapp/logs\" Enter tmux send-keys -t dev:4 \"tail -f app.log\" Enter # 创建第五个窗口：系统监控 tmux new-window -t dev:5 -n \"monitor\" tmux send-keys -t dev:5 \"htop\" Enter # 水平分割第二个窗口（服务器窗口） tmux split-window -h -t dev:2 tmux send-keys -t dev:2.1 \"cd ~/projects/myapp\" Enter tmux send-keys -t dev:2.1 \"git status\" Enter # 设置初始窗口 tmux select-window -t dev:1 fi # 附加到会话 tmux attach -t dev 参考链接：\n# tmux 使用和基礎配置 從入門到加班 一個視頻全搞定！ # Tmux + Vim 工作流! 同时操作多个项目, 追求极致的. 滑流畅! # Tmux has forever changed the wa. I write code. # you need to learn tmux RIGHT NO. !!",
    "description": "背景 全称terminal multiplexer 一个session可以包含多个子进程window，可以切换显示多个子进程 一个window又可以分割成多个分屏pane，方便编辑和后台运行任务 一个session保存了多个子进程信息，关闭terminal后可以重新恢复session 可以使用tmux运行长时间任务，后台运行防止关闭terminal导致杀死进程 可以在不同session、window和pane之间切换 使用说明 命令行操作 tmux：自动创建session tmux new -s 名字：创建命名session tmux ls：列出tmux全部session tmux a -t 数字：进入指定session，如果只有一个session，可以省略-t参数 tmux kill-session -t 数字或名字：退出指定session tmux has-session -t 数字或名字：是否运行着指定session 内部操作 前缀（ctrl+s）+：",
    "tags": [],
    "title": "Tmux使用教程",
    "uri": "/hugo-blog/blogs/tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Zapier官网 自动化流程配置工具，将你的多种不同的app的信息串联起来 gmail邮箱收到邮件，在whatsapp上设置bot提醒 stripe收到货款，导入google sheet新建一行 等等 支持AI辅助新建工作流 参考链接：\n# Zapier AI Tutorial for Beginners: Automation Made Simple 🟧",
    "description": "Zapier官网 自动化流程配置工具，将你的多种不同的app的信息串联起来 gmail邮箱收到邮件，在whatsapp上设置bot提醒 stripe收到货款，导入google sheet新建一行 等等 支持AI辅助新建工作流 参考链接：\n# Zapier AI Tutorial for Beginners: Automation Made Simple 🟧",
    "tags": [],
    "title": "自动化流程平台Zapier",
    "uri": "/hugo-blog/blogs/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B%E5%B9%B3%E5%8F%B0zapier/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "这里汇总了不同的内容（主要是文档，如obsidian和notion中）整理方式\nACE Atlas：与时间无关的想法，笔记 Calendar：按时间例行化的内容，如日记、周记 Efforts：项目进度 参考链接：\n# Create Your Digital Home: Obsidian Walkthrough PARA Projects：项目进展，有时间节点限制，有任务要求 Areas：某一方面的计划和记录，不像projects那样有任物属性（时间节点或ddl），比如健康，某个领域的学习进展等 Resources：可以理解成与时间无关的ideas，例如blog、来自某本书的quote等 Archives：过时的内容，暂存 参考链接：\n# Organize Your ENTIRE Digital Life in Seconds (The PARA Method)",
    "description": "这里汇总了不同的内容（主要是文档，如obsidian和notion中）整理方式\nACE Atlas：与时间无关的想法，笔记 Calendar：按时间例行化的内容，如日记、周记 Efforts：项目进度 参考链接：\n# Create Your Digital Home: Obsidian Walkthrough PARA Projects：项目进展，有时间节点限制，有任务要求 Areas：某一方面的计划和记录，不像projects那样有任物属性（时间节点或ddl），比如健康，某个领域的学习进展等 Resources：可以理解成与时间无关的ideas，例如blog、来自某本书的quote等 Archives：过时的内容，暂存 参考链接：\n# Organize Your ENTIRE Digital Life in Seconds (The PARA Method)",
    "tags": [],
    "title": "内容整理方式",
    "uri": "/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E6%95%B4%E7%90%86%E6%96%B9%E5%BC%8F/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "内容管理系统（CMS）方便记录各种信息，包括文章、想法、日程、计划等\nObsidian 优点 可以对笔记及内部元素进行双向连接，从而形成关系图谱 支持插件，包括built-in和自定义，例如图片上传等 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持高级markdown格式，如callout等 数据本地化，方便离线操作 缺点 相比notion，缺少日程表类似的功能 不支持AI功能 参考链接：\n# 39分鐘上手Obsidian！基礎操作介紹（電腦、平板、手機全面教學） # Give Me 15 Minutes. I’ll Teach You 80% of Obsidian Notion 优点 页面UI丰富，支持各种拖拽、增减等操作 方便地进行任务规划，例如对database进行table或calendar格式的view 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持丰富的页面可视化，例如icon等 支持团队协作编辑文档，可以给外部分享并评论 目前notion 3.0提供 Notion Agent的AI功能 缺点 对于纯粹的内容管理系统而言，功能冗余 对于每日行程规律的人友好，但反之就会有很多冗余操作 页面之间的关系是上下级，缺乏obsidian点对点的联系 可以通过@方式实现页面的跳转，但是不是概念（单词）粒度的联系 参考链接：\n# 全世界在学的软件，到底怎么用？Notion十分钟入门指南。 # How to Get Started with Notion (without losing your mind) Notion Agent 也称为notion 3.0，1.0为document，2.0为database，3.0加入AI agent 通过添加不同的agent为你工作，他们的工作包括：\n创建、查询和编辑database 创作和更新内容 在Notion、外部工具（如gmail、slack等）和网络中查询分析信息 参考链接：\n# Getting started with Notion Agent # Notion’s AI Agent is a Game-Changer (Notion made EASY!)",
    "description": "内容管理系统（CMS）方便记录各种信息，包括文章、想法、日程、计划等\nObsidian 优点 可以对笔记及内部元素进行双向连接，从而形成关系图谱 支持插件，包括built-in和自定义，例如图片上传等 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持高级markdown格式，如callout等 数据本地化，方便离线操作 缺点 相比notion，缺少日程表类似的功能 不支持AI功能 参考链接：\n# 39分鐘上手Obsidian！基礎操作介紹（電腦、平板、手機全面教學） # Give Me 15 Minutes. I’ll Teach You 80% of Obsidian Notion 优点 页面UI丰富，支持各种拖拽、增减等操作 方便地进行任务规划，例如对database进行table或calendar格式的view 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持丰富的页面可视化，例如icon等 支持团队协作编辑文档，可以给外部分享并评论 目前notion 3.0提供 Notion Agent的AI功能 缺点 对于纯粹的内容管理系统而言，功能冗余 对于每日行程规律的人友好，但反之就会有很多冗余操作 页面之间的关系是上下级，缺乏obsidian点对点的联系 可以通过@方式实现页面的跳转，但是不是概念（单词）粒度的联系 参考链接：",
    "tags": [],
    "title": "内容管理系统",
    "uri": "/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 学习notion基本功能，并基于notion搭建个人博客 学习css、html、javascript基础知识，搭建简单的github.io主页 基于notion + notion next + vercel搭建个人博客 调研notion建站方案 原生publish：直接点击页面的publish，notion原生样式 开源notion next： 通过next js实现notion内容拉取和前端渲染，可以定制化页面样式 参考链接 notion next fork仓库后可以修改配置 复制notion next作者的模版页面到自己的notion主页中，编辑内容进行博客编写 官网 vercel负责托管部署 关联github账号，import上述仓库，填写NOTION_PAGE_ID，部署 配置域名等与github pages类似 博客链接 学习css、html基础知识并简单搭建github.io css \u0026 html课程学习 HTML学习手册-基础概念 CSS学习手册-基础用法 课程链接 # 成為網頁設計師的第一步！快速上手 HTML \u0026 CSS 展開你的網頁設計之旅！ # HTML \u0026 CSS Full Course for free # HTML \u0026 CSS Full Course - Beginner to Pro 搭建简单静态网页 github.io 学习javascript基础知识 javascript课程学习 课程链接 # JavaScript 快速上手！用一個實戰範例迅速掌握所有重點語法！ 知识 作为CMS，notion更偏向于项目管理，而obsidian更偏向于文档记录 在html的body中加入script可以插入js代码，页面f12可以看到console.log的打印 通过在元素上添加listener来捕捉用户行为（点击、输入等），实现相应逻辑（如添加表单元素等） 待办 javascript基础知识 javascript进阶：ES6+新特性",
    "description": "总结 学习notion基本功能，并基于notion搭建个人博客 学习css、html、javascript基础知识，搭建简单的github.io主页 基于notion + notion next + vercel搭建个人博客 调研notion建站方案 原生publish：直接点击页面的publish，notion原生样式 开源notion next： 通过next js实现notion内容拉取和前端渲染，可以定制化页面样式 参考链接 notion next fork仓库后可以修改配置 复制notion next作者的模版页面到自己的notion主页中，编辑内容进行博客编写 官网 vercel负责托管部署 关联github账号，import上述仓库，填写NOTION_PAGE_ID，部署 配置域名等与github pages类似 博客链接 学习css、html基础知识并简单搭建github.io css \u0026 html课程学习 HTML学习手册-基础概念 CSS学习手册-基础用法 课程链接 # 成為網頁設計師的第一步！快速上手 HTML \u0026 CSS 展開你的網頁設計之旅！ # HTML \u0026 CSS Full Course for free # HTML \u0026 CSS Full Course - Beginner to Pro 搭建简单静态网页 github.io 学习javascript基础知识 javascript课程学习 课程链接 # JavaScript 快速上手！用一個實戰範例迅速掌握所有重點語法！ 知识 作为CMS，notion更偏向于项目管理，而obsidian更偏向于文档记录 在html的body中加入script可以插入js代码，页面f12可以看到console.log的打印 通过在元素上添加listener来捕捉用户行为（点击、输入等），实现相应逻辑（如添加表单元素等） 待办 javascript基础知识 javascript进阶：ES6+新特性",
    "tags": [
      "周记"
    ],
    "title": "Week3 个人博客搭建",
    "uri": "/hugo-blog/weekly/week3/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "OSI模型是计算机通信网络的抽象模型",
    "description": "OSI模型是计算机通信网络的抽象模型",
    "tags": [
      "技术笔记"
    ],
    "title": "OSI模型",
    "uri": "/hugo-blog/blogs/osi%E6%A8%A1%E5%9E%8B/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log \u003e First Day",
    "content": "first day things",
    "description": "first day things",
    "tags": [],
    "title": "First Day Things",
    "uri": "/hugo-blog/log/first-day/first-day-things/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log",
    "content": "hello from third day",
    "description": "hello from third day",
    "tags": [],
    "title": "Third Day",
    "uri": "/hugo-blog/log/third-day/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log",
    "content": "hello from second day",
    "description": "hello from second day",
    "tags": [],
    "title": "Second Day",
    "uri": "/hugo-blog/log/second-day/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log",
    "content": "hello from first day",
    "description": "hello from first day",
    "tags": [],
    "title": "First Day",
    "uri": "/hugo-blog/log/first-day/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 域名注册 \u0026 图床搭建 尝试利用现有平台进行个人博客的搭建部署，不涉及具体框架或代码开发 学习obsidian基本功能 cloudflare域名注册 ruoruoliu.com，每年10刀左右 cloudflare账户 基于clodflare R2 + piclist搭建免费图床 由于picgo的S3插件上传有问题，替换为piclist 参考链接 基于hexo + github pages搭建个人博客 hexo主要是基于模版的页面生成和部署工具，可以命令行生成并部署到github pages中 每次修改source中的md文件，hexo在generate过程中更新public中对应的部分 github.io作为个人主页，对于仓库名有要求：ruoruoliu.github.io 页面搭建在github pages中的项目页面中： 博客链接 是否需要nginx解决网址栏输入最后没有/导致访问不同的问题？ 参考链接 基于obsidian + hugo + github pages搭建个人博客 hugo将md文件转化为html页面，并进行部署 调研hugo的theme，包括paper、paperMod、terminal和relearn 后续可以在博客更新过程中不断学习relearn基本功能： 主题官网 Obsidian编写md文件，hugo是和hexo类似的cms工具 处理obsidian的内部链接，转化为hugo可接受链接\n使用obsidian插件“image auto upload”，插入图片时自动上传图床\n通过obsidian插件“shell commands”一键同步文件到本地github目录： 参考链接， 插件连接，再push到github远程仓库，触发github pages更新部署\n整体仓库push到main 网页代码通过”git subtree“ push到gh-pages分支 加入obsidian内部链接转化为markdown格式的处理 页面搭建在github pages中的项目页面中： 博客链接\n知识 个人博客平台化搭建范式 内容编辑：通常是md格式，可以基于obsidian、notion等 内容到页面转化工具：hexo、hugo、nextjs等 服务托管平台：github pages、cloudflare pages、vercel等 待办 学习notion基本功能，并基于notion搭建个人博客 学习css、html、javascript基础知识",
    "description": "总结 域名注册 \u0026 图床搭建 尝试利用现有平台进行个人博客的搭建部署，不涉及具体框架或代码开发 学习obsidian基本功能 cloudflare域名注册 ruoruoliu.com，每年10刀左右 cloudflare账户 基于clodflare R2 + piclist搭建免费图床 由于picgo的S3插件上传有问题，替换为piclist 参考链接 基于hexo + github pages搭建个人博客 hexo主要是基于模版的页面生成和部署工具，可以命令行生成并部署到github pages中 每次修改source中的md文件，hexo在generate过程中更新public中对应的部分 github.io作为个人主页，对于仓库名有要求：ruoruoliu.github.io 页面搭建在github pages中的项目页面中： 博客链接 是否需要nginx解决网址栏输入最后没有/导致访问不同的问题？ 参考链接 基于obsidian + hugo + github pages搭建个人博客 hugo将md文件转化为html页面，并进行部署 调研hugo的theme，包括paper、paperMod、terminal和relearn 后续可以在博客更新过程中不断学习relearn基本功能： 主题官网 Obsidian编写md文件，hugo是和hexo类似的cms工具 处理obsidian的内部链接，转化为hugo可接受链接\n使用obsidian插件“image auto upload”，插入图片时自动上传图床\n通过obsidian插件“shell commands”一键同步文件到本地github目录： 参考链接， 插件连接，再push到github远程仓库，触发github pages更新部署",
    "tags": [
      "周记"
    ],
    "title": "Week2 个人博客搭建",
    "uri": "/hugo-blog/weekly/week2/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 完成科学上网环境搭建 完成远程开发环境搭建 科学上网配置 购买vps节点： 尝试了vultr和bandwagon两个服务商 vultr按小时收费，最低每月5刀，搬瓦工按quarter收费，三个月50刀 节点选择： 考虑是否支持运营商vip骨干网络，比如移动为cmi 搬瓦工可以使用日本节点，延迟控制在100ms以内，vscode响应时间可接受 参考链接 vscode remote配置 github copilot配置 免费试用 pro 1个月，后续切换到 free plan 后续可以尝试cursor以及claude code 知识 科学上网流程 购买vps节点 vps节点安装并配置代理客户端（s-ui）的tls 本地v2ray配置vps的tls 计算机网络 OSI模型 计算机网络通讯的【系统性】扫盲——从“基本概念”到“OSI 模型” 待办 搭建个人博客，方便后续记笔记",
    "description": "总结 完成科学上网环境搭建 完成远程开发环境搭建 科学上网配置 购买vps节点： 尝试了vultr和bandwagon两个服务商 vultr按小时收费，最低每月5刀，搬瓦工按quarter收费，三个月50刀 节点选择： 考虑是否支持运营商vip骨干网络，比如移动为cmi 搬瓦工可以使用日本节点，延迟控制在100ms以内，vscode响应时间可接受 参考链接 vscode remote配置 github copilot配置 免费试用 pro 1个月，后续切换到 free plan 后续可以尝试cursor以及claude code 知识 科学上网流程 购买vps节点 vps节点安装并配置代理客户端（s-ui）的tls 本地v2ray配置vps的tls 计算机网络 OSI模型 计算机网络通讯的【系统性】扫盲——从“基本概念”到“OSI 模型” 待办 搭建个人博客，方便后续记笔记",
    "tags": [
      "周记"
    ],
    "title": "Week1 开发环境配置",
    "uri": "/hugo-blog/weekly/week1/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/hugo-blog/categories/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Projects",
    "uri": "/hugo-blog/projects/index.html"
  }
]
