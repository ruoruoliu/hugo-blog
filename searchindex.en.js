var relearn_searchindex = [
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "This is a log.",
    "description": "This is a log.",
    "tags": [],
    "title": "Log",
    "uri": "/hugo-blog/log/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/hugo-blog/tags/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM和agent的训练 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\\mu$ 和 $\\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 待办 强化学习技术实战",
    "description": "总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM和agent的训练 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\\mu$ 和 $\\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 待办 强化学习技术实战",
    "tags": [
      "周记"
    ],
    "title": "Week12 强化学习技术跟进",
    "uri": "/hugo-blog/weekly/week12/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Weeklies",
    "uri": "/hugo-blog/weekly/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: 周记",
    "uri": "/hugo-blog/tags/%E5%91%A8%E8%AE%B0/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Blogs",
    "uri": "/hugo-blog/blogs/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction \u0026 Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation \u0026 Exploration 参考链接:\nDeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：\nOnline Network（在线网络 / 评估网络）：负责产生即时的 $Q(s, a)$，并进行反向传播更新参数，每一步都在更新 Target Network（目标网络）：专门用来计算目标值（TD Target）中的 $Q(s’, a’)$ 部分，不经过梯度更新，而是每隔一段时间（比如1000步），从Online Network拷贝参数 $Q_{online}(s, a) \\leftarrow R + \\gamma Q_{target}(s’, \\arg\\max_{a’} Q_{target}(s’, a’))$\n参考链接：\nHuman-level control through deep reinforcement learning Double DQN DQN采用Bootstrapping估计Q，而Bootstrapping与Q-learning中的最大化操作会导致对Q的高估：\n在训练初期或由于函数近似的不精确，预估通常包含噪声，而max操作会自动选择噪声并将包含噪声的Q+noise作为更新目标 Bootstrapping用高估的目标去估计下一个目标，导致高估被传递并放大 高估问题对当前state的不同action是不均匀的，因为每次数据采样到就会被高估一次，采样多的 $（s_t, a_t）$ 就会被高估的严重，导致最终最优policy有问题\n为了缓解DQN的高估问题，Double DQN将选action和计算action的value拆分为两部分，分别交给Online Network和Target Network完成：\nOnline Network：负责选动作 Target Network：负责估计动作的value 这样即便由于高估选择了一个错误的动作，在计算value的时候，由于是另一个网络，仍然会给他较低的分数，防止高估传递下去\n$Q_{online}(s, a) \\leftarrow R + \\gamma Q_{target}(s’, \\arg\\max_{a’} Q_{online}(s’, a’))$\n参考链接：\nDeep Reinforcement Learning with Double Q-learning Dueling Network 在Dueling Network中，DQN中的Q被分成了两部分，$Q(s, a) = V(s) + A(s, a)$：\n状态价值函数 $V(s)$：评估当前state本身有多好 优势函数 $A(s, a)$：评估在当前state下，选择某个action比平均水平好多少 为了让优势函数A的均值为0，强制V分支去捕捉状态的平均价值，采用对A去中心化的方法： $Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{n} \\sum_{a’} A(s, a’) \\right)$\n参考链接：\nDueling Network Architectures for Deep Reinforcement Learning Experience Replay 经验回放有两个主要优点：\n避免训练数据($s_t, a_t, r_t, s_{t+1}$)的浪费，可以重复使用 避免训练过程中采样的同分布，即相邻序列在训练过程中相邻，防止过拟合 经验回放引入了off-policy，即训练数据的策略（旧）已经不是当前学习的策略（新），因此对于policy-based的算法，需要重要性采样（Importance Sampling）进行修正 Policy-Based在经验回放时使用重要性采样修正的必要性\nPrioritized Experience Replay 用非均匀采样代替均匀采样，对于TD error（$\\delta_t$）较大的样本采样权重更大：\n基于TD error：$p_t \\propto |\\delta_t| + \\epsilon$ 基于TD error的排序：$p_t \\propto \\frac{1}{\\text{rank}(t)}$ Prioritized Experience Replay中的采样效率问题 由于采样权重导致数据分布发生变化（参考off policy），需要对学习率进行调整，即采样权重大的样本，学习率调小：$w_i = \\left( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\right)^\\beta$，其中在均匀采样中 $P(i)=\\frac{1}{N}$，不影响学习率 DQN在PER时使用重要性采样修正的必要性\n参考链接：\nPRIORITIZED EXPERIENCE REPLAY Policy-based Actor-Critic TRPO TRPO的背景在于RL中梯度更新的难度远比监督学习大，按监督学习的learning rate方案极不稳定： RL与监督学习在SGD上的差异\n置信域算法（Trust Region Methods）分为两个阶段：\nApproximation：在当前点 $x_k$ 附近，用一个简单的数学模型来代替复杂的原始函数 $f(x)$ Maximization：在信任范围内（满足单调性），找到能让模型下降（或增益极大化）的最佳位移 $p$ 置信域算法通过自适应方法来确定置信域区间，从而在置信域区间进行迭代求解原函数最小值： $$\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}$$ 其中：\n当 $\\rho_k$ 接近1时，近似非常准确，优化阶段可以更激进，增大 $\\Delta_k$ 当 $\\rho_k$ 接近0或负数时，近似完全失效，缩小 $\\Delta_k$，重新进行近似 当 $\\rho_k$ 适中时，近似尚可，保持现状 什么是置信域算法（Trust Region Methods）？\nTRPO不直接约束参数空间的步长，而是约束策略空间的距离（新旧policy之间的KL 散度），确保了新旧policy之间的差距在“置信域”内，尽可能取最大的有效步长，避免了手动设置学习率，使训练收敛更稳定： $$\\max_{\\theta} E_{s \\sim \\rho_{\\theta_{old}}, a \\sim \\pi_{\\theta_{old}}} \\left[ \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)} A_{\\theta_{old}}(s,a) \\right]$$\n$$\\text{subject to } E_{s \\sim \\rho_{\\theta_{old}}} [D_{KL}(\\pi_{\\theta_{old}}(\\cdot|s) || \\pi_{\\theta}(\\cdot|s))] \\le \\delta$$ 这转化为一个约束最优化问题的求解，使用泰勒展开：\n对目标函数进行一阶展开（得到梯度 $g$） 对约束条件进行二阶展开（得到费雪信息矩阵 $H$，也叫Hessian矩阵） 最后推导出来的参数更新位置公式为： $$\\Delta \\theta \\approx \\sqrt{\\frac{2\\delta}{g^T H^{-1} g}} H^{-1} g$$ 直接把 $\\theta$ 推到了那个理论上在约束范围内最完美的位置 参考链接：\nTrust Region Policy Optimization # TRPO 置信域策略优化 (Trust Region Policy Optimization) PPO PPO的背景在于TRPO中 $H^{-1}g$ 计算量巨大，于是简化约束条件KL散度，而采用截断的方式来控制新旧策略的差异，假设新旧策略差异为： $$r_t(\\theta) = \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{old}}(a|s)}$$ 截断损失函数为： $$L^{CLIP} = \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right)$$ 假设 $\\epsilon=0.2$，有以下几种情况：\n$r_t(\\theta) \u003c 0.8$：新策略action概率变得过小 如果 $\\hat{A}_t \u003e 0$（action是好动作）：min取真实$r_t(\\theta)$，即修正错误（变大）不设限 如果 $\\hat{A}_t \u003c 0$（action是坏动作）：min取0.8，梯度停止更新，即不会继续让action概率更小 $r_t(\\theta) \u003e 1.2$：新策略action概率变得过大 如果 $\\hat{A}_t \u003e 0$（action是好动作）：min取1.2，梯度停止更新，即不会继续让action概率更大 如果 $\\hat{A}_t \u003c 0$（action是坏动作）：min取真实$r_t(\\theta)$，即修正错误（变小）不设限 $0.8 \u003c r_t(\\theta) \u003c 1.2$：新策略action小幅变化，clip不生效，取真实$r_t(\\theta)$ 如果 $\\hat{A}_t \u003e 0$（action是好动作）：继续梯度更新，让action概率变大 如果 $\\hat{A}_t \u003c 0$（action是坏动作）：继续梯度更新，让action概率变小 PPO两大工程优势：\n防止策略崩溃：即使你把 Learning Rate 设得稍微大了一点，clip也会挡住那些过度的更新 数据高效利用：只要 $r_t$ 还没被clip，这批数据就可以反复用来跑好几次 SGD PPO中采样数据被高效利用的原因 PPO中Clip操作对Bias和Variance的Trade-Off\n参考链接：\nProximal Policy Optimization Algorithms DDPG 离散控制与连续控制：\n离散动作空间只包含有限个动作，比如grid中的四个方向的运动 连续动作空间的动作是连续的，比如机械手转动的角度，属于一个连续范围 对连续动作离散化可以近似解决，但是会有维度爆炸的问题，所以只适用于维度小的问题 参考链接：\n# 离散控制与连续控制 (连续控制 1/3)) 由于连续动作空间中，action是一个数值变量：\n如果还按离散控制的方式，采样来计算梯度，会发现由于维度爆炸，采样数据稀疏导致极不稳定 action是数值变量意味着可以求导，因此可以通过链式法则，直接对 $\\theta$ 求导： $$\\mathbf{g} = \\frac{\\partial q(s, \\pi(s; \\boldsymbol{\\theta}); \\mathbf{w})}{\\partial \\boldsymbol{\\theta}} = \\frac{\\partial a}{\\partial \\boldsymbol{\\theta}} \\cdot \\frac{\\partial q(s, a; \\mathbf{w})}{\\partial a}$$ DPG中由于critic和actor的梯度更新都依赖于critic的 $Q$ ，会导致不稳定相比离散控制更强烈：\ntarget network：引入target actor和target critic 参数软更新： target网络的参数 $w^-$ 以极小的比例 $\\tau$（如 0.001）缓慢跟随主网络 参考链接：\nDeterministic Policy Gradient Algorithms CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING # 确定策略梯度 Deterministic Policy Gradient, DPG (连续控制 2/3) SAC SAC通过随机策略来解决连续控制问题，预测action的均值 $\\mu$ 和方差 $\\sigma$，将确定性action转化为高斯分布，从而最大化动作的累积收益\n在 SAC 的目标函数中，除了奖励 $r$ 之外，还最大化熵 ($H$)： $$J(\\pi) = \\sum_{t=0}^{T} E_{(s_t, a_t) \\sim \\rho_\\pi} [r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t))]$$\n防止模型过早陷入局部最优解，鼓励去尝试不同的动作 随机策略可以学到多种种同样好的解法，如果环境发生轻微扰动，表现得更稳健 连续控制中使用 $\\nabla_{\\theta} J(\\theta) = \\mathbb{E}{a \\sim \\pi{\\theta}} [ \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s) \\cdot Q(s, a) ]$ 求解策略梯度方差极大，因此SAC采用了和DDPG同样的确定性梯度的路径（链式法则）；由于action不是数值变量，而是从分布中采样得到的，因此需要重参数化的技巧，将随机采样的过程转化为一个确定的函数： $$a = f_{\\theta}(s, \\epsilon) = \\mu_{\\theta}(s) + \\sigma_{\\theta}(s) \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)$$\nSAC单峰假设遇到多峰情况时如何解决？\n参考链接：\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor # 随机策略做连续控制 (连续控制 3/3) Exploitation \u0026 Exploration #todo 看下面这篇blog\n参考链接：\n# Exploration Strategies in Deep Reinforcement Learning 应用 Gymnasium 官网链接\nTianShou Verl Sim2Real AlphaGo MuZero 参考链接：\n# MuZero: Mastering Go, chess, shogi and Atari without rules Dreamer 参考链接：\n# Introducing Dreamer: Scalable Reinforcement Learning Using World Models # Mastering Atari with Discrete World Models # Mastering Diverse Control Tasks through World Models # Training Agents Inside of Scalable World Models 相关领域 #todo 动手学强化学习这些章节\nImitation Learning Model Predictive Control Offline RL Multi-agent RL 多agent之间的关系分为以下几种：\n完全合作（fully cooperative）：多agent目标相同 完全竞争（fully competitive）：零和博弈 合作/竞争混合（mixed cooperative \u0026 competitive）：自己团队内部为合作，对方团队为竞争 利己主义（self-interested）：只考虑自己，不在乎对方 多agent策略收敛：\n纳什均衡时策略收敛，即当一个agent改变策略而其他agent不变时，他的return不会变好 除非所有agent彼此独立，才能用单agent方法求解最优策略 单agent方法有可能无法收敛，因为改变自己参数时，会改变其他agent的目标 多agent（合作）需要通过通信共享信息从而达到最优策略的收敛 多agent的通信方式与训练：\n完全去中心化（fully decentralized）：多agent不通信 agent视角与单agent完全一样，最终可能无法收敛 完全中心化（fully centralized）：agent将信息发送给controller，controller为所有agent决策 由于controller的决策需要用到所有agent的观测，因此不能在agent上独立执行 可以把多agent理解为一个大的agent，汇合了所有agent的state、action和reward 同步比较耗时，很难做到实时性 中心训练、去中心执行：训练时使用controller，执行时不使用 controller根据全部的信息为每个agent训练专属的critic网络 每个agent根据controller的critic训练自己的actor，然后在执行中使用 参考链接：\n# 多智能体强化学习(1/2)：基本概念 Multi-Agent Reinforcement Learning # 多智能体强化学习(2/2)：三种架构 Multi-Agent Reinforcement Learning 参考链接：\n# The FASTEST introduction to Reinforcement Learning on the internet 动手学强化学习 # 深度强化学习基础【王树森】 OpenAI Spinning Up",
    "description": "基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction \u0026 Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation \u0026 Exploration 参考链接:\nDeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：",
    "tags": [
      "技术笔记"
    ],
    "title": "Reinforcement Learning学习手册",
    "uri": "/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: 技术笔记",
    "uri": "/hugo-blog/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 强化学习基础知识 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Prediction DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Control DeepMind x UCL Introduction to RL 2015 课程笔记-Value Function Approximation DeepMind x UCL Introduction to RL 2015 课程笔记-Policy Gradient DeepMind x UCL Introduction to RL 2015 课程笔记-Integrating Learning and Planning DeepMind x UCL Introduction to RL 2015 课程笔记-Exploration and Exploitation 知识 当我们不知道环境的具体信息（model）时，如何进行策略的评估（prediction）和最优策略的选择（control），称为model-free RL model-free主要基于采样，即在环境中通过采样拿到真实reward，来迭代value function，从而评价state/action的好坏 采样完整序列的reward的方式称为Monte-Carlo采样，采样下一步然后基于下一步的value function进行bootstrapping的方式称为TD learning；进一步地，通过TD($\\lambda$)可以在MC和TD之间平滑，效果一般更优 选择最优策略的方式一般是基于评估后通过greedy或者$\\epsilon-greedy$的方式选取action；如果行为策略和目标策略相同，称为on-policy，如果不同，称为off-policy TD learning + $\\epsilon-greedy$ = SARSA（on-policy） TD learning + max = Q-learning（off-policy） 简单问题可以通过查表完成，但是真实环境的RL问题状态空间通常很大，需要使用Value Function Approximator来表示，一般可以理解为神经网络的近似，比如DQN policy gradient的方法直接计算最优策略，避免了value-based的max操作，具有更好的收敛性质 可以通过采样reward结合策略梯度的方式，计算最优策略对应的action的分布 actor-critic通过同时优化evaluation（critic）和improvement（actor）减小variance 另外我们可以先通过采样求解model，把问题转化为model-based RL，再求解策略 同时结合model-based以及model-free来充分的压榨样本数据，比如Dyna-Q 基于价值函数进行动作采样，基于model生成sub MDP，再通过model-free求解，比如MCTS 利用和探索是希望我们在”最大化即时收益“和”获取新信息“之间做出权衡，人们通过多臂赌博机问题发展出很多策略：包括UCB和Thompson Sampling，且这些策略均可以在MDP上应用 待办 强化学习基础知识 跟进大模型强化学习技术",
    "description": "总结 强化学习基础知识 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Prediction DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Control DeepMind x UCL Introduction to RL 2015 课程笔记-Value Function Approximation DeepMind x UCL Introduction to RL 2015 课程笔记-Policy Gradient DeepMind x UCL Introduction to RL 2015 课程笔记-Integrating Learning and Planning DeepMind x UCL Introduction to RL 2015 课程笔记-Exploration and Exploitation 知识 当我们不知道环境的具体信息（model）时，如何进行策略的评估（prediction）和最优策略的选择（control），称为model-free RL model-free主要基于采样，即在环境中通过采样拿到真实reward，来迭代value function，从而评价state/action的好坏 采样完整序列的reward的方式称为Monte-Carlo采样，采样下一步然后基于下一步的value function进行bootstrapping的方式称为TD learning；进一步地，通过TD($\\lambda$)可以在MC和TD之间平滑，效果一般更优 选择最优策略的方式一般是基于评估后通过greedy或者$\\epsilon-greedy$的方式选取action；如果行为策略和目标策略相同，称为on-policy，如果不同，称为off-policy TD learning + $\\epsilon-greedy$ = SARSA（on-policy） TD learning + max = Q-learning（off-policy） 简单问题可以通过查表完成，但是真实环境的RL问题状态空间通常很大，需要使用Value Function Approximator来表示，一般可以理解为神经网络的近似，比如DQN policy gradient的方法直接计算最优策略，避免了value-based的max操作，具有更好的收敛性质 可以通过采样reward结合策略梯度的方式，计算最优策略对应的action的分布 actor-critic通过同时优化evaluation（critic）和improvement（actor）减小variance 另外我们可以先通过采样求解model，把问题转化为model-based RL，再求解策略 同时结合model-based以及model-free来充分的压榨样本数据，比如Dyna-Q 基于价值函数进行动作采样，基于model生成sub MDP，再通过model-free求解，比如MCTS 利用和探索是希望我们在”最大化即时收益“和”获取新信息“之间做出权衡，人们通过多臂赌博机问题发展出很多策略：包括UCB和Thompson Sampling，且这些策略均可以在MDP上应用 待办 强化学习基础知识 跟进大模型强化学习技术",
    "tags": [
      "周记"
    ],
    "title": "Week11 强化学习基础知识",
    "uri": "/hugo-blog/weekly/week11/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Intro 强化学习与机器学习的差别：\n没有supervisor，只有reward 反馈是滞后的，不是实时的 时间序列，每一个时间步不是i.i.d的 agent的行为会影响后续数据 reward是什么：\n一个标量的反馈信号 表示agent在t时刻的表现 agent的目标是最大化累积reward 序列决策是什么：\n目标是选择action，最大化累积reward action有长期的影响，因为reward是滞后的，意味着可能需要牺牲短期reward来获取长期reward 序列H包含observation、action、reward的循环：O1、A1、R1、O2、A2、R2、… state是什么：\nstate是序列的函数，即当前的状态包含了观察、行为、奖励序列的全部信息 environment state是指环境的内部状态表示，通常是agent不可见的；即使可见，也通常包含一些不相关的噪音 agent state是agent的内部状态表示，作为下一次action选择的输入，也可理解为RL算法的输入 markov state是指包含之前全部信息的当前状态，只依赖于当前状态，与之前状态独立 在完全可观测环境中，agent state等于environment state，构成MDP（markov decision process）；在部分可观测环境中，agent state不等于environment state，构成POMDP（partially observable markov decision process） agent agent的组成：\npolicy：agent的行为函数，是从state到action的map，可以是确定的，也可以是概率分布 value function：agent基于某个policy下，对于当前状态的未来累积价值评估 model主要有两种： transition model：agent基于当前状态和行为，对于环境下一时刻状态的预测 reward model：agent基于当前状态和行为，对于reward的预测 agent的分类：\nvalue-based：没有显式的policy，根据value function选择最大的action policy-based：没有显式的value function，通过尝试不同的policy来找到最好的policy（action） actor-critic：结合policy和value function，通过value function给出baseline，基于相对baseline的好坏来优化policy 基于是否包含model来分类，即是否对环境进行学习，是否预测下一时刻环境的状态 MDP RL中的MDP：\nMDP描述了RL问题的environment，当environment是完全可观测的。 几乎所有RL问题都可以形式化为MDP，部分可观测的问题可以转化为MDP Markov Process Markov Process通过状态和状态转移矩阵来描述： Markov Reward Process 如果加上reward，就可以得到一个带衰减的累积reward： Markov Reward Process中折扣因子(Discount Factor) 必要性\n而状态value function就是在某个状态下上述return的期望： Bellman Equation for MRPs 将value function分成当前reward和未来的衰减value两部分： 未来的状态由转移矩阵确定： 直接求解线性方程复杂度为 $O(n^3)$，因此只适合小规模MRP的求解； 对于大规模MRP，通常使用迭代求解的方式：\n动态规划 Monte-Carlo evaluation TD learning（Temporal-Difference） Markov Decision Process 在MRP的基础上加入decisions，得到MDP： Policy policy定义为在给定状态下选择action的概率分布： 通过policy可以完全表示agent的行为 MDP policy依赖于当前状态（markov性质） policy是静态的，即不随时间变化 Value Function 基于policy的value function看作是在policy下的value期望； 其中state value function可以看成是action value function在policy下的加权平均： Bellman Expectation Equation state/action value function也都可以转化为Bellman方程的形式： 而他们互相之间的转换为： 相应地，通过两次转换，可以得到state/action value function的迭代： Optimal Value Function 最优state/action value function分别是： 对于 $q_(s,a)$，因为知道了当前状态不同action的value，直接选择最大的action就可以了；但是如果知道了 $v_(s)$，即知道我下一步想去哪个state，但是由于不知道action对应的状态转移矩阵（一般情况），没办法知道要选择哪个action最终value最大\nOptimal Value Function中q和v的差别\n在任意state上实现optimal value function的policy，称为optimal policy 根据 $q_*(s,a)$ 选择action的policy即optimal policy\nBellman Optimality Equation state/action optimal value function表述为Bellman Optimality Equation的形式： Bellman Optimality Equation由于包含max操作，是非线性的方程，一般没有closed form solution，可以使用迭代的算法求解：\nValue Iteration Policy Iteration Q-learning Sarsa 什么时候MDP的bellman方程求解可以转化为MRP？\nPlanning by Dynamic Programming Markov Process满足动态规划的两个性质：\n最优子结构：最优解可以被分解为多个子问题 Bellman方程就是一种子问题的拆解表示 重叠子问题：子问题重复很多次，且能被缓存和复用 value function保存了信息且被复用 Policy Evaluation 对所有state的value设定初始值后，每一轮基于当前policy可达state的value加权求和，得到新一轮的value；经过足够多轮次迭代后，value逐渐收敛到当前policy的value真值：\nPolicy Iteration 基于原始 $\\pi$ 迭代计算所有state的value，收敛后得到 $v_\\pi(s)$：\n在某一个state下选择value最大的action： 在每一个state选择value最大的action，那么你就得到了一个在全局所有状态下都表现得更好的新策略 $\\pi^{\\prime}$，更接近 $\\pi^*$： 通过不断地：\n评估：根据当前policy计算全部状态的value 优化：根据上述value贪婪地选择动作 就可以不断地优化policy，直到最优policy ## GridWorld: Dynamic Programming Demo\nValue Iteration 我们是否需要value收敛到真值后再优化policy？\n可以选择value的 $\\epsilon$ 足够小的时候停止迭代value并优化 可以选择每k步优化 是否可以每步优化？ 每步评估进行一次优化（取max）：每次采用可达state中最大的value来计算当前state的新一轮value，通过这种方式不断更新value，最终得到每个状态的 $v^*(s)$，称为value iteration\nPolicy Evaluation、Policy Iteration和Value Iteration的对比 Model-Free Prediction Model-Free中的model指的是环境，即我们不知道环境如何基于我们的action进行状态转移，也不知道会得到一个怎么样的reward，是一个未知的MDP。\n对于一个未知的MDP的value function的估计，称为Model-Free Prediction\nMonte-Carlo Learning MC从完整的实际经验中学习，如果经验不完整，无法使用MC，因为MC要求最终的return 基于最简单的想法，即value就是实际经验的平均return MC Policy Evaluation分为两类： First-Visit：只统计轨迹中到达该state的第一次，即便同一条轨迹中多次到达该状态，是无偏估计，收敛稳健 Every-Visit：每次到达都记为一次，即一条轨迹贡献多个样本分，可以更充分利用数据 通过增量更新value function： 对于非静态系统，可以将增量的系数设为常数，可以更有效忘记旧的样本： Temporal-Difference Learning TD可以从不完整的实际经验中学习，而MC不行 通过bootstrapping在不确定的环境中边走边学： TD learning中的bootstrapping 相比于MC中的目标值最终return $G_t$，TD（以TD(0)为例）基于下一状态的return作为目标值： $V(S_t) \\leftarrow V(S_t)+\\alpha(R_{t+1}+\\gamma V(S_{t+1}) - V(S_t))$ 其中： $R_{t+1}+\\gamma V(S_{t+1})$ 被称为TD target $\\delta_t = R_{t+1}+\\gamma V(S_{t+1}) - V(S_t)$ 被称为TD error TD和MC在bias/variance上的对比： TD target是有偏的，因为是基于一个有偏的 $V（S_{t+1}）$ 来估计的；而MC是无偏的，因为是真实样本的平均 TD的variance是小的，因为只包含当前的reward和对value的估计；而MC的variance是大的，因为MC中每一步的action和transition都引入variance TD对初始值敏感；而MC不敏感 TD learning相比MC learning效率高的原因 TD和MC在处理有限经验数据时的对比： MC 的目标是让价值函数 $V(s)$ 尽可能地贴近return，不考虑状态转移关系，只关注结果，可以理解为最小化均方误差（MSE） TD的目标是寻找一个最符合当前数据的MDP（状态转移和奖励），然后基于这个模型计算价值，可以理解为最大似然markov模型（MLE） 由于TD利用了Markov性质，在Markov环境中通常比MC更有效率，因为能够充分利用数据在状态之间传递信息 MC、TD和dynamic programming的更新对比： n-step TD learning，每次更新考虑n步，而不只是TD(0)的一步： 定义n步的TD target： 得到n步的TD learning： 这里以random walk为例，观察不同的n和alpha在10个episodes数据上的error： 曲线主要体现了bias/variance的trade-off：\n小n：variance小，bias大，因此较大的alpha更有利于快速优化 大n：variance大，bias小，因此较小的alpha更有利于稳定优化 中n：bias和variance合适，可以较快的收敛到更优解 对比online和offline：\nonline意味着每一步都更新：可以在较大的alpha达到最优解，因为它可以实时修正，所以敢于用更大的步长去快速吸收新信息 offline意味着batch更新：需要在较小的alpha达到最优解，因为一次性大量的更新会导致模型极其不稳定（因为这些更新都是基于旧的、可能错误的估计值同时发生的） 提升效果的手段：\n增加训练数据量，可以减小“小n”的bias，也可以减小“大n”的variance alpha如果固定，最终error会震荡，大n的震荡更明显，如果alpha衰减，最终error都会归零 TD($\\lambda$) 通过对不同step的TD进行加权平均，既利用了大步数带来的快速信息传播，又通过小步数维持了系统的稳定性；这个算法能够在一个统一的框架下，自动实现从 TD(0)到 MC的平滑过渡： TD($\\lambda$)对近期步数分配较高权重，对远期步数分配较低权重： 距离越近，因果关系越明确，因果关系越明确 给远期步数分配较小的权重可以抑制方差的累积 数学上的考虑，可以利用资格迹实现反向视角 TD($\\lambda$)的正向视角是坐在现在看未来，需要完整序列；而如果采用反向视角，即站在现在看过去，可以利用资格迹（Eligibility Traces）保存每一个state对当前state的return的贡献，从而实时更新当前步的return到之前的state上，且不需要序列是完整的： TD(λ)的反向视角\nModel-Free Control Model-Free Control用来解决那些未知MDP，或者已知MDP，但由于复杂所以只能采样的问题\nModel-Free Control主要分为：\nOn Policy：基于策略 $\\pi$ 上的采样学习策略 $\\pi$，在自己的认知范围内，行动后调整 Sarsa Off Policy：基于策略 $\\mu$ 上的采样学习策略 $\\pi$，可以复用其他策略的数据，总结别人的经验 Q-Learning、DQN（经验回放） On-Policy Monte-Carlo Control 为了使MC能在policy iteration中使用，由于我们是model-free的，不能采样 $v(s)$，而需要采样 $q(s, a)$，从而直接基于采样结果选择最佳action，来进行greedy policy improvement\n如何进行greedy policy improvement：\n与MDP已知不同，我们需要探索所有行为的可能，而没办法确定某个行为是最优的，这也是从planning到learning的跨越 通过 $\\epsilon-greedy$ 策略来选择action，在保证探索的前提下，同时保证收敛到更优： 不需要过多采样来精确得到 $q(s,a)$，可以只采样一条序列，更新序列中每一个s和a的pair的action value function，然后进行 $\\epsilon-greedy$ 操作来加速迭代 如果探索最终满足GLIE（Greedy in the Limit with Infinite Exploration），则能够保证最终收敛到全局最优，比如 $\\epsilon_k=\\frac{1}{k}$ 就是一个例子： GLIE条件及其工程实现 On-Policy TD Learning（SARSA） 将On-Policy的MC改成TD，就是Sarsa算法： Sarsa算法收敛到最优策略的条件：\n满足GLIE，例如通过 $\\epsilon-greedy$ 选择动作即可 步长 $\\alpha$ 满足Robbins-Monro条件： $\\sum\\limits_{t=1}^{\\infty} \\alpha_t = \\infty$：确保算法有足够的动力消除初始误差 $\\sum\\limits_{t=1}^{\\infty} \\alpha_t^2 \u003c \\infty$：确保随机噪声最终会被抵消，实现稳定 实际应用中，大部分不考虑Robbins-Monro条件，甚至有时候不考虑GLIE，Sarsa也能work 与TD($\\lambda$)类似的思考，通过对n-step Sarsa的average，我们可以得到Sarsa($\\lambda$)： 同样得到基于资格迹的后向视角的工程实现： Off-Policy Off-Policy的意义：\n复用数据：参考其他策略的action和结果，帮助优化自己的policy 在采用exploration的policy的情况下，学习optimal policy 在采用某个policy得情况下，学习多个policy 通过important of sampling来利用策略Q的采样来评估策略P： 对于策略Q产生的样本，由于策略不同，导致这些样本在策略P中出现的概率不同，因此在用于评估策略的时候的权重也不同，因此如果要准确地利用这些样本评估策略P，需要做权重的调整： 由于以下两个原因，off-policy的MC在实际应用中几乎不可用：\n数据饥渴：当 $\\mu(A|S)=0$ 但是 $\\pi(A|S)\\neq0$ 的时候，这条样本不可用，即便可以通过 $\\epsilon-greedy$ 的策略保证 $\\mu(A|S)\\neq0$，但是这时候的权重会极小，$\\epsilon/m$，依然不可用 数值爆炸：重要性采样会因为概率的连乘很大程度增加方差 off-policy的TD Learning使用了bootstrapping，重要性权重只加在最近的一次reward上，大大缓解数据饥渴和数值爆炸的问题： Q-Learning Q-Learning可以避免使用重要性采样，更方便地进行off policy的control：\n通过Q和行为策略 $\\mu$（通常使用 $\\epsilon-greedy$ 来保证探索）选择当前state $S_t$ 要更新的action $A_t$ 确定 $S_t$ 和 $A_t$ 后，环境给出reward，$R_{t+1}$ 和下一个state，$S_{t+1}$ 这部分称为SARS四元组，可以使用历史数据进行回放的方式复用数据 在计算Q的更新时，$A’$ 是通过目标策略 $\\pi$ 选择出来的（max选择最优action），这样可以避免使用重要性采样（因为目标策略 $\\pi$ 是确定性的，不是分布，不需要纠偏） 为什么Q-Learning有逃避“重要性采样”的特权？ 可以理解为：行为策略 $\\mu$ 决定更新哪个state和action，目标策略 $\\pi$ 决定怎么更新（具体数值） 可以理解成：SARSA通过迭代 $Q_{\\pi}$，然后最终 $Q_{\\pi}=Q_$ 的方式找到 $Q_$；而Q-Learning直接迭代 $Q_*$ Q-Learning和Sarsa之间的关系\nTD和DP的关系对比： Value Function Approximation 现实世界中，问题的状态规模是巨大的，这带来两个问题：\n无法存储全部的state/action在memory中 全部更新所有的state/action太慢 Value Function Approximation通过函数来近似表示value： 可以从已知的状态泛化到未知的状态 通过MC或者TD来更新参数w function approximator：\n典型结构： 实现方式： 特征线性组合 神经网络 决策树 最近邻居 傅立叶/小波变换 训练数据特点： 非固定策略：即训练过程中数据有分布会随策略优化而变化 none-iid：即训练数据来自序列且前后关联的，模型可能会在短时间内过度拟合某一段连续的轨迹，导致参数更新剧烈波动 Incremental Methods 采用SGD的方式拟合，由于RL问题中没有label，我们用return作为target来计算梯度：\nMC：$\\Delta \\mathbf{w} = \\alpha( G_t - \\hat{v}(S_t, \\mathbf{w}))\\nabla_{\\mathbf{w}}\\hat{v}(S_t, \\mathbf{w})$ TD(0)：$\\Delta \\mathbf{w} = \\alpha( R_{t+1} + \\gamma\\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}))\\nabla_{\\mathbf{w}}\\hat{v}(S_t, \\mathbf{w})$ TD($\\lambda$)：$\\Delta \\mathbf{w} = \\alpha( G_t^{\\lambda} - \\hat{v}(S_t, \\mathbf{w}))\\nabla_{\\mathbf{w}}\\hat{v}(S_t, \\mathbf{w})$ 反向视角： 资格迹的更新规则要求累积价值函数关于参数的梯度： 预测更新：$E_t = \\gamma \\lambda E_{t-1} + \\nabla_{\\mathbf{w}} \\hat{v}(S_t, \\mathbf{w})$ 控制更新：$E_t = \\gamma \\lambda E_{t-1} + \\nabla_{\\mathbf{w}} \\hat{q}(S_t, A_t, \\mathbf{w})$ Function Approximation中资格迹的更新规则 当解决控制问题时，将上述方法使用在action value function上，然后做policy evaluation/improvement的迭代即可\nprediction算法收敛性： On-Policy TD在非线性拟合价值函数时不收敛的原因 Off-Policy TD在函数拟合时不收敛的原因是，死亡三要素条件共同作用下，算法往往不稳定：\nFunction Approximation：参数耦合，导致更新一个状态的价值可能会改变其他状态的价值 Bootstrapping：估计有偏，偏差就会在迭代过程中不断积累和循环 Off-Policy：行为策略不一定向价值高的地方走，导致偏差不会及时发现 control算法收敛性： Batch Methods 为了更有效率地利用样本，我们通常利用agent的经验序列作为数据集，不断采样（经验回放）然后进行SGD求解Least Squares Prediction： DQN DQN使用经验回放和固定Q-targets的方式：\n基于Q-network的计算结果 $Q(s,a,w_i)$，根据 $\\epsilon-greedy$ 策略选择action $a_t$ 存储四元组 $(s_t, a_t, r_{t+1}, s_{t+1})$ 进入回放memory 每次从memory中采样minibatch 基于旧参数 $w^-$ 计算Q-targets 利用SGD优化Q-network和Q-targets之间的MSE，更新Q-network的参数权重 每隔一段时间（～1000步），更新 $w^-$ 到最新参数 DQN能够稳定收敛的两个原因：\n经验回放打散了序列训练数据，让minibatch样本直接减少关联 使用两套网络参数，每次冻结Q-target的网络，更新Q-network，避免在不固定的target上进行bootstrapping 另一种绕过梯度更新的方式是LSPI，基于特征的线性映射，直接通过矩阵运算一次求解参数： Least-Squares Policy Iteration是什么？\nPolicy Gradient 相比求解value function的方式来迭代policy，policy gradient直接优化policy Policy Gradient的优点，主要在于避免了value-based方法的max操作：\n更好的收敛性质：相比value-based方法中取max的操作会导致参数剧烈波动，policy-based的学习目标是一个分布，不会跳变 在高纬度或连续action空间更有效：这种空间本身就很难取max 可以学习随机策略：即最终的策略是一个分布，而不是基于value的max Policy Gradient的缺点：\n通常收敛到局部最优，而不是全局最优 评估policy通常效率低，且高variance Policy Objective Function，即如何评价policy：\n在周期性任务中以初始状态 $s_1$ 起始的value：$J_1(\\theta) = V^{\\pi_\\theta}(s_1) = \\mathbb{E}{\\pi\\theta} [v_1]$ 在连续性任务中的平均value：$J_{avV}(\\theta) = \\sum_{s} d^{\\pi_\\theta}(s) V^{\\pi_\\theta}(s)$ 其中 $d^{\\pi_\\theta}(s)$ 是策略 $\\pi_\\theta$ 下状态的平稳分布 在连续性任务中的每步平均value：$J_{avR}(\\theta) = \\sum_{s} d^{\\pi_\\theta}(s) \\sum_{a} \\pi_\\theta(s, a) \\mathcal{R}_s^a$ Finite Difference Policy Gradient 通过对当前policy不同参数进行微小扰动，评估objective得到对应的delta，作为该参数的梯度： Monte-Carlo Policy Gradient（REINFORCE） 目标 $J(\\theta)$ 是在当前策略下，所有可能路径 $\\tau$ 的总回报 $R(\\tau)$ 的期望值：\n$J(\\theta) = \\mathbb{E}{\\pi\\theta}[R(\\tau)] = \\sum_{\\tau} P(\\tau|\\theta) R(\\tau)$\n对 $J(\\theta)$ 求导，将策略的导数转化为策略对数的导数的期望： $$\\begin{flalign*} \\nabla_\\theta J(\\theta) \u0026= \\sum_{\\tau} \\nabla_\\theta P(\\tau|\\theta) R(\\tau) \u0026\\ \u0026= \\sum_{\\tau} P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta) R(\\tau) \u0026\\ \u0026= \\mathbb{E}{\\pi\\theta} [\\nabla_\\theta \\log P(\\tau|\\theta) R(\\tau)] \u0026 \\end{flalign*}$$\n展开 $P(\\tau|\\theta)$，即一条路径的概率等于：初始状态概率 × 策略概率 × 环境转移概率： $P(\\tau|\\theta) = \\mu(s_0) \\prod_{t=0}^{T} \\pi_\\theta(a_t|s_t) P(s_{t+1}|s_t, a_t)$\n取log得到： $\\log P(\\tau|\\theta) = \\log \\mu(s_0) + \\sum_{t=0}^{T} \\log \\pi_\\theta(a_t|s_t) + \\sum_{t=0}^{T} \\log P(s_{t+1}|s_t, a_t)$\n其中 $\\mu(s_0)$（初始状态）和 $P(s_{t+1}|s_t, a_t)$（环境物理规则）都与 $\\theta$ 无关，可以舍弃：\n因此梯度简化为： $\\nabla_\\theta \\log P(\\tau|\\theta) = \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n最终我们得到： $\\nabla_\\theta J(\\theta) = \\mathbb{E}{\\pi\\theta} \\left[ \\left( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) R(\\tau) \\right]$\n可以用一次或几次真实的采样来近似这个期望： $\\nabla_\\theta J(\\theta) \\approx \\sum_{t=0}^{T} R(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n其中，log prob的梯度我们称为score function\n当动作空间是离散的，假设策略是softmax policy，策略 $\\pi$ 的prob是通过 $\\phi(s, a)^\\top \\theta$ 的softmax得到，那么：$\\nabla_\\theta \\log \\pi_\\theta(s, a) = \\phi(s, a) - \\mathbb{E}{\\pi\\theta} [\\phi(s, \\cdot)]$ 其中：\n$\\phi(s, a)$：是你实际采样做出的动作 $a$ 的特征 $\\mathbb{E}{\\pi\\theta} [\\phi(s, \\cdot)]$：是你当前模型下，对所有可能动作特征的平均预期 当动作空间是连续的，假设策略是gaussian policy，均值为 $\\mu(s) = \\phi(s)^\\top \\theta$，即 $a \\sim \\mathcal{N}(\\mu(s), \\sigma^2)$ 那么：$\\nabla_\\theta \\log \\pi_\\theta(s, a) = \\frac{(a - \\mu(s))\\phi(s)}{\\sigma^2}$\n上述两者都可以理解为，当你的action包含这个特征较多且最终证明是正确的时候，加强这个特征\nPolicy Gradient定理 只要你的目标函数可以表示为某种加权状态分布下的期望收益，其梯度都可以统一写成以下公式： 总结Monte-Carlo Policy Gradient（REINFORCE）的思路：\n通过采样获得一组轨迹，以及最终的return 根据return（一般+1或者-1，代表方向）与log prob的梯度得到整体参数关于objective的梯度 更新参数，优化objective Actor-Critic Policy Gradient REINFORCE的奖励是最终累积，且训练数据来自于相同序列，导致方差较大 为减小variance，添加critic部分：\ncritic： 采用MC或者TD（更常用），基于最小化MSE的目标更新参数 $w$ 进行对当前状态的价值 $Q_w(s, a)$ 进行预测，避免累积奖励导致的波动问题 actor： 根据critic给出的价值进行policy gradient的计算和更新 critic的引入带来的bias，即目标价值不一定是无偏的： Compatible Function Approximation解决critic的bias\nAdvantage Actor-Critic（A2C）引入value function作为baseline可以进一步减少方差 同时计算Q和V（维护两套参数w和v），相减得到优势函数： 进一步的，V的TD error就是优势函数的期望： 只需要维护一套参数v，来计算状态的价值函数V，就可以把TD error当成优势函数 Actor-Critic中用V的TD error作为actor的policy gradient？\n与value-based的方案一样，我们考虑到TD(0)的更新方式bias较高，希望引入TD($\\lambda$)减小bias： 如何通过资格迹解决actor的在线更新？\n总结actor的不同实现： Deterministic Policy Gradient 相比于上述随机策略（stochastic）输出动作的概率分布，DPG直接输出确定的动作值：\nSPG由于使用的是似然比技巧（Likelihood Ratio Trick），当训练接近后期，概率分布的variance非常小，很难探索其他动作 DPG采用链式法则（Chain Rule），即使策略已经非常稳定，只要Critic发现更好的Q，动作也会调整： 换一个角度理解： SPG$\\approx$On-policy：为了计算 $\\nabla_\\theta J(\\theta) = \\mathbb{E}{a \\sim \\pi\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a)]$，最直接的方法就是让策略 $\\pi_\\theta$ 去环境中跑出数据，一旦策略更新了，旧的数据就不能直接用了（除非important sampling） DPG$\\approx$Off-policy：由于DPG输出的是确定的动作 $a = \\mu_\\theta(s)$，它本身缺乏探索能力。为了训练它，我们必须使用一个“带噪声的动作”去探索环境，这本质上就是Off-policy Integrating Learning and Planning 相比value-based（通过经验学习value function）和policy-based（通过经验学习policy），我们还可以通过经验学习model，然后通过planning来构造value function和policy RL中Planning和Control的区别： 对于求解最优策略：model-based称为planning，model-free称为control\n优点：\n可以快速利用监督学习的方法学习model 可以基于model的不确定性进行推理：agent会针对不确定的区域进行保守动作/主动学习 缺点： 先学model，再学value function，引入了两次误差 model是一个MDP的表示，包含S（状态空间）、A（动作空间）、P（转移方程）、R（reward方程），其中我们一般假设S和A是已知的，因此model的学习是基于监督数据： 其中：\nR的学习是一个regression问题 P的学习是一个密度估计问题（概率分布） Model-Based RL 通过不同类型的方法学习model：包括Table Lookup、Linear Expectation、Linear Gaussian、Gaussian Process、Deep Belief Network等 基于model，利用model-based的planning方法：Value Iteration、Policy Iteration、Tree Search等，求解MDP 或者基于model生成样本，利用model-free的control方法：Monte-Carlo、Sarsa、Q-learning等 Sample-Based Planning相比直接Model-free Planning的优势 基于不准确的model，使用model-based的方法势必得到非最优解： 当模型完全错误的时候，转而采用model-free的方法 对模型不确定性显示建模： RL中对Model Uncertainty显式建模的方法 Integrated Architectures Dnya-Q Dyna将model-free和model-based结合在一起：\n从真实样本学习model 从真实和模拟样本学习value function和policy Simulation-Based Search Forward Search 基于当前state进行模拟：随机生成样本，截止到n步，构造sub-MDP 基于sub-MDP进行model-free的学习，更新Q和policy 利用MC叫Monte-Carlo Search，利用Sarsa叫TD Search 学习只在sub-MDP上，每次学完就抛弃，并不维护全局Q Forward Search和Dyna的优劣对比 Monte-Carlo Tree Search 在MC Search的基础上，不对每一个动作采样，而是根据动作的价值期望采样 价值大的动作采样更多，保证探索充分，预估精准 价值期望一般通过UCB策略来判断 相比MC Search按固定policy采样，MCTS每次采样分为两个阶段，不断迭代这两个阶段，使采样policy不断提升，最终搜索树会逐渐长成最有效的形状： in-tree：当前state在已探索过的tree内部，采用 $\\epsilon-greedy$ 或者UCB策略 out-of-tree：当前state在tree外部，采用random来快速获得大致价值 Dyna-2 基于Dyna的思路，TD learning+TD search： Long-term memory：通过TD learning迭代全局价值信息 Short-term memory：通过TD search模拟当前局部价值信息 最终的value function是两者的和 Exploration and Exploitation 寻求exploration和exploitation的平衡：\nexploration收集信息 exploitation基于当前信息选择最好的action 探索的三种策略：\n随机探索：探索随机的action（$\\epsilon-greedy$，softmax） 不确定性乐观：判断不确定性的价值，倾向于探索高不确定性的action（UCB） information state space（信息状态空间）： 将agent的信息作为状态的一部分（state=环境（agent外部）+信息（agent内部）） 预判信息是否对reward有帮助 探索的两种维度：\nstate-action探索：在state和action中探索，比较常见 parameter探索：尝试不同的参数，比如policy gradient中的policy参数 优点：在一定步数内保持相同的探索（参数），相比state-action基本上每一步确定是否探索 缺点：对state/action未知，相当于在黑盒中按不同parameter探索 Multi-Armed Bandits 多臂赌博机问题中，我们希望最小化总体regret（与最优action的gap）\nRandom Exploration greedy和 $\\epsilon-greedy$ 策略都是随次数线性增加的总体regret Optimism in the face of Uncertainty Optimistic Initialization： 所有action初始设为最大值，通过采样慢慢收敛到真实值，保证所有action都会被探索到 policy使用greedy或者 $\\epsilon-greedy$，仍然线性增加的总体regret Decaying $\\epsilon_t-greedy$：假设我们知道gap（现实中不可能），可以设置一个衰减速率达到logarithmic asymptotic的总体regret Lai\u0026Robbins定理说明多臂赌博机问题的总体regret下界是logarithmic asymptotic的： $\\triangle_a$ 是最优action收益和其他action收益的差值 $KL(R^a||R^{a^*})$ 是最优action的收益分布和其他action的收益分布的KL散度（分布差异大小） 如果最优和其他action的平均差异大，但是分布又很接近，则总体regret就会大 UCB：通过动作的置信上界（upper confidence）来判断action的好坏： 根据Hoeffding不等式，如果我们希望真值超过UCB的概率是p，则 $U_t(a) = \\sqrt{\\frac{-\\log p}{2N_t(a)}}$ 我们希望随次数变大，p变小，可以设 $p=t^{-4}$，则 $U_t(a) = \\sqrt{\\frac{2 \\log t}{N_t(a)}}$ 最终：$a_t = \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} , Q(a) + \\sqrt{\\frac{2 \\log t}{N_t(a)}}$ Bayesian Bandits：基于价值的先验分布，根据采样得到后验分布，通过后验分布判断action： Bayesian UCB：利用后验分布的variance作为UCB的不确定分数，$a_t = \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} , \\mu_a + c \\sigma_a / \\sqrt{N(a)}$ Probability Matching：基于一个action是最优action的概率来进行采样 Thompson Sampling： 通过对每个动作的value随机采样实现 Information State Space Information State Search：基于信息的价值（长期收益-短期损失）来判断 衡量value of information：不确定性高且与最优action相关的action，包含较大信息价值 RL中信息价值与不确定性的关系 Information State Space Bandits：通过将历史统计作为state的一部分，构成一个新的MDP，可以使用不同的方式求解： Model-free RL：Q-learning Bayesian Model-based RL：Gittins Indices、Bayesian-adaptive MDPs Contextual Bandits 什么是Contextual Bandits？\nMDPs 上述所有的探索策略，都可以应用于MDP上：\n以UCB为例，$a_t = \\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} , Q(s_t, a) + U_1(s_t, a) + U_2(s_t, a)$： 评估不确定性可以简单的加入UCB的不确定项 $U_1$ 对于Q的预测不管是评估不确定性，还有策略改进带来的不确定性 $U_2$，这部分计算比较困难 Classic Games Game Theory 作为单独的agent：\n其他agent变成了环境的一部分，游戏变成了MDP，最优策略就是这个MDP的最优策略 纳什均衡是self-play RL中的一个固定点，$\\pi^i = \\pi^i_*(\\pi^{-i})$： 经验序列通过agent间进行游戏产生 每个agent学习如何迭代policy，适应其他agent并构成其他agent的环境 在学习的过程中，每个agent的最优策略都会被学习到 寻找纳什均衡的两种方法：\nGame tree search（planning） Self-play RL 游戏分为perfect游戏和imperfect游戏：\nperfect即完全可观测：国际象棋、围棋等 imperfect即不完全可观测：扑克等 Minimax Search 以两个玩家的游戏为例，$\\pi = \\langle \\pi^1, \\pi^2 \\rangle$\nminimax value function是指在最小化玩家二的value的策略下（玩家二的目标就是最小化reward，从他的视角），最大化玩家一的value：$v_{*}(s) = \\max_{\\pi^1} \\min_{\\pi^2} v_{\\pi}(s)$ minimax policy是指达到minimax value function的policy，即纳什均衡 通过minimax search的树状结构，搜索每一条路径的value，然后交替使用min和max来决策： 树的大小指数增长 替代查表，可以使用value function approximator来表示value 通过固定步深度left的value，向上进行min和max的操作 Self-Play RL value function：可以直接应用MC或者TD算法 policy improvement：由于游戏是deterministic的，即当前state下action的下一个state是确定的，因此可以直接选择max或者min来迭代policy，即 $A_t = \\arg\\max_a v_{*}(\\operatorname{succ}(S_t, a))$\nCombining RL and Minimax Search 将基础RL算法和minimax search相结合的几种方式：\nSimple TD：先用TD learning计算value function，然后用minimax search再过一遍更新 TD Root：利用下一步state的minimax search得到的value学习当前步的value function TD Leaf：利用下一步state的minimax search选取的leaf的value，更新当前state的minimax search选取的leaf的value TD Leaf算法没落的原因 TreeStrap：利用当前state的minimax search所有选取路径上的search value更新路径上全部对应state的value Simulation-Based Search：用self-play代替minimax search，比如MCTS RL in Imperfect-Information Games 在imperfect游戏中，由于看不到对手的信息，许多不同的真实状态可能对应于同一个状态（以你的视角），因此，双方各自维护自己的搜索树\nSmooth UCT Search是在MCTS的UCT算法的基础上，以一定概率基于对方平均行为进行应对： 参考链接：\nDeepMind x UCL | Introduction to Reinforcement Learning 2015",
    "description": "Intro 强化学习与机器学习的差别：\n没有supervisor，只有reward 反馈是滞后的，不是实时的 时间序列，每一个时间步不是i.i.d的 agent的行为会影响后续数据 reward是什么：\n一个标量的反馈信号 表示agent在t时刻的表现 agent的目标是最大化累积reward 序列决策是什么：\n目标是选择action，最大化累积reward action有长期的影响，因为reward是滞后的，意味着可能需要牺牲短期reward来获取长期reward 序列H包含observation、action、reward的循环：O1、A1、R1、O2、A2、R2、… state是什么：\nstate是序列的函数，即当前的状态包含了观察、行为、奖励序列的全部信息 environment state是指环境的内部状态表示，通常是agent不可见的；即使可见，也通常包含一些不相关的噪音 agent state是agent的内部状态表示，作为下一次action选择的输入，也可理解为RL算法的输入 markov state是指包含之前全部信息的当前状态，只依赖于当前状态，与之前状态独立 在完全可观测环境中，agent state等于environment state，构成MDP（markov decision process）；在部分可观测环境中，agent state不等于environment state，构成POMDP（partially observable markov decision process） agent agent的组成：",
    "tags": [
      "技术笔记"
    ],
    "title": "DeepMind x UCL Introduction to RL 2015 课程笔记",
    "uri": "/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "强化学习 RLHF PPO DPO RLVR #todo 强化学习调整agent行为模式：retroformer、voyager #todo rlvr， 清华上交paper：提高pass@1的稳定性 #todo 田渊栋：latent reasoning coconut / attention sync / streaming llm #todo thinking machine lab: tinker api / 自己搭megatron、deepspeed #todo PRM #todo interleaving thinking后训练 #todo verl slime #todo Search-R1",
    "description": "强化学习 RLHF PPO DPO RLVR #todo 强化学习调整agent行为模式：retroformer、voyager #todo rlvr， 清华上交paper：提高pass@1的稳定性 #todo 田渊栋：latent reasoning coconut / attention sync / streaming llm #todo thinking machine lab: tinker api / 自己搭megatron、deepspeed #todo PRM #todo interleaving thinking后训练 #todo verl slime #todo Search-R1",
    "tags": [
      "技术笔记"
    ],
    "title": "LLM和agent的训练",
    "uri": "/hugo-blog/blogs/llm%E5%92%8Cagent%E7%9A%84%E8%AE%AD%E7%BB%83/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 强化学习基础知识 搭建商品收货记录系统 搭建商品收货记录系统 学习bootstrap基础知识 前端开源css框架，主要用于快速开发响应式的网站，拥有Grid System和丰富预设组件 预设div的一些class：方便基于grid布局的响应式变化，包括grid的数量、顺序以及offset 提供预设的组件：buttons、下拉栏、以及modal（支持js）等 参考链接 整体思路采用flask+sqlite+bootstrap的方式: 从零开始构建商品收货系统 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-基本概念 DeepMind x UCL Introduction to RL 2015 课程笔记-MDP DeepMind x UCL Introduction to RL 2015 课程笔记-Planning by Dynamic Programming 知识 强化学习主要解决agent在环境中选择policy从而达到最大化序列reward的问题 agent与环境的交互（state和action）可以表示为Markov Process 如果加入reward，就变成MRP（Markov Reward Process） 如果再加入decision（$\\pi$），就变成MDP（Markov Decision Process） value function，即从当前state起到最后的序列reward的带衰减的累积，用来衡量当前状态的好坏： state value function：只基于当前state，称为$v(s)$ action value function：基于当前state和选择的action，称为$q(s,a)$ 只考虑MRP的情况，value function是线性可解的，只是复杂度为$O(n^3)$ 通过基于policy的加权求和，可以在v和q之间转化，也可以经过两次转化（v到q到v）得到状态之间的递推关系，称为Bellman Expectation Equation 对于固定policy，可以简化为MRP，value function是线性可解的； MDP求最优policy，即Bellman Optimality Equation，递推关系中由于引入了max操作，是线性不可解的，只能采用迭代法等： policy iteration：通过迭代value function（减小计算复杂度）来评估policy，然后优化policy（选择最大value的state作为action），可以理解为利用Bellman Expectation Equation+Greedy Policy value iteration：不直接迭代value function，而是每次都选择当前最大value的state作为action，然后再根据新的action来更新value，可以理解为利用Bellman Optimality Equation value iteration可以看成每次只迭代一次求value function的policy iteration，避免把时间浪费在“烂策略”的value迭代上 待办 强化学习基础知识 跟进大模型强化学习技术",
    "description": "总结 强化学习基础知识 搭建商品收货记录系统 搭建商品收货记录系统 学习bootstrap基础知识 前端开源css框架，主要用于快速开发响应式的网站，拥有Grid System和丰富预设组件 预设div的一些class：方便基于grid布局的响应式变化，包括grid的数量、顺序以及offset 提供预设的组件：buttons、下拉栏、以及modal（支持js）等 参考链接 整体思路采用flask+sqlite+bootstrap的方式: 从零开始构建商品收货系统 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-基本概念 DeepMind x UCL Introduction to RL 2015 课程笔记-MDP DeepMind x UCL Introduction to RL 2015 课程笔记-Planning by Dynamic Programming 知识 强化学习主要解决agent在环境中选择policy从而达到最大化序列reward的问题 agent与环境的交互（state和action）可以表示为Markov Process 如果加入reward，就变成MRP（Markov Reward Process） 如果再加入decision（$\\pi$），就变成MDP（Markov Decision Process） value function，即从当前state起到最后的序列reward的带衰减的累积，用来衡量当前状态的好坏： state value function：只基于当前state，称为$v(s)$ action value function：基于当前state和选择的action，称为$q(s,a)$ 只考虑MRP的情况，value function是线性可解的，只是复杂度为$O(n^3)$ 通过基于policy的加权求和，可以在v和q之间转化，也可以经过两次转化（v到q到v）得到状态之间的递推关系，称为Bellman Expectation Equation 对于固定policy，可以简化为MRP，value function是线性可解的； MDP求最优policy，即Bellman Optimality Equation，递推关系中由于引入了max操作，是线性不可解的，只能采用迭代法等： policy iteration：通过迭代value function（减小计算复杂度）来评估policy，然后优化policy（选择最大value的state作为action），可以理解为利用Bellman Expectation Equation+Greedy Policy value iteration：不直接迭代value function，而是每次都选择当前最大value的state作为action，然后再根据新的action来更新value，可以理解为利用Bellman Optimality Equation value iteration可以看成每次只迭代一次求value function的policy iteration，避免把时间浪费在“烂策略”的value迭代上 待办 强化学习基础知识 跟进大模型强化学习技术",
    "tags": [
      "周记"
    ],
    "title": "Week10 强化学习基础知识",
    "uri": "/hugo-blog/weekly/week10/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 需求：老家亲戚是做衣服加工的，分为横机和套口两个种类，现在要做一个功能，来汇总和清晰的展示一些统计数据 现状：目前使用excel来完成：图1包含了横机的单位重量（重量/件）、施工单位和套口的施工单位的基础信息；图2包含了横机按时间、施工单位、件数、重量（件数 * 重量/件）的明细；图3包含了套口按时间、施工单位、件数（包含发出和收回）的明细 痛点：亲戚目前每天手动填写当天的新增条目（哪个单位给了多少，什么款式，如果是套口还包含收回多少），然后需要下拉excel单元格显示结果（excel的列目前用公式指定怎么计算结果）。现在希望可以只用手动添加条目，不用excel下拉的方式，且统一品类在同一页面方便截图 想法：做一个网页，加一个后台数据库，直接访问网页操作，但是考虑数据隐私和亲戚电脑没有联网的情况，设计为本地保存数据 整体思路 基于上述要求，gemini的思路： 方案二细节： 从零开始构建商品收货系统-方案细节\n具体实现 代码链接 product-record-manager\n网页布局 侧边栏分为横机、套口两个子页面，子页面逻辑几乎一致（套口不包含重量） 子页面（以横机为例）： 顶部两个卡片添加款式和加工单位，使用频率低所以默认收缩，可点击展开，展开固定三行，超过部分用scroll方式查看 新增记录部分选择款式、加工单位和件数，通过关联款式基础表的单位重量计算总重量，插入明细表 明细展示部分支持按日期、款式和加工单位筛选，结果中默认展示明细，明细支持修改和删除；点击聚合按钮按日期、款式和加工单位聚合件数和重量，聚合后不支持修改和删除 技术细节 框架: Flask + SQLAlchemy + Bootstrap 5 数据库: SQLite 设计模式: Models (模型层): 定义了数据结构（款式，加工单位，横机记录，套口记录) Templates (视图层): 使用Jinja2模板引擎，index.html作为基布局，hengji.html和taokou.html作为具体页面 Routes (控制层): app.py中定义的函数处理URL请求、数据库操作和页面跳转 PRG 模式 (Post/Redirect/Get) 通过PRG模式，实现用户刷新页面的时候，筛选条件清空，且不弹出提示表单内容重置的警告\n后端接收 POST 请求，不直接查询，而是把筛选条件存入用户的session(类似浏览器的 Cookie 缓存)，然后redirect重定向回首页 (GET)\n@app.route('/', methods=['GET', 'POST']) def hengji_index(): if request.method == 'POST': # Store filters in session and Redirect to GET session['date_range'] = request.form.get('date_range', '') session['unit_filter'] = request.form.getlist('unit_filter') session['style_filter'] = request.form.getlist('style_filter') return redirect(url_for('hengji_index')) # GET request: Consume session filters (Flash pattern) # Pop them so they don't persist on next refresh date_range = session.pop('date_range', '') unit_filters = session.pop('unit_filter', []) style_filters = session.pop('style_filter', []) show_aggregate = session.pop('show_aggregate', False) # 实际处理逻辑 Flash消息机制（Flask Flash） flash用于在请求之间传递临时的“一次性”消息（比如“添加成功”、“删除失败”）\n后端，消息被加密存储在 Session 中\nflash('添加成功', 'success') # 参数1: 消息内容, 参数2: 类别(用于前端样式) return redirect(url_for('hengji_index')) 前端，使用了Bootstrap Toast组件来美观地显示这些消息\n\u003cdiv class=\"toast-container ...\"\u003e {% with messages = get_flashed_messages(with_categories=true) %} \u003c!-- 遍历并渲染所有消息 --\u003e {% endwith %} \u003c/div\u003e Flatpickr 日期选择器 一个轻量级、功能强大的日期选择 JS 库，比原生的\u003cinput type=“date”\u003e更漂亮且支持范围选择\n初始化\nflatpickr(\"#dateRange\", { mode: \"range\", // 开启范围选择模式 dateFormat: \"Y-m-d\", // 提交给后台的格式 altInput: true, // 启用以更友好的格式显示给用户 altFormat: \"Y年m月d日\", // 用户看到的格式 locale: \"zh\", // 汉化 onClose: function(...) { // 当用户关闭选择器（选完日期）时，自动提交筛选表单 document.getElementById('filterForm').submit(); } }); 每次#dataRange修改触发表单提交和页面刷新\nfunction setQuickDate(daysAgo) { const end = new Date(); const start = new Date(); start.setDate(start.getDate() - daysAgo); const el = document.querySelector(\"#dateRange\"); if (el \u0026\u0026 el._flatpickr) { el._flatpickr.setDate([start, end], true); document.getElementById('filterForm').submit(); } } Modal（模态框/弹窗） 在 Bootstrap 中，Modal是一种非常流行的 UI 组件，它会在页面的顶层创建一个覆盖层，弹出一个对话框。模态框通常被用来执行“添加新记录”、“编辑记录”或“确认删除”等操作，这样用户就不必离开当前页面\n在 Bootstrap 的模态框（Modal）结构中：\n.modal（遮罩/容器层）：负责全屏覆盖，包括那个半透明的黑色背景（backdrop）。它控制整个弹窗的显示与隐藏，并且负责监听点击背景关闭弹窗的行为 .modal-dialog（定位/尺寸层）：负责弹窗在屏幕中的位置（居中还是顶部）以及弹窗的宽度 .modal-content（内容/皮肤层）：负责弹窗的视觉样式，通常里面会再细分为modal-header、modal-body和modal-footer \u003c!-- Add Unit Modal --\u003e \u003cdiv class=\"modal fade\" id=\"addUnitModal\" tabindex=\"-1\"\u003e \u003cdiv class=\"modal-dialog\"\u003e \u003cdiv class=\"modal-content\"\u003e ... \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e 模板继承 (Jinja2 Template Inheritance) index.html中，利用Jinja2 模板引擎的占位符（坑位）：{% block content %}{% endblock %}\n\u003cdiv class=\"d-flex w-100 overflow-hidden\"\u003e # 侧边栏内容 ... \u003cdiv class=\"col-md-10 content flex-grow-1\" style=\"overflow-x: hidden;\"\u003e {% block content %}{% endblock %} \u003c/div\u003e \u003c/div\u003e hengji.html中开头写\n{% extends \"index.html\" %} 然后后面写\n{% block content %} ...具体内容... {% endblock %} 模板引擎在渲染时，就会自动把content的部分填充到index的坑位里面\n运行细节 打包为exe（build_portable_mac.sh）：\n下载python的window版本以及依赖的whl 打包并编写.bat文件，在无联网环境下双击直接打开 启动逻辑：\n启动 → 检查端口是否被占用 ├─ 已占用 → 打开浏览器 → 退出 └─ 未占用 → 启动服务器 + 心跳监控 app.py中加入心跳检测，判断网页是否alive，如果用户关闭网页，则后台自动关闭，并关闭windows的cmd窗口 index.html中加入每两秒发送心跳的逻辑 每次用户点击.bat，都会首先检测下5000是否在使用（之前打开过），如果有则打开浏览器并直接关闭新后台，防止多开",
    "description": "背景 需求：老家亲戚是做衣服加工的，分为横机和套口两个种类，现在要做一个功能，来汇总和清晰的展示一些统计数据 现状：目前使用excel来完成：图1包含了横机的单位重量（重量/件）、施工单位和套口的施工单位的基础信息；图2包含了横机按时间、施工单位、件数、重量（件数 * 重量/件）的明细；图3包含了套口按时间、施工单位、件数（包含发出和收回）的明细 痛点：亲戚目前每天手动填写当天的新增条目（哪个单位给了多少，什么款式，如果是套口还包含收回多少），然后需要下拉excel单元格显示结果（excel的列目前用公式指定怎么计算结果）。现在希望可以只用手动添加条目，不用excel下拉的方式，且统一品类在同一页面方便截图 想法：做一个网页，加一个后台数据库，直接访问网页操作，但是考虑数据隐私和亲戚电脑没有联网的情况，设计为本地保存数据 整体思路 基于上述要求，gemini的思路： 方案二细节： 从零开始构建商品收货系统-方案细节\n具体实现 代码链接 product-record-manager",
    "tags": [
      "技术笔记"
    ],
    "title": "从零开始构建商品收货系统",
    "uri": "/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%95%86%E5%93%81%E6%94%B6%E8%B4%A7%E7%B3%BB%E7%BB%9F/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "KV-cache PD分离 #todo 什么是PD分离，为什么要用PD分离",
    "description": "KV-cache PD分离 #todo 什么是PD分离，为什么要用PD分离",
    "tags": [
      "技术笔记"
    ],
    "title": "大模型工程实践",
    "uri": "/hugo-blog/blogs/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 跟进大模型进展 跟进大模型进展 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Memory Deep Agents Multi-agent 知识 Memory主要包含working、episodic、semantic和procedure四种 working基本就是短期的上文对话 episodic和context engineering主要相关，用于提取长上文摘要信息 semantic和rag主要相关，用于提取外部信息 procedure则更多通过模型训练方式内化到参数中，形成解决任务方式的记忆 Deep Agents是agent解决长程任务的一种范式，其中重要的一类任务是Deep Research，通过Agentic Search方式搜索信息生成详细的技术报告 Multi-agent目前主要受限于context共享和任务分工后产生的矛盾问题，基本采用简单的planner/executor的串行模式，基于上下文压缩解决过长的问题 待办 强化学习基础知识",
    "description": "总结 跟进大模型进展 跟进大模型进展 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Memory Deep Agents Multi-agent 知识 Memory主要包含working、episodic、semantic和procedure四种 working基本就是短期的上文对话 episodic和context engineering主要相关，用于提取长上文摘要信息 semantic和rag主要相关，用于提取外部信息 procedure则更多通过模型训练方式内化到参数中，形成解决任务方式的记忆 Deep Agents是agent解决长程任务的一种范式，其中重要的一类任务是Deep Research，通过Agentic Search方式搜索信息生成详细的技术报告 Multi-agent目前主要受限于context共享和任务分工后产生的矛盾问题，基本采用简单的planner/executor的串行模式，基于上下文压缩解决过长的问题 待办 强化学习基础知识",
    "tags": [
      "周记"
    ],
    "title": "Week9 大模型进展",
    "uri": "/hugo-blog/weekly/week9/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "NLP #todo langextract\n推荐",
    "description": "NLP #todo langextract\n推荐",
    "tags": [
      "技术笔记"
    ],
    "title": "大模型在其他技术中的应用",
    "uri": "/hugo-blog/blogs/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9C%A8%E5%85%B6%E4%BB%96%E6%8A%80%E6%9C%AF%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "什么是长程任务 长程任务（Long-horizon tasks）任务通常包含数十步甚至上百步的推理与操作，且往往涉及跨软件交互、长时间跨度和不确定的环境反馈\nDeep Agent Deep Agent一般认为由四部分组成：\nplanning tool：前期任务规划 将计划写入文件，标注状态（pending/working/finished） 后续更新状态，并在最终所有计划中的step完成时，结束任务 sub agents： 防止占用主agent的context 拥有专家能力 能力可复用 灵活的权限管理 file system：通过文件系统扩充context 与外部的交互通常token巨大，如web的html数据、pdf等 context过长容易导致模型能力衰减 context过长带来的成本问题 system prompt： 通常很长（几百甚至上千行）且很详细，结构清晰 包含few-shot prompt tool使用规范，包含好的和坏的使用样例 参考链接：\n# What are Deep Agents? # Implementing deepagents: a technical walkthrough # Build AI Agents That Work While You Sleep | Deep Agents Human-in-the-loop 在长程任务中的三大作用：\n遇到不可逆的高风险操作（例如：删除服务器数据、进行大额支付）时，需要人工确认 进入死循环（例如：网页改版导致爬虫失效，或者逻辑推理陷入死结）时，需要人工提示 人类对Agent每一个步骤的反馈（点赞、踩、修改意见）会被记录下来，作为RLHF的训练数据 长程任务有两大技术痛点，必须考 HITL 解决：\n意图对齐：用户说“帮我策划一场旅行”，Agent很难一次性猜准用户所有的偏好。通过HITL，Agent可以在第一阶段（选目的地）询问用户反馈，再进行第二阶段（订酒店） 长程衰减：即使是高级别的模型，在执行到第 20 步时，往往会忘记第一步的初衷。人类介入可以起到“锚点”作用，重置Agent的专注力 Human-in-the-loop通过checkpointer，记录了任务每个环节的状态，从而方便：\n暂停与恢复：到达需要人工审批的任务节点时，将状态存入数据库，然后进程可以完全关闭 回溯：让Agent回滚到每一个节点，从那个节点重新分支执行 故障恢复：执行过程中遇到网络中断或程序崩溃，保证中间结果不丢失 并发执行：在处理成千上万个长程任务时，系统通过thread_id知道哪个状态属于哪个用户 参考链接：\n# Adding Human-in-the-Loop to DeepAgents Agentic Search Agentic Search是指一种全新的搜索体验或产品形态，它改变了过去输入关键词给出链接列表的模式，而是寻找问题的答案或完成复杂的调研任务\n参考链接：\n# Framework-less Agentic Search Deep Research Deep Agent一个具体应用方向，通过分析用户问题，指定研究计划，agentic search的方式进行搜索和反思，最终形成一份用户问题相关的研究报告\n参考链接：\n# Open Deep Research 能力评估 BrowseCamp Plus 包含了需要多轮检索和推理的复杂的问题集合，被认为是衡量agent在复杂搜索和长程推理任务（Long-horizon tasks）上性能的重要标尺：\n固定语料库：提供一个包含约 10 万份文档的固定库，不会出现原版实时检索的结果波动 多维度解耦：它可以分别测试Retriever和Agent的表现 人工校验： 测试集中的答案和支撑文档都经过了人工验证，确保正确性 参考链接：\nBrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent 产品形态 Manus Context Engineering的一些思考 Manus中的Context Engineering\nContext Offloading的三层抽象 Function Calling：保留10个左右基础的工具调用，例如文件读写、shell命令执行、网页搜索等，这些原子工具写在system prompt中，基本固定 Sandbox Utilities：每个session都运行在虚拟环境中，环境中预安装了很多自定义的命令供agent调用 Packages \u0026 APIs：通过编写的python脚本直接请求api获取数据 技术细节 Q：agent如何知道有哪些命令，以及如何使用这些命令？ A：由于agent在sandbox中执行，可以在system prompt中加入hint，告诉他查看/usr/bin中的命令，由于这些命令都是manus自己开发的，都包含相同格式的–help文档，供agent查看学习\nQ：Manus在使用file system的时候是否实时构建索引，怎么搜索内容？ A：Manus没有实时构建索引，还是通过grep和glob命令来查询中间的文件结果，但是如果需要长期或者企业级知识库，还是需要建索引来完成\nQ：Manus如何处理长期的跨session的memory呢，是否也有像claude.md文件这样的东西？ A：每次session都会总结并向用户确认是否要保存这份知识（当前session的关键信息），目前也在探索如何自动化的处理用户偏好\nQ：Manus会随着模型进步逐渐简化一些基础逻辑吗？ A：会的，从25年3月到10月已经更新了五个版本，需要根据最新最强的模型的能力变化，预先判断架构是否仍然合理，做好准备\nQ：怎么确定文件的存储格式，是使用markdown、plain text还是什么？ A：一般使用line based的plain text，方便进行grep以及查询一定范围行数的内容，markdown有一个问题是某些模型厂商的模型在markdown上训练过于充分导致太容易出现bullet point\nQ：Summarization的prompt的怎么写的，如果写不好会损失信息，一般的做法是基于高召回来调整，Manus呢？ A：不要让模型自由发挥，设定一个类似schema的东西，一些field让模型填写，保持稳定以及保证某些内容一定会被总结到（比如目标）\nQ：Compaction就是把工具的结果写入文件，然后返回一个文件名吗？ A：是的，但是不光是文件，基本所有操作都有一个外部存储方式，比如网页操作有一个url、搜索操作有一个query，他们都天然的有一个用于compaction的外部存储方式\nQ：比如对于搜索，如果返回的token很多，是先做summarization在返回还是整体返回再等着compaction？ A：如果搜索是个复杂搜索，比如多个query那种，会调用高级搜索，本质就是一个sub-agent，他会进行agentic workflow的处理，然后返回固定格式的结果，但是如果是简单的搜索，比如google一下，就是全文返回，然后等待compaction，这种情况下也会让模型写下一些中间的insight存入文件\nQ：Manus是怎么处理agent和agent之间的交流的？ A：由于Manus是在sandbox中运行的，因此agent之间的sharing context就是共用sandbox就可以了，这不难，难得是怎么确保从sub-agent能得到期望的输出。我们指定主agent在创建sub-agent的时候，就需要制定好返回的structured output，然后在sub-agent返回结果的时候，使用限制解码来控制得到我们期望的输出\nQ：Manus是使用anthropic的模型，但是你们有尝试过开源模型吗，怎么选择的？ A：更多是成本上的考虑。当需要使用distributed kv-cache的时候，更前沿的模型厂商拥有更有保障的架构支持，反而比你自己实现成本更低。另外每个大模型厂商的模型技术优势各不相同（claude在代码上强，gemini在多模态上强，openai在数学和推理上强），可以进行task、subtask甚至step粒度的routing\nQ：Manus是不是一种混合模式，在有些时候使用原子操作，有些时候使用codeact模式？ A：是的，这很重要，因为如果都用codeact模式，没办法进行限制解码，导致结果不可控\nQ：Manus的planning是怎么样的？ A：一开始Manus也是使用todo.md，这样你会发现三分之一的操作都是在执行todo.md的更新，很浪费token，后来我们使用了一个agent as tool来专门规划，节省token\nQ：Manus的multi agent是怎么样规划设计的？ A：Manus的agent不是传统的按角色定义的，只有planner agent、executor agent、knowledge management agent以及api registration agent这么几个，我们对于添加agent很谨慎，因为agent之间的交流很难，更多的是使用agent as tool\nQ：Manus是如何做安全防护的？ A：由于Manus是运行在sandbox里的，只要能连接到互联网都不安全，所以我们会监控从sandbox流出的信息，确保不含有带有token信息的内容，即便这样，Manus在agent使用一些敏感操作时，还是会向用户确认，或者直接让用户接手\nQ：Manus是如何进行评估的？ A：一开始刚发布的时候是使用gaia评估集，但是发现分数高的模型反而用户不喜欢。目前是采取以下三种方式：1、每个完成的session都要求用户打分（1-5分）；2、自己构建了一些关于execution的自动化评测集，可以在sandbox里面跑测试；3、招实习生来标注，因为一些如web generation、data visualization的任务需要人工评估（很难有好的reward model）\nQ：Manus会基于提供的工具进行RL训练吗？ A：如果要支持MCP，agent的action space就会非常自由多变，这个情况下RL的方式很难训练。如果有充足的资源可以尝试，但是大模型的厂商都在做这件事，所以我们不需要重复造轮子，我们尽量使用parameter free的方式（不改变模型权重）\n参考链接：\nContext Engineering for AI Agents with LangChain and Manus #todo monica插件使用体验 OpenAI Deep Research lead agent根据用户query设计策略，创建子agent进行不同方面的探索 子agent通过多步搜索动态分析每次的搜索结果 citation agent单独在最终完成citation插入的定向任务 主要通过prompt engineering解决multi-agent的以下问题，\n简单query创建过多子agent 在web中持续搜索不存在内容 过度更新context产生互相干扰 如何有效评估multi-agent：\n从很小的评测集开始，快速迭代 到某一阶段后通过llm-as-judge来评估：事实性、引用准确、完整性、来源质量和工具效率等 人工校验自动化疏漏（通过prompt修改）：如自动化流程通常倾向于seo较好的文档来源 工业可靠性：\n通过agent的state来快速恢复出现错误之前的state并retry，包括tool call的错误 添加trace，包括决策、交互结构等，方便debug agent的更新升级，不是同时的（会导致运行中的agent报错），而是以agent粒度渐进的 期待后续实现异步调用，目前的同步方式会使通信成为瓶颈，且耗时（等待最慢的子agent） 参考链接：\n# How we built our multi-agent research system open-source prompts in our Cookbook Claude Code",
    "description": "什么是长程任务 长程任务（Long-horizon tasks）任务通常包含数十步甚至上百步的推理与操作，且往往涉及跨软件交互、长时间跨度和不确定的环境反馈\nDeep Agent Deep Agent一般认为由四部分组成：\nplanning tool：前期任务规划 将计划写入文件，标注状态（pending/working/finished） 后续更新状态，并在最终所有计划中的step完成时，结束任务 sub agents： 防止占用主agent的context 拥有专家能力 能力可复用 灵活的权限管理 file system：通过文件系统扩充context 与外部的交互通常token巨大，如web的html数据、pdf等 context过长容易导致模型能力衰减 context过长带来的成本问题 system prompt： 通常很长（几百甚至上千行）且很详细，结构清晰 包含few-shot prompt tool使用规范，包含好的和坏的使用样例 参考链接：\n# What are Deep Agents? # Implementing deepagents: a technical walkthrough # Build AI Agents That Work While You Sleep | Deep Agents Human-in-the-loop 在长程任务中的三大作用：",
    "tags": [
      "技术笔记"
    ],
    "title": "长程任务：Deep Agent",
    "uri": "/hugo-blog/blogs/%E9%95%BF%E7%A8%8B%E4%BB%BB%E5%8A%A1deep-agent/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Agent整体概括 Agent和workflow的主要区别在于，workflow的工作流是用户预定义好的，而Agent基于用户目标自己探索工作流。Agent的优势在于给予模型自由度，从而当模型的能力提升时，Agent的能力也会随着提升 Generative Agents通过在虚拟环境中多个agent的交互，证明拥有plan、reflect和memory能力的agent可以更好的作出符合人类共识的判断 Agent的四个核心部分：LLM + 规划 + 记忆 + 工具使用 参考链接：\n# Building effective agents A practical guide to building agents # LLM Powered Autonomous Agents Planning COT 通过强制模型将问题分成多个步骤，形成思维链（chain of thoughts），提升回复准确性 参考链接：\nChain-of-Thought Prompting Elicits Reasoning in Large Language Models TOT Tree of thoughts：构建解决任务的树状结构路径，在每一层扩展N种方案，并进行评估可行性，如果当前分支看起来行不通，可以回溯到之前的节点，尝试另一个分支 参考链接：\nTree of Thoughts: Deliberate Problem Solving with Large Language Models ReAct 将大模型交互的过程拆分为thought、action和observation的循环 结合大模型推理能力进行observation和thought，以及工具调用能力进行action 参考链接：\nREACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS # Understanding ReACT with LangChain # Building a LangGraph ReAct Mini Agent # Talking to a LangChain ReAct Voice Agent Plan and Excute 为了弥补ReAct中一步步方式可能导致的短视问题，采取先整体规划再执行的策略 经过Agent对task的处理后，判断是否已经完成，否则进行replan 参考链接：\nPlan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models ReWOO Reasoning Without Observation 相比ReAct和Plan and Excute每次执行一步观察的方式，ReWoo预先写下整体流程，中间执行结果用占位符代替，最终使用solver整体汇总给出答案，解耦了推理和执行，极大节省token和延时 参考链接：\nReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models Reflexion Reflection是指在推理-观察之后的反思步骤，相比之下ReAct只是观察后的下一步推理 Reflexion 可以被视为一种将 Reflection模式制度化、结构化的特定 Agent 框架 Reflexion框架包括三个核心部分： Actor（执行者）：负责尝试解决问题（生成尝试） Evaluator（评估者）：负责给执行结果打分（判断对错） Self- reflection（反思者）：这是一个拥有“长期记忆”的组件。它会分析失败的原因，并生成一条自然语言提示词（如：“下次不要用 X 库，改用 Y 库”），存入记忆库中 参考链接：\nReflexion: Language Agents with Verbal Reinforcement Learning # Reflection Agents LLM Compiler 和ReWOO的思想类似，是个进化版，做了以下两点优化： planner的输出变成stream，即在每个token输出后都检查是否有新任务，减少等待时间 task list转化为DAG图，只要发现依赖项确认，就直接执行，从而在一些可以并行的任务上并行调用工具 参考链接：\nAn LLM Compiler for Parallel Function Calling LATS LATS (Language Agent Tree Search)是一种将蒙特卡洛树搜索 (MCTS)与 LLM 的Reflection（反思）能力结合的高级规划框架 LATS主要分为以下六个步骤： selection：基于UCB（upper confidence bound，选择收益大且被选中次数少的）分数选择下一步该做什么 expansion：通过大模型进行扩展，进入下一层节点， evaluation：通过大模型给当前节点打分，用于后续selection，判断是否值得继续探索，同时进行剪枝以及局部reflection，防止扩展到不必要的节点 simulation：继续模拟直到截止条件 backpropagation：将最终结果反向传播回所有祖先节点，更新分数 和TOT的对比： 参考链接：\nLanguage Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models #todo system1、2、3\nTools CodeAct 将生成和执行可执行代码（如 Python）作为智能体与环境交互的统一接口，而不是传统的 JSON 或简单的工具调用 目前普遍大模型的代码能力极强，受益于大量的开源代码训练数据 代码执行后产生的错误信息天然作为reflection的输入，纠正模型的下一次行为 参考链接：\n# Executable Code Actions Elicit Better LLM Agents # LangGraph CodeAct Memory 参考 Cognitive Architectures for Language Agents，agent记忆主要分为四类：\nWorking Memory：对话过程中的工作记忆，即对话上下文 Episodic Memory：需要一段时间（每轮或更久）进行提炼得到的用户习惯，注意事项等，参考 Context Engineering学习手册 Semantic Memory：外部知识，可以理解为RAG手段获取的那部分信息，参考 RAG学习手册 Procedural Memory：大模型本身的行为习惯，通常是训练内化到模型能力中，类似于人的本能，参考 Agent训练 参考链接： # Building Brain-Like Memory for AI | LLM Agent Memory Systems\nMemGPT 提出“虚拟内存管理”概念，将向量数据库视为硬盘，将 Context Window 视为内存，实现按需调入\n参考链接：\nMemGPT: Towards LLMs as Operating Systems Mem0 Mem0采用了一种增量处理模式，主要分为两个阶段：\n提取阶段 (Extraction Phase)：通过结合当前的对话摘要和最近的消息序列，利用LLM动态提取出具有持久价值的“显著记忆”事实 更新阶段 (Update Phase)：对提取出的事实进行评估。系统会通过向量检索查找现有记忆，并使用LLM的“工具调用”能力决定对记忆库执行四种操作之一： 添加 (ADD) 更新 (UPDATE) 删除 (DELETE)（当新信息与旧记忆冲突时） 无操作 (NOOP) Mem0还提出了一个增强版本Mem0g。它利用基于图的记忆表示来捕捉实体之间复杂的关联结构，类似于GraphRAG\n参考链接：\nMem0: Building Production-Ready AI Agents with Scalable Long-Term Memory A-Mem A-Mem的工作流程模仿了卡片笔记的构建过程，主要包含以下四个步骤：\n笔记构建：当有新信息进入时，系统生成包含上下文描述、关键词和标签的结构化笔记卡片 链接生成：系统会分析新笔记与历史笔记之间的关联，自动建立语义链接 记忆进化：当新经验产生时，它不仅被添加，还会触发对旧记忆的更新，实现认知的持续演进 记忆检索：利用建立好的链接网络，进行关联检索 参考链接：\nA-Mem: Agentic Memory for LLM Agents #todo memr3\nMulti-agent multi-agent相比single agent带来的能力提升大多数（80%）归功于更多token的使用，其次是工具的数量和模型的选择，相比chatbot，agent的token使用量是4x，multi-agent一般是15x。\n这引出了multi-agent的使用场景：\n高价值产出的任务、可以并行化的任务、context过长的任务、使用复杂工具数量过多的任务 如果多个agent之间需要共享全部context或者有许多相互依赖，则不适合 参考链接：\n# How we built our multi-agent research system 应用实例：\nOpenAI Deep Research 为什么不要构建multi-agent agent通过context engineering维护可靠性（这里可靠性主要指任务完成能力），而子agent之间缺失的部分context会导致任务失败 - 原则1：共享context、以及完整的agent traces，而不只是消息 - 原则2：行为隐含了决策，而不同的决策会矛盾导致出错\n因此，multi-agent的结构最终只能简化为多个agent的串联，然而这在 长程任务中会导致context overflow的问题，需要agent之间进行context compression，但这很难： 可行的multi-agent：\nclaude code：子agent只搜索信息，不参与编程，保证不会出现子agent之间冲突的问题 多agent交互：通过子agent之间的讨论最终达成共识，解决冲突，但这目前很难 参考链接：\n# Don’t Build Multi-Agents 框架实现 langgraph 基于工具：构建handoffs工具，供agent调用 子agent共同连接到一个parent上，parent保存global的信息，每次子agent调用handoffs工具，会把message更新到global里，同时跳转到另一个子agent上 参考链接：\n# Building a multi-agent researcher with llms.txt # Understanding multi-agent handoffs A2A #todo a2a介绍\nswarm 参考链接：\nswarm autgen 参考链接：\nautogen 如何构建agent 构建可靠Agent的12个关键： 1. natural language to tool calls：将自然语言的需求转化为函数名、参数的能力 2. own your prompts：不依赖框架定义的prompt，自己写，方便调试迭代 3. own your context window：参考 Context Engineering学习手册 4. tools are structured outputs：大模型输出结构化的函数调用、函数执行、反馈结果给大模型 5. unify execution state and business state：执行和业务状态统一，减少复杂性，方便debug和恢复现场 6. launch/pause/resume：需要具备随时启动、暂停、继续的能力 7. contact humans with tool calls：总是通过工具调用的方式进行下一步（包括返回答案、澄清问题和调用工具），使得模型不至于需要在第一个token（“the” or “|JSON”）就明确下一步目标 8. own your control flow：自己定义运行逻辑，提高可控性，包括summarize/compaction、judge等 9. compact errors into context window：通过大模型的自愈能力来解决error 10. small, focused agents：把agent的功能聚焦，context可控、明确分工、方便测试和debug 11. tigger from anywhere, meet users where they are：在不同应用中出现，帮助用户（作者产品的广告） 12. make your agent a stateless reducer：agent尽可能是无状态的reducer，意味着每轮对话都是一个当前状态+用户输入到新状态的函数执行\n参考链接：\n# 12-Factor Agents: Patterns of reliable LLM applications LangChain # What is LangChain? # LangChain vs LangGraph: A Tale of Two Frameworks LangGraph 使用 LangGraph来搭建不同范式的workflow和Agent\t参考链接：\n# Workflows and agents # Building Effective Agents with LangGraph # LangGraph: Planning Agents Deep Agents 使用 deepagents来搭建Deep Agent\n自带各种工具及其对应中间件：planning、sub-agent delegation、filesystem 用户可以提供工具、指令以及sub-agent deepagents-quickstarts中包含了一个deep research的例子 参考链接：\n# Build a Research Agent with Deep Agents ADK Google在2025年发布的agent搭建开源框架，通过提供模块化、结构化的工具，帮助开发者更轻松地构建、评估和部署复杂的智能体系统\n#todo 试用adk 参考链接：\nADK是什么 如何训练agent LLM和agent的训练",
    "description": "Agent整体概括 Agent和workflow的主要区别在于，workflow的工作流是用户预定义好的，而Agent基于用户目标自己探索工作流。Agent的优势在于给予模型自由度，从而当模型的能力提升时，Agent的能力也会随着提升 Generative Agents通过在虚拟环境中多个agent的交互，证明拥有plan、reflect和memory能力的agent可以更好的作出符合人类共识的判断 Agent的四个核心部分：LLM + 规划 + 记忆 + 工具使用 参考链接：\n# Building effective agents A practical guide to building agents # LLM Powered Autonomous Agents Planning COT",
    "tags": [
      "技术笔记"
    ],
    "title": "Agent学习手册",
    "uri": "/hugo-blog/blogs/agent%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Context Rot 虽然大模型上下文窗口的上限不断增加，但任性的塞满上下文窗口其实反而会带来回复效果的损失，造成context rot现象 参考链接：\nContext Rot: How Increasing Input Tokens Impacts LLM Performance # How Long Contexts Fail Context Engineering 大模型回复的context应该包含以下几方面： system prompt：系统提示词，回复的高级指导 long-term/short-term memory：长短期，用户对话过程中对后续有用的关键信息 RAG：外部参考数据 tools：工具返回数据 structured output：回复的格式要求 user prompt：用户的上文 context engineering的工作主要包含以下四个方面： 写入：长期记忆（用户偏好的总结）、短期记忆（当前会话的摘要和状态） 选取：选取相关工具和知识、选取当前会话摘要和长期记忆 通过规则，比如指定读取某个memory文件 通过大模型，比如判断合适的工具 通过检索，比如RAG 压缩：基本分为compaction和summarization，经常同时使用 compaction：将信息offload到文件系统中，减少context长度，无损，可以回溯 summarization：直接通过大模型总结，容易造成信息损失 为什么需要压缩： # Don’t Build Multi-Agents 隔离：设定信息的边界，也可以理解为一种信息筛选 tool的调用相当于一种context隔离，只将tool需要的context传递 多agent相当于每个子agent对信息进行隔离，只关注自己需要处理的部分 参考链接： # Effective context engineering for AI agents # The New Skill in AI is Not Prompting, It’s Context Engineering # Context Engineering for Agents # Effective context engineering for AI agents # Build Hour: Agent Memory Patterns Manus中的Context Engineering 围绕KV缓存进行设计 考虑到大模型token成本在有缓存和无缓存情况下的价格相差巨大，且agent相比chatbot而言输入输出token比大得多（由于每一步都包含工具调用结果等），需要面向kv-cache缓存命中率设计context策略：\n保持前缀稳定，注意system prompt中不要包含时间戳 context只追加，注意一些json库会变动内部结构顺序 在需要时明确标记缓存断点，某些大模型厂商不支持自动前缀缓存，需要人工添加 遮蔽，而非移除 在agent任务迭代过程中，不要中途减少工具，这由于以下原因：\n工具定义一般在system prompt的上下，修改定义会导致前缀缓存失效 模型对于当前轮的观察如果基于不存在的工具，会导致下一步出现工具幻觉 更好的方式是对某些token的logits进行mask，从而达到杜绝工具使用的目的。一般模型厂商提供响应预填充选项：\n自动：模型可以选择调用或不调用函数，通过仅预填充回复前缀实现：\u003c|im_start|\u003eassistant 必需：模型必须调用函数，但选择不受约束。通过预填充到工具调用令牌实现：\u003c|im_start|\u003eassistant\u003ctool_call\u003e 指定：模型必须从特定子集中调用函数。通过预填充到函数名称的开头实现：\u003c|im_start|\u003eassistant\u003ctool_call\u003e{“name”: “browser_ 因此可以通过指定+mask指定工具的首token来完成对于对工具的禁用 使用文件系统作为上下文 固定窗口上下文具有以下三个痛点：\n观察结果可能非常庞大，尤其是当代理与网页或PDF等非结构化数据交互时，很容易溢出 模型性能往往会下降，超过一定的上下文长度后，即使技术上支持该窗口大小 长输入成本高昂，即使使用前缀缓存。你仍然需要为传输和预填充每个token付费 而文件系统大小不受限制，天然持久化，并且代理可以直接操作 通过复述操控注意力 Manus通过创建一个todo.md文件，并在任务进行过程中逐步更新它，勾选已完成的项目，来帮助agent保持对任务目标的注意力 保留错误的内容 当模型看到一个失败的行动，以及由此产生的观察结果或堆栈跟踪，它会隐式地更新其内部信念，即改变其先验，降低重复相同错误的可能性。因此不要将agent的错误痕迹擦除，保留在上下文中 不要被少样本示例所困 如果你的上下文充满了类似的过去行动-观察对，模型将倾向于遵循该模式，即使这不再是最优的。解决方法是增加few-shot的多样性，即不同的序列化模板、替代性措辞、顺序或格式上的微小噪音 参考链接：\n# Context Engineering for AI Agents: Lessons from Building Manus",
    "description": "Context Rot 虽然大模型上下文窗口的上限不断增加，但任性的塞满上下文窗口其实反而会带来回复效果的损失，造成context rot现象 参考链接：\nContext Rot: How Increasing Input Tokens Impacts LLM Performance # How Long Contexts Fail Context Engineering 大模型回复的context应该包含以下几方面： system prompt：系统提示词，回复的高级指导 long-term/short-term memory：长短期，用户对话过程中对后续有用的关键信息 RAG：外部参考数据 tools：工具返回数据 structured output：回复的格式要求 user prompt：用户的上文",
    "tags": [
      "技术笔记"
    ],
    "title": "Context Engineering学习手册",
    "uri": "/hugo-blog/blogs/context-engineering%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "RAG Retrieval-Augmented Generation 通过检索获取实时信息，补充模型预训练阶段缺失的知识 需要在两方面确保RAG能真正提升效果： retriever检索到的信息是高度相关且正确的 generator能辨别retriever提供信息的可靠性和正确性，或者抛弃检索信息 参考链接：\n../Answers/2025年RAG技术回顾与展望 CAG CAG（cache-augmented）通过将目标文档全文塞进prompt中，避免RAG中检索不精准的问题，目标文档通过kv-cache的方式预先计算，减少推理开销 关于CAG的存在意义以及其局限性： ../Answers/CAG的存在意义以及其局限性 参考链接：\n# RAG vs. CAG: Solving Knowledge Gaps in AI Models GraphRAG 索引阶段：对于每个chunk进行实体和关系的识别，对图进行社区发现算法，对于每一层社区进行llm摘要 检索阶段： global：主要针对概括性的问题，对每个社区（不同层级）进行llm判断，是否和query相关，排序top的部分进入prompt local：主要针对细节问题，先用向量检索实体，再对实体链接的社区摘要进行llm判断，是否和query相关，排序top的部分进入prompt 缺点： 无法很快更新知识数据，需要重跑社区发现算法和社区摘要生成 检索阶段的global方法需要计算每个摘要的相关度，也很耗费token 参考链接： From Local to Global: A GraphRAG Approach to Query-Focused Summarization # AI知识图谱 GraphRAG 是怎么回事？ graphrag neo4j系列视频 # Intro to GraphRAG — Zach Blumenfeld # GraphRAG: The Marriage of Knowledge Graphs and RAG # Practical GraphRAG: Making LLMs smarter with Knowledge Graphs # Agentic GraphRAG: AI’s Logical Edge LightRAG 对每个chunk中识别出的实体和链接，构造high-level和low-level的kv对，保存概括和细节信息 检索的时候将query也转成high-level和low-level，用向量匹配，匹配到的部分以及1-hop、2-hop进行打分排序，最终拼成prompt 索引阶段，相比GraphRAG需要跑社区检测算法，LightRAG只将新的节点添加进图即可，实时更新知识数据 参考链接：\nLIGHTRAG: SIMPLE AND FAST RETRIEVAL-AUGMENTED GENERATION Zep Zep = GraphRAG + 时序版本控制 + 工业级异步中间件\n三层子图：\nEpisode Subgraph：存储最原始的输入单元（消息、文本、JSON） Semantic Entity Subgraph：从情节（Episodes）中提取出的实体节点及其语义边（关系） Community Subgraph：对强相关的实体进行聚类形成的高阶摘要节点 两层存储：\n消息存储层：使用关系型数据库 (Postgres)，存储完整的对话原文、Token 计数、Session ID 知识存储层：使用图数据库 (Neo4j / Graphiti)，存储三层子图的所有节点和边 对话数据进入后，先存入消息存储，然后异步构建三个子图，子图中的内容都对应到消息存储中，包含了时间戳，保证新的事实替代旧的事实\n参考链接：\nZEP: A TEMPORAL KNOWLEDGE GRAPH ARCHITECTURE FOR AGENT MEMORY # Stop Using RAG as Memory Cognee 一个将非结构化数据转化为确定性图谱的框架 处理关系密集型的数据（如法律卷宗、医疗病历、企业内部文档） 在大模型应用中建立一套可维护、可演进的知识库，而不仅仅是临时的上下文补充 参考链接：\n# Cognee: Superior AI Memory \u0026 Knowledge For AI Agents! Greatly Beats ChatGPT! (Opensource) RAG评估 Ragas 基于大模型对检索质量和生成质量两方面进行评估： 检索质量： 召回：ground truth信息点中检索的覆盖比例，保证没有漏掉关键信息点 准确：检索中与question相关的比例，保证信息的纯净度 生成质量包括幻觉和相关性 支持自动生成测试集 通过大模型提取文档信息，构建临时的语义图 基于高信息文本快生成直接问题 基于语义图进行问题进化，包括多跳进化、推理进化、条件过滤进化、抽象进化 参考链接： Ragas: Automated Evaluation of Retrieval Augmented Generation Ragas Agentic RAG 相比传统的RAG只检索一次，然后拼到prompt里，Agentic RAG利用一个单独的agent来处理整体检索的行为，包括判断： 检索哪个数据源 使用什么工具 query是否需要改写 当前query是否足够进行检索，是否需要用户澄清 是否需要迭代式的检索 参考链接： # What is Agentic RAG? AGENTIC RETRIEVAL-AUGMENTED GENERATION: A SURVEY ON AGENTIC RAG # Build a custom RAG agent with LangGraph",
    "description": "RAG Retrieval-Augmented Generation 通过检索获取实时信息，补充模型预训练阶段缺失的知识 需要在两方面确保RAG能真正提升效果： retriever检索到的信息是高度相关且正确的 generator能辨别retriever提供信息的可靠性和正确性，或者抛弃检索信息 参考链接：\n../Answers/2025年RAG技术回顾与展望 CAG CAG（cache-augmented）通过将目标文档全文塞进prompt中，避免RAG中检索不精准的问题，目标文档通过kv-cache的方式预先计算，减少推理开销 关于CAG的存在意义以及其局限性： ../Answers/CAG的存在意义以及其局限性 参考链接：\n# RAG vs. CAG: Solving Knowledge Gaps in AI Models GraphRAG",
    "tags": [
      "技术笔记"
    ],
    "title": "RAG学习手册",
    "uri": "/hugo-blog/blogs/rag%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "MCP model context protocol在2024年底由anthropic开源， 用于方便大模型agent获取和操作内部数据或者外部API接口 MCP规定了MCP client（agent）和MCP server上的tool、resource、prompt之间的交互协议 MCP中的分工： MCP server MCP server的实现，以python版本github的list_repo_issues为例： from mcp.server.fastmcp import FastMCP import httpx import os # 1. 初始化 FastMCP # name 会显示在 AI 客户端中 mcp = FastMCP(\"GitHub Manager\") # 从环境变量获取 Token GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\") # 2. 定义一个工具 (Tool) # FastMCP 会根据函数签名、类型提示和 Docstring 自动生成 MCP 所需的 Schema @mcp.tool() async def list_repo_issues(owner: str, repo: str) -\u003e str: \"\"\" 获取指定 GitHub 仓库的公开 Issue 列表。 :param owner: 仓库所有者 (例如 'psf') :param repo: 仓库名称 (例如 'requests') \"\"\" url = f\"https://api.github.com/repos/{owner}/{repo}/issues\" headers = { \"Authorization\": f\"token {GITHUB_TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\" } async with httpx.AsyncClient() as client: response = await client.get(url, headers=headers) response.raise_for_status() issues = response.json() # 格式化输出给 AI 看 results = [] for issue in issues[:10]: # 仅取前10个 results.append(f\"#{issue['number']}: {issue['title']}\") return \"\\n\".join(results) if results else \"没有找到打开的 Issue。\" if __name__ == \"__main__\": # 3. 启动 Server（默认使用 stdio 传输） mcp.run() MCP server的配置 { \"mcpServers\": { \"my_python_github\": { \"command\": \"python3\", \"args\": [\"/你的绝对路径/github_server.py\"], \"env\": { \"GITHUB_TOKEN\": \"你的_PERSONAL_ACCESS_TOKEN\" } } } } MCP Host（client） MCP Host会把各个的大模型工具调用json转换为统一的MCP格式调用json，并在MCP server中进行调用（server通常是在Host本地起的一个子进程），从而用户只要关心MCP上不同工具的配置参数即可，不同厂家的模型都可以无缝使用这些工具 MCP Host开始只是大模型厂商在做，逐渐演变成编辑器（IDE）、容器（docker）等加入，只要支持对json的翻译能力就可以 MCP Host初始化 会进行工具查询，与配置文件里的所有 MCP Server 进行“握手” 当你开启一个“新对话”并输入第一句话时，Host 会把缓存里的工具定义转换成模型能懂的格式，塞进system prompt 专业的host甚至在每一轮对话都会传入，以便模型维持记忆，并进行动态筛选+缓存（prompt caching） MCP Gateway 使用MCP网关可以帮我们减少在host上的mcp server的配置工作，简单说就是只用配置一个MCP server，即MCP网关，而MCP网关内部帮我们配置了多个MCP server 如果后续切换Host，或者你有多个Host，也不用重新设置一遍配置，或者在多个Host修改配置 这种网关比如docker desktop： 参考链接：\n# Introducing the Model Context Protocol # What is the Model Context Protocol (MCP)? # MCP是啥？技术原理是什么？一个视频搞懂MCP的一切。Windows系统配置MCP，Cursor,Cline 使用MCP‘ # 用过上百款编程MCP，只有这15个真正好用，Claude Code与Codex配置MCP详细教程 # you need to learn MCP RIGHT NOW!! (Model Context Protocol) Smithery Skills Claude发布，用于将一些重复的能力固化在md文件中，当用户问题与之相关时，自动读取Skill的md文件，加载到prompt中 skill的主要特点，渐进式披露： 元数据按需加载：根据用户query，模型会根据skills的描述选择合适的skill，之后用户确认使用后，再将skill的全部内容发给模型查看 reference：在过程中如果需要某个引用文件（一般内容较多，如规章制度、法律法规等），可以在skill.md中添加对应的查看条件，做到skill内部文件的按需加载 script：在过程中如果需要调用脚本程序，也可以在skil.md中添加对应的使用条件，自动触发脚本执行，注意模型不查看scirpt内容，不消耗token Claude自带的create-skill这个skill可以帮助我们创建自己的skill，通过几轮对话完善一个定制化的skill的md文件 与MCP不同，MCP用于获取外部数据和工具，而Skills用于指导大模型如何做某件事，比如规范、要求等 参考链接：\n# Claude Skills Explained - Step-by-Step Tutorial for Beginners # Equipping agents for the real world with Agent Skills # 停止构建智能体，开始构建技能：Anthropic Agent Skills的深度洞察与AI范式变革 # Agent Skill 从使用到原理，一次讲清",
    "description": "MCP model context protocol在2024年底由anthropic开源， 用于方便大模型agent获取和操作内部数据或者外部API接口 MCP规定了MCP client（agent）和MCP server上的tool、resource、prompt之间的交互协议 MCP中的分工： MCP server MCP server的实现，以python版本github的list_repo_issues为例： from mcp.server.fastmcp import FastMCP import httpx import os # 1. 初始化 FastMCP # name 会显示在 AI 客户端中 mcp = FastMCP(\"GitHub Manager\") # 从环境变量获取 Token GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\") # 2. 定义一个工具 (Tool) # FastMCP 会根据函数签名、类型提示和 Docstring 自动生成 MCP 所需的 Schema @mcp.tool() async def list_repo_issues(owner: str, repo: str) -\u003e str: \"\"\" 获取指定 GitHub 仓库的公开 Issue 列表。 :param owner: 仓库所有者 (例如 'psf') :param repo: 仓库名称 (例如 'requests') \"\"\" url = f\"https://api.github.com/repos/{owner}/{repo}/issues\" headers = { \"Authorization\": f\"token {GITHUB_TOKEN}\", \"Accept\": \"application/vnd.github.v3+json\" } async with httpx.AsyncClient() as client: response = await client.get(url, headers=headers) response.raise_for_status() issues = response.json() # 格式化输出给 AI 看 results = [] for issue in issues[:10]: # 仅取前10个 results.append(f\"#{issue['number']}: {issue['title']}\") return \"\\n\".join(results) if results else \"没有找到打开的 Issue。\" if __name__ == \"__main__\": # 3. 启动 Server（默认使用 stdio 传输） mcp.run() MCP server的配置 { \"mcpServers\": { \"my_python_github\": { \"command\": \"python3\", \"args\": [\"/你的绝对路径/github_server.py\"], \"env\": { \"GITHUB_TOKEN\": \"你的_PERSONAL_ACCESS_TOKEN\" } } } } MCP Host（client） MCP Host会把各个的大模型工具调用json转换为统一的MCP格式调用json，并在MCP server中进行调用（server通常是在Host本地起的一个子进程），从而用户只要关心MCP上不同工具的配置参数即可，不同厂家的模型都可以无缝使用这些工具 MCP Host开始只是大模型厂商在做，逐渐演变成编辑器（IDE）、容器（docker）等加入，只要支持对json的翻译能力就可以 MCP Host初始化 会进行工具查询，与配置文件里的所有 MCP Server 进行“握手” 当你开启一个“新对话”并输入第一句话时，Host 会把缓存里的工具定义转换成模型能懂的格式，塞进system prompt 专业的host甚至在每一轮对话都会传入，以便模型维持记忆，并进行动态筛选+缓存（prompt caching） MCP Gateway 使用MCP网关可以帮我们减少在host上的mcp server的配置工作，简单说就是只用配置一个MCP server，即MCP网关，而MCP网关内部帮我们配置了多个MCP server 如果后续切换Host，或者你有多个Host，也不用重新设置一遍配置，或者在多个Host修改配置 这种网关比如docker desktop： 参考链接：",
    "tags": [
      "技术笔记"
    ],
    "title": "MCP和Skills",
    "uri": "/hugo-blog/blogs/mcp%E5%92%8Cskills/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 跟进大模型进展 跟进大模型进展 了解MCP和Skills： MCP和Skills 跟进RAG技术进展： RAG学习手册 了解Context Engineering： Context Engineering学习手册 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Planning Memory 知识 MCP统一模型调用外部接口的协议，从而让模型方和外部接口方共同努力实现自己到协议的转写，方便打通链路 Skills和workflow中的规划很像，只是用md文件表示，定义了统一的文件结构，方便模型理解workflow RAG演变到2025年主要采用图谱+向量的混合搜索方式（hybrid search），且通常采用agentic的方式进行图谱的构建 Context Engineering是agent如何选取对于当前轮对话最有用的context的技术 Agent=LLM + 规划 + 记忆 + 工具使用 人为定义好的规划，称为workflow，大模型在过程中实现自我规划，称为Agent 待办 跟进Agent技术进展",
    "description": "总结 跟进大模型进展 跟进大模型进展 了解MCP和Skills： MCP和Skills 跟进RAG技术进展： RAG学习手册 了解Context Engineering： Context Engineering学习手册 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Planning Memory 知识 MCP统一模型调用外部接口的协议，从而让模型方和外部接口方共同努力实现自己到协议的转写，方便打通链路 Skills和workflow中的规划很像，只是用md文件表示，定义了统一的文件结构，方便模型理解workflow RAG演变到2025年主要采用图谱+向量的混合搜索方式（hybrid search），且通常采用agentic的方式进行图谱的构建 Context Engineering是agent如何选取对于当前轮对话最有用的context的技术 Agent=LLM + 规划 + 记忆 + 工具使用 人为定义好的规划，称为workflow，大模型在过程中实现自我规划，称为Agent 待办 跟进Agent技术进展",
    "tags": [
      "周记"
    ],
    "title": "Week8 大模型进展",
    "uri": "/hugo-blog/weekly/week8/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: Javascript",
    "uri": "/hugo-blog/tags/javascript/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "新建项目 npx create-react-app appName node_modules：保存依赖的库 public：保存静态文件 manifest.json：记录app的元数据，如名字，主题，字体等 robots.txt：设置User-agent、Disallow和Allow，提供网络交互routing规则 src：源代码 index.js：app启动入口，连接到index.html的root节点 App.js：具体app逻辑，可以理解为html的index.html 语法为JSX（Javascript XML），将javascript和html结合 采用function component，即App这个函数，返回一个“动态的html” export default，外部可以复用 App.test.js：测试文件 reportWebVitals.js：性能测试文件 package.json：记录关键信息，例如依赖、版本、启动脚本等 package-lock.json：记录依赖版本，保证协同开发版本一致 参考链接：\n# Master React JS in easy way 基本概念 Components 只返回一个元素：需要把要返回的部分包起来，比如\u003cdiv\u003e或者空的\u003c\u003e mount指添加组件到DOM，unmount指从DOM移除组件 props：用来给Component返回的元素加入属性，来实现不同的具体内容 component中： function Greeting(props) { return \u003ch1\u003e{props.text}\u003c/h1\u003e } 使用中： \u003cGreeting text={'yo'}/\u003e key props：用于区分component，可以用数字或str，一般在map函数中使用： {items.map((item =\u003e ( \u003cComponent key={item.id} /\u003e ))} propTypes：用来确保传入的prop的属性类型正确 array用PropTypes.arrayof object用PropTypes.shape({x: PropTypes.xxx, y: PropTypes.yyy}) Student.propTypes = { name: PropTypes.string, age: PropTypes.number, isStudent: PropTypes.bool, } defaultProp：用来填充prop的默认值 Rendering 利用虚拟DOM（VDOM）进行渲染：react做的三步 当state改变，更新VDOM 通过diffs检查改变 reconciliation：协调改变真实DOM Hook State hooks：useState/useReducer 记录状态，返回状态变量和更新函数 const [count, setCount] = useState(0) 实现受控组件（controlled components），提供数据驱动的能力，将UI和用户行为产生的数据关联在一起 function ControlledInput() { const [value, setValue] = useState('') return ( \u003cinput value={value} onChange={(e) =\u003e setValue(e.target.value)} /\u003e ) } 与原生JS实现的区别： 立即执行 vs 函数引用\n立即执行：onClick={func()}，还没点就运行了 函数引用：onClick={() =\u003e func()} 或 onClick={func} ，只有点的那下才运行 监听函数传入一定是函数引用，否则 多次调用更新函数，react会batch处理，比如三次setValue(value+1)，使用同样的value，最终结果还是value+1，而不是value+3\n可以使用updater function，react会将函数放入队列，顺序执行，是个好习惯 function increment() { setValue(v =\u003e v + 1) setValue(v =\u003e v + 1) setValue(v =\u003e v + 1) } Context hooks：useContext 避免prop需要层层传递的情况，直接通过context交给底层 在生产者组件ComponentA中： import {createContext} from 'react'; export const MyContext = createContext(); 在组件中包裹child \u003cMyContext.Provider value={value}\u003e \u003cChild /\u003e \u003c/MyContext.Provider 在消费者组件中： 导入MyContext import React, { useContext } from 'react'; import { MyContext } from './ComponentA; const value = useContext(MyContext); MyContext只能通过value（规定属性名称）传递一个对象，可以把想要传递的所有内容组成一个大的对象传递下去，也是通用做法 Reference hooks：useRef 和useState一样，都是用来保存数据，但是不希望关联到页面渲染， 相比useState在每次值变化时更新渲染，useRef不会 使用ref的current来存储DOM对象，对current进行操作，从而： 直接访问/交互html的DOM元素 处理focus、animation、transition 管理timer和interval Effect hooks：useEffect 在组件主逻辑运行时的side code，额外做一些事 用effect包起来，可以精确控制执行的条件，如： 组件重新渲染：useEffect(() =\u003e {}) 组件mount：useEffect(() =\u003e {}, [])， []代表空依赖，只在mount时生效 组件内状态变化：useEffect(() =\u003e {}, [value])，在mount和状态值变化时生效 在effect里面返回一个箭头函数，用于组件unmount时清理资源，如remove listener 一般用于： 事件监听：组件mount的写法避免每次渲染都添加新的listener DOM 操作 订阅实时更新 从API获取数据 unmount组件 Performance hooks：useMemo/useCallback Purity 保证component纯净，即相同的输入对应相同的输出 component只返回JSX 不要在render之前在component外部修改component里面的元素 Portal Suspense 加载图标：需要获取数据的时候，提供更好的UX Error Boundaries 通过添加ErrorBoundary的FallbackComponent来控制错误出现时的反应 CSS styling external 提供global作用域的style，适合小项目 class名称在大型项目中可能会重复，导致覆盖和难以管理 module 普通css文件是全局生效的，但如果css文件名为xxx.module.css，vite或者react会识别这个文件名，会自动给你的类名加一个“随机后缀”（哈希值） 缺点包括：导致动态类名写起来麻烦、使用第三方库时，需要:global跳出局部作用域等 inline 除了用module方式，还可以使用inline的方式，即把css样式直接写在component的jsx文件里 适用于简单样式 参考链接：\n# React Full Course for free ⚛️ # Every React Concept Explained in 12 Minutes # ALL React Hooks Explained in 12 Minutes",
    "description": "新建项目 npx create-react-app appName node_modules：保存依赖的库 public：保存静态文件 manifest.json：记录app的元数据，如名字，主题，字体等 robots.txt：设置User-agent、Disallow和Allow，提供网络交互routing规则 src：源代码 index.js：app启动入口，连接到index.html的root节点 App.js：具体app逻辑，可以理解为html的index.html 语法为JSX（Javascript XML），将javascript和html结合 采用function component，即App这个函数，返回一个“动态的html” export default，外部可以复用 App.test.js：测试文件 reportWebVitals.js：性能测试文件 package.json：记录关键信息，例如依赖、版本、启动脚本等 package-lock.json：记录依赖版本，保证协同开发版本一致 参考链接：\n# Master React JS in easy way 基本概念 Components 只返回一个元素：需要把要返回的部分包起来，比如\u003cdiv\u003e或者空的\u003c\u003e mount指添加组件到DOM，unmount指从DOM移除组件 props：用来给Component返回的元素加入属性，来实现不同的具体内容 component中： function Greeting(props) { return \u003ch1\u003e{props.text}\u003c/h1\u003e } 使用中： \u003cGreeting text={'yo'}/\u003e key props：用于区分component，可以用数字或str，一般在map函数中使用： {items.map((item =\u003e ( \u003cComponent key={item.id} /\u003e ))} propTypes：用来确保传入的prop的属性类型正确 array用PropTypes.arrayof object用PropTypes.shape({x: PropTypes.xxx, y: PropTypes.yyy}) Student.propTypes = { name: PropTypes.string, age: PropTypes.number, isStudent: PropTypes.bool, } defaultProp：用来填充prop的默认值 Rendering 利用虚拟DOM（VDOM）进行渲染：react做的三步 当state改变，更新VDOM 通过diffs检查改变 reconciliation：协调改变真实DOM Hook State hooks：useState/useReducer 记录状态，返回状态变量和更新函数 const [count, setCount] = useState(0) 实现受控组件（controlled components），提供数据驱动的能力，将UI和用户行为产生的数据关联在一起 function ControlledInput() { const [value, setValue] = useState('') return ( \u003cinput value={value} onChange={(e) =\u003e setValue(e.target.value)} /\u003e ) } 与原生JS实现的区别：",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "React学习手册",
    "uri": "/hugo-blog/blogs/react%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 利用原生JS完成tetris 学习react基础知识 利用原生JS完成tetris 从零开始构建Tetris React基础知识学习 React学习手册 知识 React是javascript的library，通过JSX（javascript XML）编写，提供了一种通过compenent复用样式的能力，支持基于数据驱动的页面更新 待办 跟进大模型进展",
    "description": "总结 利用原生JS完成tetris 学习react基础知识 利用原生JS完成tetris 从零开始构建Tetris React基础知识学习 React学习手册 知识 React是javascript的library，通过JSX（javascript XML）编写，提供了一种通过compenent复用样式的能力，支持基于数据驱动的页面更新 待办 跟进大模型进展",
    "tags": [
      "周记"
    ],
    "title": "Week7 React学习",
    "uri": "/hugo-blog/weekly/week7/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 在完成css、html、javascript的基础上，构建Tetris游戏，以巩固以上知识点 游戏布局 整体游戏布局分为两部分： 主画面：20 * 10 的grid sidebar：包含以下几部分： next：4 * 4 的grid提示下一个是什么 分数栏：包含当前分数和最高分数 按钮栏：包含暂停（pause）和开始（play） Game Loop 用户进入界面后，游戏状态为“ready” 点击play按钮开始Game Loop，游戏状态进入“play” 每50帧，block向下一格 如果向下位置已经有颜色，则锁住当前block的颜色，新建block，同时清理整行 如果当前block无法新建，即新建位置之前被填充颜色，则失败 游戏过程中点击pause按钮暂停Game Loop，暂停所有keydown事件监听，游戏状态进入“pause” 用户点击play按钮继续Game Loop，游戏状态进入“play” 游戏失败后，游戏状态变为“end”，弹出对话框，显示分数，play按钮文本变为replay 用户点击replay按钮重新初始化，开始Game Loop，游戏状态进入“play” 技术细节 Grid背景 主画面的20 * 10的grid的显示，需要在scene中新建200个div，且每个div之间有gap，scene本身的背景颜色设为深色，div的背景颜色设为白色，这样gap会显示为深色的线 #scene \u003e div { background-color: white; } #scene { display: grid; grid-template-columns: repeat(10, 1fr); grid-template-rows: repeat(20, 1fr); gap: 1px; background-color: #999; /* 网格线颜色 */ border: 3px solid blue; margin: 10px; position: relative; } 移动和旋转 游戏过程中只有一个block是active的，也就是用户控制的 block的状态包含形状（index）、旋转（四种方向）和位置 let blockState = { index: 0, rotation: 0, x: 1, y: 4, } 通过block的index以及x、y，结合旋转，可以确定整个block的所有div 每次旋转90度：[x, y] = [y, -x] const TETROMINOES = [ // I 块 (直线) [ [-1, 0], [0, 0], [1, 0], [2, 0] ], // O 块 (方形) [ [0, 0], [-1, 0], [-1, 1], [0, 1] ], // L 块 [ [-1, 0], [0, 0], [1, 0], [1, 1] ], // J 块 [ [-1, 0], [0, 0], [1, 0], [-1, 1] ], // T 块 [ [-1, 0], [0, 0], [1, 0], [0, 1] ], // S 块 [ [-1, 0], [0, 0], [0, 1], [1, 1] ], // Z 块 [ [0, 0], [1, 0], [-1, 1], [0, 1] ], ]; 当block在边缘处旋转时，有超出边框的可能，这时候需要向里移动一格 新建Block 新建Block的时候，需要将之前设定的nextBlockIndex作为新的blockIndex，同时随机产生一个新的nextBlockIndex，根据这两个index渲染next的grid和scene的grid 如果发现新建Block的div上已经填充颜色，说明scene满了，游戏失败，返回false，否则返回true；Game Loop里通过这个返回值跳出 // 七种模块在next中的index const nextBlockIndices = [ [2, 6, 10, 14], // I [5, 6, 9, 10], // O [1, 5, 9, 10], // L [1, 2, 5, 9], // J [1, 5, 6, 9], // T [1, 5, 6, 10], // S [2, 5, 6, 9], // Z ] function NewBlock() { blockIndex = nextBlockIndex; blockState = { index: blockIndex, rotation: 0, x: 1, y: 4, }; let newBlockIndices = GetBlockAllIndices(blockState); for (let [newX, newY] of newBlockIndices) { if (blockColors[newX][newY] != \"white\") { return false; } } nextBlockIndex = Math.floor(Math.random() * 7); for (let i=0; i \u003c 16; i++) { if (nextBlockIndices[nextBlockIndex].includes(i)) { nextBlocks[i].style.background = index2Color[nextBlockIndex]; } else { nextBlocks[i].style.background = 'white'; } } console.log(`NewBlock: ${blockIndex}, ${nextBlockIndex}`); return true; } Ghost Block 为提升用户体验，为当前block预期掉落位置Ghost Block加border，方便用户通过空格键快速掉落block // 画预期掉落block (Ghost Block) // 1. 计算能掉落多远 let moveX = 0; while (canMoveBlock(moveX + 1, 0)) { // 注意这里要探测 +1 的位置 moveX++; } // 2. 只有当能移动时才画 ghost if (moveX \u003e 0) { let dropBlockState = {...blockState}; dropBlockState.x += moveX; let dropBlockIndices = GetBlockAllIndices(dropBlockState); for (let [x, y] of dropBlockIndices) { let dropIndex = x * colNum + y; // 确保不覆盖已经存在的方块颜色（虽然 ghost 通常在空白处，但为了保险） if (blockColors[x][y] === 'white') { sceneBlocks[dropIndex].style.border = `1px dashed ${index2Color[blockState.index]}`; } } }",
    "description": "背景 在完成css、html、javascript的基础上，构建Tetris游戏，以巩固以上知识点 游戏布局 整体游戏布局分为两部分： 主画面：20 * 10 的grid sidebar：包含以下几部分： next：4 * 4 的grid提示下一个是什么 分数栏：包含当前分数和最高分数 按钮栏：包含暂停（pause）和开始（play） Game Loop 用户进入界面后，游戏状态为“ready” 点击play按钮开始Game Loop，游戏状态进入“play” 每50帧，block向下一格 如果向下位置已经有颜色，则锁住当前block的颜色，新建block，同时清理整行 如果当前block无法新建，即新建位置之前被填充颜色，则失败 游戏过程中点击pause按钮暂停Game Loop，暂停所有keydown事件监听，游戏状态进入“pause” 用户点击play按钮继续Game Loop，游戏状态进入“play” 游戏失败后，游戏状态变为“end”，弹出对话框，显示分数，play按钮文本变为replay 用户点击replay按钮重新初始化，开始Game Loop，游戏状态进入“play” 技术细节 Grid背景 主画面的20 * 10的grid的显示，需要在scene中新建200个div，且每个div之间有gap，scene本身的背景颜色设为深色，div的背景颜色设为白色，这样gap会显示为深色的线 #scene \u003e div { background-color: white; } #scene { display: grid; grid-template-columns: repeat(10, 1fr); grid-template-rows: repeat(20, 1fr); gap: 1px; background-color: #999; /* 网格线颜色 */ border: 3px solid blue; margin: 10px; position: relative; } 移动和旋转 游戏过程中只有一个block是active的，也就是用户控制的 block的状态包含形状（index）、旋转（四种方向）和位置 let blockState = { index: 0, rotation: 0, x: 1, y: 4, } 通过block的index以及x、y，结合旋转，可以确定整个block的所有div 每次旋转90度：[x, y] = [y, -x] const TETROMINOES = [ // I 块 (直线) [ [-1, 0], [0, 0], [1, 0], [2, 0] ], // O 块 (方形) [ [0, 0], [-1, 0], [-1, 1], [0, 1] ], // L 块 [ [-1, 0], [0, 0], [1, 0], [1, 1] ], // J 块 [ [-1, 0], [0, 0], [1, 0], [-1, 1] ], // T 块 [ [-1, 0], [0, 0], [1, 0], [0, 1] ], // S 块 [ [-1, 0], [0, 0], [0, 1], [1, 1] ], // Z 块 [ [0, 0], [1, 0], [-1, 1], [0, 1] ], ]; 当block在边缘处旋转时，有超出边框的可能，这时候需要向里移动一格 新建Block 新建Block的时候，需要将之前设定的nextBlockIndex作为新的blockIndex，同时随机产生一个新的nextBlockIndex，根据这两个index渲染next的grid和scene的grid 如果发现新建Block的div上已经填充颜色，说明scene满了，游戏失败，返回false，否则返回true；Game Loop里通过这个返回值跳出 // 七种模块在next中的index const nextBlockIndices = [ [2, 6, 10, 14], // I [5, 6, 9, 10], // O [1, 5, 9, 10], // L [1, 2, 5, 9], // J [1, 5, 6, 9], // T [1, 5, 6, 10], // S [2, 5, 6, 9], // Z ] function NewBlock() { blockIndex = nextBlockIndex; blockState = { index: blockIndex, rotation: 0, x: 1, y: 4, }; let newBlockIndices = GetBlockAllIndices(blockState); for (let [newX, newY] of newBlockIndices) { if (blockColors[newX][newY] != \"white\") { return false; } } nextBlockIndex = Math.floor(Math.random() * 7); for (let i=0; i \u003c 16; i++) { if (nextBlockIndices[nextBlockIndex].includes(i)) { nextBlocks[i].style.background = index2Color[nextBlockIndex]; } else { nextBlocks[i].style.background = 'white'; } } console.log(`NewBlock: ${blockIndex}, ${nextBlockIndex}`); return true; } Ghost Block 为提升用户体验，为当前block预期掉落位置Ghost Block加border，方便用户通过空格键快速掉落block // 画预期掉落block (Ghost Block) // 1. 计算能掉落多远 let moveX = 0; while (canMoveBlock(moveX + 1, 0)) { // 注意这里要探测 +1 的位置 moveX++; } // 2. 只有当能移动时才画 ghost if (moveX \u003e 0) { let dropBlockState = {...blockState}; dropBlockState.x += moveX; let dropBlockIndices = GetBlockAllIndices(dropBlockState); for (let [x, y] of dropBlockIndices) { let dropIndex = x * colNum + y; // 确保不覆盖已经存在的方块颜色（虽然 ghost 通常在空白处，但为了保险） if (blockColors[x][y] === 'white') { sceneBlocks[dropIndex].style.border = `1px dashed ${index2Color[blockState.index]}`; } } }",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "从零开始构建Tetris",
    "uri": "/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAtetris/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: CSS",
    "uri": "/hugo-blog/tags/css/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础用法 id：采用id的方式（比如\u003cp id=“p1”\u003e），可以用#（比如#p1） class：给元素在html里加上class，在css里面可以针对class加格式，比如.class combinators descendant：使用“ ”，对所有子节点加格式，比如.class p child：使用“\u003e” ，对儿子节点加格式（不包括孙子及更远下属） general sibling：使用“～”，兄弟节点 adjacent sibling：使用“+”，相邻兄弟节点 伪class：定义元素在特殊状态时的格式，比如： 悬浮：hover（li:hover，li:not(:hover)） 选中：active 某个：nth-child（li:nth-child(2)，li:nth-child(even)） 伪元素：对某一类元素加细粒度格式（比如p的第一行），可以用::（比如p::first-line、p::selection） span或者div（称为container）用来分组，从而对各组或整体进行排版 span是inline的，只对内容生效，div是block的，对整体block生效 通过nested layout技术组合横向或竖向（display设为block/inline-block）的div，得到几乎任意排版效果 box model：padding就是字和border的距离，margin就是border和其他元素的距离 position： static：元素原本位置，不受left、right、top、bottom影响，也不作为上级给absolute做基准定位 relative：元素原本位置加上left、right、top、bottom的影响，多用于flex的下级元素 fixed：基于窗口（即browser view）作为基准定位的位置 absolute：基于上级（如果没有上级，即page）作为基准定位的位置 sticky：跟着scroll走 z-index：在html中的出现顺序决定了元素的默认渲染顺序，即后出现的在上面。可以通过设置z-index解决谁覆盖谁的问题（越大越靠上） grid：可以实现行列m x n布局，m和n也可以根据页面大小自适应调整 flexbox：可以方便地对container内部的元素进行动态的水平和垂直布局 可以解决两个div之间由于html换行导致的空格间隙 相比grid先定义行列再填充内容的方式，flexbox根据内容自动调整行列大小 参考链接： # Learn Flexbox CSS in 8 minutes transform：可以使对象旋转、缩放、变形等，会使用gpu加速，做动画场景时优先考虑 aspect-ratio：保持元素的比例，例如16:9 @media：考虑显示器的大小，实现相对应的不同样式，比如横向排版在宽度变小后改为纵向排版 @keyframes：利用transform、opacity、background-color等实现简单的动画效果 var：变量，实现属性的重复使用。变量名一般在:root伪作用域中定义，以双破折号–开头，通过var()函数来引用定义的值 :root { --primary-color: #3498db; /* 蓝色 */ --font-size-base: 16px; } body { background-color: var(--primary-color); font-size: var(--font-size-base); } calc：可以结合变量进行赋值操作，如animation-delay: calc(var(X)) * 100ms;",
    "description": "基础用法 id：采用id的方式（比如\u003cp id=“p1”\u003e），可以用#（比如#p1） class：给元素在html里加上class，在css里面可以针对class加格式，比如.class combinators descendant：使用“ ”，对所有子节点加格式，比如.class p child：使用“\u003e” ，对儿子节点加格式（不包括孙子及更远下属） general sibling：使用“～”，兄弟节点 adjacent sibling：使用“+”，相邻兄弟节点 伪class：定义元素在特殊状态时的格式，比如： 悬浮：hover（li:hover，li:not(:hover)） 选中：active 某个：nth-child（li:nth-child(2)，li:nth-child(even)） 伪元素：对某一类元素加细粒度格式（比如p的第一行），可以用::（比如p::first-line、p::selection） span或者div（称为container）用来分组，从而对各组或整体进行排版 span是inline的，只对内容生效，div是block的，对整体block生效 通过nested layout技术组合横向或竖向（display设为block/inline-block）的div，得到几乎任意排版效果 box model：padding就是字和border的距离，margin就是border和其他元素的距离 position： static：元素原本位置，不受left、right、top、bottom影响，也不作为上级给absolute做基准定位 relative：元素原本位置加上left、right、top、bottom的影响，多用于flex的下级元素 fixed：基于窗口（即browser view）作为基准定位的位置 absolute：基于上级（如果没有上级，即page）作为基准定位的位置 sticky：跟着scroll走 z-index：在html中的出现顺序决定了元素的默认渲染顺序，即后出现的在上面。可以通过设置z-index解决谁覆盖谁的问题（越大越靠上） grid：可以实现行列m x n布局，m和n也可以根据页面大小自适应调整 flexbox：可以方便地对container内部的元素进行动态的水平和垂直布局 可以解决两个div之间由于html换行导致的空格间隙 相比grid先定义行列再填充内容的方式，flexbox根据内容自动调整行列大小 参考链接： # Learn Flexbox CSS in 8 minutes transform：可以使对象旋转、缩放、变形等，会使用gpu加速，做动画场景时优先考虑 aspect-ratio：保持元素的比例，例如16:9 @media：考虑显示器的大小，实现相对应的不同样式，比如横向排版在宽度变小后改为纵向排版 @keyframes：利用transform、opacity、background-color等实现简单的动画效果 var：变量，实现属性的重复使用。变量名一般在:root伪作用域中定义，以双破折号–开头，通过var()函数来引用定义的值 :root { --primary-color: #3498db; /* 蓝色 */ --font-size-base: 16px; } body { background-color: var(--primary-color); font-size: var(--font-size-base); } calc：可以结合变量进行赋值操作，如animation-delay: calc(var(X)) * 100ms;",
    "tags": [
      "技术笔记",
      "CSS"
    ],
    "title": "CSS学习手册",
    "uri": "/hugo-blog/blogs/css%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Tags",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tag :: HTML",
    "uri": "/hugo-blog/tags/html/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础概念 HTML本质上 就是和markdown 一样的标记语言，用于给文本添加格式。\nmarkdown的设计目标是对应markup做的简化版本\ndata-* ../Answers/HTML中的data-* Canvas canvas创建 \u003ccanvas width=\"300\" height=\"200\"\u003e\u003c/canvas\u003e context 设置使用2D canvas，获取canvas的context，后续操作都是通过context\nconst ctx = canvas.getContext(\"2d\"); 绘制API 矩形 ctx.fillRect(x, y, width, height); 直线 ctx.beginPath(); ctx.moveTo(x1, y1); ctx.lineTo(x2, y2); ctx.stroke(); 图像 参数包括（可以分别选择前3、5、9个参数）： image、paste坐标（x1、y1）、paste大小（w1、h1）、原图坐标（x2、y2）、原图大小（w2、h2）\nconst img = new Image(); img.src = 'xxx.png'; ctx.drawImage(img, x1, y1, w1, h1, x2, y2, w2, h2); 参考链接：\n# How to Draw Images to HTML Canvas (JavaScript Tutorial) 更多链接： HTML5 Canvas Tutorials for Beginners # Canvas HTML5 JavaScript Full Tutorial # HTML Canvas DEEP DIVE",
    "description": "基础概念 HTML本质上 就是和markdown 一样的标记语言，用于给文本添加格式。\nmarkdown的设计目标是对应markup做的简化版本\ndata-* ../Answers/HTML中的data-* Canvas canvas创建 \u003ccanvas width=\"300\" height=\"200\"\u003e\u003c/canvas\u003e context 设置使用2D canvas，获取canvas的context，后续操作都是通过context",
    "tags": [
      "技术笔记",
      "HTML"
    ],
    "title": "HTML学习手册",
    "uri": "/hugo-blog/blogs/html%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "npm npm init在项目中来创建package.json –yes（-y）直接默认 package.json的作用： 管理项目依赖 scripts中支持脚本运行初始化构建项目 package-lock.json：由于不同时间npm install的包版本不一致，需要用这个文件来固定版本，保证不同人安装同样的版本 npm install安装依赖，创建node_modules目录存放 –save（-S）可以自动在package.json中记录 –save-dev（-D）表示只用于develop，不用于production -g：全局（global）安装，不会出现在package.json中，可以命令行执行，例如live-server @X.X.X：安装指定版本 ^X.X.X指保持大版本，小版本和patch更新到最新 ~X.X.X指保持大版本和小版本，patch更新到最新 *指更新到最新 npm list列出项目依赖 –depth 0，不显示子依赖，1，显示一级子依赖 –global true，显示全局安装 npm update更新依赖到指定的最新版本 npm prune删除package.json中不存在的已安装依赖 npm run运行package.json中scripts指定的命令 参考链接：\nnpm Tutorial for Beginners # NPM Full Course For Beginners - Learn NPM fundamentals and basics Yarn yarn add来安装包 yarn.lock对应package-lock.json的作用 yarn set version berry：设置为v2版本 v2版本支持pnp（plug and play）：Yarn 在您的本地文件系统上维护了一个全局缓存目录，所有通过 Yarn 下载的包都会被存储在这个目录中，且是zip压缩的，从而实现离线安装 参考链接：\n# Yarn Package Manager Crash Course pnpm 速度快 node modules中的包文件基于链接，避免重复的io操作 支持monorepos 避免flat结构的node modules带来的幻影依赖问题 参考链接：\n# What Is pnpm? # Why I Switched From NPM/Yarn to PNPM And Why You Should Too! 一些概念 幻影依赖 幻影依赖指的是您的项目代码中直接使用了某个包，但该包并没有在您的package.json文件的dependencies或devDependencies中明确声明。 比如你的包A依赖B，B依赖C，安装包A的依赖的时候会把B和C都安装到node modules中，是你在开发A时可以直接引用C，但是其实package.json中没有显式写出C。 这会导致，当后续B不再依赖C的时候，node moduels中将不再有C，你的包A中的对C的引用会突然失效。 用户项目 用户项目就是指最终使用您的代码，并直接在package.json中声明了对您的包依赖的那个应用程序或库。peerDependency就是指 peer Dependency 对比 处理 Peer Dependency 的方式是包管理工具（npm、Yarn、pnpm）之间差异最大的地方之一： 对于幻影依赖： 参考链接：\n# How JavaScript package managers work: npm vs. yarn vs. pnpm vs. npx Bun Bun是一个Javascript的运行时工具，包括前端、后端 包含了bundler、transpiler、任务执行和npm客户端的综合体 参考链接： ../Answers/Javascript中的bundler和transpiler 替代nodejs和npm，并兼容他们 基本操作 watch mode： bun –watch index.ts，监控改动反应到页面，类似node的nodemon bun –hot index.ts，相比watch不用手动reload页面 .env：存储环境变量，process.env.XXX或者bun.env.XXX bunx：不用安装直接运行，对应npx 支持sqlite 文件读写： 写： const data = 'I love Javascript'; await Bun.write('output.txt', data); 读： const file = await Bun.file('output.txt'); console.log(await file.text()); 测试： import { describe, expect, test, beforeAll } from 'bun:test' beforeAll(() =\u003e { // setup tests }); describe('math', () =\u003e { test('addition', () =\u003e { expect(2 + 2).toBe(4); }) }) bundler：将代码打包成可使用的js文件 bun build ./src/index.ts –outfile=./dist/bundle.js 同时支持watch mode 参考链接： Bun 1.0 # Bun Crash Course | JavaScript Runtime, Bundler \u0026 Transpiler Bun 1.3 bun index.html可以直接起服务 数据库支持增加redis Bun.secrets包裹的部分可以存在keychain中（macOS），提升安全性",
    "description": "npm npm init在项目中来创建package.json –yes（-y）直接默认 package.json的作用： 管理项目依赖 scripts中支持脚本运行初始化构建项目 package-lock.json：由于不同时间npm install的包版本不一致，需要用这个文件来固定版本，保证不同人安装同样的版本 npm install安装依赖，创建node_modules目录存放 –save（-S）可以自动在package.json中记录 –save-dev（-D）表示只用于develop，不用于production -g：全局（global）安装，不会出现在package.json中，可以命令行执行，例如live-server @X.X.X：安装指定版本 ^X.X.X指保持大版本，小版本和patch更新到最新 ~X.X.X指保持大版本和小版本，patch更新到最新 *指更新到最新 npm list列出项目依赖 –depth 0，不显示子依赖，1，显示一级子依赖 –global true，显示全局安装 npm update更新依赖到指定的最新版本 npm prune删除package.json中不存在的已安装依赖 npm run运行package.json中scripts指定的命令 参考链接：\nnpm Tutorial for Beginners # NPM Full Course For Beginners - Learn NPM fundamentals and basics Yarn yarn add来安装包 yarn.lock对应package-lock.json的作用 yarn set version berry：设置为v2版本 v2版本支持pnp（plug and play）：Yarn 在您的本地文件系统上维护了一个全局缓存目录，所有通过 Yarn 下载的包都会被存储在这个目录中，且是zip压缩的，从而实现离线安装 参考链接：",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "Javascript包管理",
    "uri": "/hugo-blog/blogs/javascript%E5%8C%85%E7%AE%A1%E7%90%86/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "基础用法 console.log：打印变量，可以在页面中inspect看到，或者使用node直接界面打印 对于多个{}的打印可以console.log({foo, bar, baz})可以看到每个map的名字 可以console.table来对结构化数据打印 console.time(XXX)和console.timeEnd(XXX)可以用来计时 console.trace可以追踪执行代码位置 ``：反引号表示模版字面量，类似formatted string 标签模版字面量：函数后面紧接着模版字面量 foo(strs, …values)，如foo`this is ${apple.id}`，strs接收被${}分开的部分，values接收${}的部分，从而自定义字符串解析和输出 可以作为DSL使用，如 ../Answers/标签模版字面量在DSL中的例子 ===：三个等号是全等，即对象类型和值均相等，两个等号只代表值相等 …：spread标识 可变参数，比如function foo (a, b, …c)，c获取剩余可变参数 将两个{}合并在一起，const ab = {…a, …b}; 给array添加元素，a = […a, “apple”, “banana”]; typeof：类型判断，如typeof xxx !== ‘function’ for循环： 对array（可迭代对象，包括map）可以用of for(ele of elements) {...} 对object属性可以用in for(property in element) {...} forEach函数接受的callback可以最多包含element、index、array三个参数 map函数相比forEach的区别：返回新的array；类似的还有filter reduce函数接受accumulator和element两个参数，最终返回一个element arrow function：一种函数定义的简化：() =\u003e … ()里面填参数，…填函数实现，多用于简单的one-liner fruits.forEach(fruit =\u003e console.log(fruit.calories)); this：使用当前对象作为this arrow function中的this的指向是代码位置确定的（词法定义域，Lexical Scope），而不是他被调用时确定的，因此在回调函数函数中可以绑定到当前对象（如果是普通函数作为回调函数，this在运行时才确定，已经和对象失联，只会绑定到global object） 可以用来实现constructor（ES6新特性支持class） super：使用父类方法 在子类中定义constructor时要首先使用父类的constructor，即super(xxx) setter和getter：用set和get关键字函数，定义class的私有属性的读写 外部只通过setter和getter读写私有属性，逻辑可控（比如类型检查） destructuring： 通过[]来解构array内容，例如swap a，b：[a,b] = [b,a] const elements = ['a', 'b', 'c', 'd', 'e']; // a = 'a', b = 'b', c = 'c', r = ['d', 'e'] const [a, b, c, ...r] = elements; 通过{}来解构object内容，可以在function的参数使用这个方式 function displayPerson({firstname, lastname}) { console.log(firstname); console.log(lastname); } sort：默认情况按lexicographic（字母+数字+符号）排序，即1、10、2、3… 可以添加cmp：numbers.sort((a,b) =\u003e a - b)，来进行数字排序 string属性不能用减法，转成NAN后都相等，需要a.name.localeCompare(b.name) shuffle：没有内置的shuffle，需要自己实现Fisher-Yates算法 setTimeout：等待一段时间（毫秒）后执行函数，可以通过clearTimeout取消 异常捕获： 使用try、catch、finally拦截异常 使用throw new Error()抛出异常 修改html： 添加三部曲：构建element、添加属性、插入DOM（基于父节点） 删除：removeChild，基于父节点 事件监听：eventListener 监听click、mouseover、mouseout、keydown、keyup事件 .addEventListener(event, callback) 可以通过DOMContentLoaded事件来等待dom加载完成后再加载图片 classList：通过api访问className，来修改元素的css类，相当于通过js给页面动态加样式 支持add、remove、toggle、replace、contains操作 JSON：stringify和parse实现js obect和json string的相互转换 fetch：异步函数，通过路径（本地或远程连接）读取数据 事件委托 ../Answers/Javascript中的事件委托 Module module：代码通过模块载入，类似python的import 在html的script标签里加上type=“module”，将index.js当作module载入 变量和函数前需要加上export关键字 在index.js头部加上import {变量名、函数名} from ‘文件路径’ 异步 setTimeout就是一个异步函数的例子，不阻碍主线程 一般通过callback、promise、async/await实现 当使用callback来串联异步函数时，会出现callback hell现象，即callback嵌套过度让代码可读性变差，可以通过promise或者async/await解决 promise：用来管理异步操作的对象 异步函数foo()返回promise对象，new Promise((resolve, reject)) =\u003e {异步逻辑} 用.then来承接异步函数，即foo().then(value =\u003e {…})，then里面写resolve处理逻辑 resolve函数可以返回另一个promise，用来串联下一个异步函数 用.catch来承接异步函数，即foo().catch(error =\u003e {…})，catch里面写reject处理逻辑 reject函数指定失败的逻辑，对整体异步链生效，无法针对其中的某个异步函数单独指定reject 如果想对某个异步函数实现单独失败处理，有如下两种方案： 方法一：.then中加入(error =\u003e {…})的部分 方法二：在异步函数中的reject参数中给出类型，在后续统一的catch中按类型实现单独逻辑 async/await：用同步的方式写异步 async让一个函数返回promise 是个语法糖，自动将函数返回结果包装在Promise.resolve()中 await让一个异步函数等待一个promise 通过写一个async关键词的函数来包含多个异步函数，每个异步函数用await来等待结果，用try/catch来捕获异步过程中的error，可以将上述promise的链式调用改写为同步风格的函数： function bar() { return new Promise((...)) } function fuz() { return new Promise((...)) } async function foo() { xxx = await bar(); yyy = await fuz(); } DOM的概念 DOM：Document Object Model 浏览器加载html构建DOM，将元素以树形结构展示 javascript可以通过DOM动态改变网页的内容、结构和样式 动态（live）获取，指查询后的改变能实时更新，类似于引用： getElementById：精准获取element getElementsClassName返回html collection，注意不等同于array，比如不支持forEach操作，可以用Array.from(XXX)来转化为array getElementsByTagName返回所有tag（如h2）下面的html collection 静态（static）获取，指查一次，子元素不再改变，类似于快照、拷贝： querySelector/querySelectorAll通过类似css的获取方式（./#） querySelector获取第一个element querySelectorAll获取nodelist，一般用forEach遍历 可以用console.dir(document)来显示结构 ES6新特性 let和const替代var： 明确的块级作用域， 避免变量提升（hoisting），鼓励先声明后使用 防止重复声明（var可以重复声明，后者覆盖前者） 引入const，var不具有const的属性 this：指向对象本身 只有在对象调用的时候才指向对象，否则指向global object（window） 可以通过bind将函数手动绑定到对象上来正确使用this 异步函数的回调函数中的this，不会自动绑定到对象上，需要使用arrow函数或者手动绑定 const person = { name: 'Alice', talk() { // 箭头函数继承了外部 talk() 的 this (即 person) setTimeout(() =\u003e { console.log(\"this\", this); // 输出: person }, 1000); } }; person.talk(); 解构： 同名结构，如果需要重命名，可以 address = { \"a\": 1, \"b\": 2 } const {a: e, b: f} = address // 这样 e 的值才是 address.a (即 1) …： 可以展开array，例如a.concat(b)等价于[…a, …b]，展开object，例如{…a, …b} 同样的可以用来clone array class： 预留constructor关键字作为构造函数名，类似之前的首字母大写的同名函数 继承：class b extends a { … }，需要在constructor中调用super() module： 对外部需要调用的class/function加上export关键字 named：export class a { … } default：export default class a { … } 对内部需要使用的部分进行import， named：import { a } from ‘文件路径’ default：import a from ‘文件路径’ 命名（named）导出和默认（default）导出： 命名导出只能有一个，而命名导出可以多个，import a, { b, c, d } from ‘文件路径’ 在html文件中需要标记\u003cscript type=‘module’ src=‘index.js’\u003e\u003c/script\u003e 面向对象 OOP的四大基本原则： encapsulation（封装）：对象的内部函数可以将属性用this关键词使用 abstraction（抽象）：定义接口，隐藏内部逻辑和变量 inheritance（继承）：减少重复代码 polymorphism（多态）：不同的子类对于相同函数签名的实现 构造函数： 函数名首字母大写，内部属性用this表示，隐式返回this 调用使用new关键字，否则当作普通函数对待 对象的属性可以随时增减，因为本质就是一个{} 可以用obj[‘xxx’] = yyy来新增属性（正常是obj.xxx = yyy），这样属性名可以不用写死 delete关键字删除属性 基本类型按值拷贝，对象（{}、函数、Array）按引用拷贝 函数内用let定义私有属性，防止外部使用 getter/setter： 通过Object.defineProperty来实现 Object.defineProperty(this, 'xxx', { get: function() { return xxx; }, set: function(value) { if (value is valid) xxx = value; } }) 或者直接定义set，get class Example { constructor(value) { this._value = value; // 约定：内部存储属性 } // 公共接口：控制对属性的访问 get value() { return this._value; } set value(newValue) { this._value = newValue; } } 扩展阅读 javascript游戏编程 Javascript Game Development Masterclass 2022",
    "description": "基础用法 console.log：打印变量，可以在页面中inspect看到，或者使用node直接界面打印 对于多个{}的打印可以console.log({foo, bar, baz})可以看到每个map的名字 可以console.table来对结构化数据打印 console.time(XXX)和console.timeEnd(XXX)可以用来计时 console.trace可以追踪执行代码位置 ``：反引号表示模版字面量，类似formatted string 标签模版字面量：函数后面紧接着模版字面量 foo(strs, …values)，如foo`this is ${apple.id}`，strs接收被${}分开的部分，values接收${}的部分，从而自定义字符串解析和输出 可以作为DSL使用，如 ../Answers/标签模版字面量在DSL中的例子 ===：三个等号是全等，即对象类型和值均相等，两个等号只代表值相等 …：spread标识 可变参数，比如function foo (a, b, …c)，c获取剩余可变参数 将两个{}合并在一起，const ab = {…a, …b}; 给array添加元素，a = […a, “apple”, “banana”]; typeof：类型判断，如typeof xxx !== ‘function’ for循环： 对array（可迭代对象，包括map）可以用of for(ele of elements) {...} 对object属性可以用in for(property in element) {...} forEach函数接受的callback可以最多包含element、index、array三个参数 map函数相比forEach的区别：返回新的array；类似的还有filter reduce函数接受accumulator和element两个参数，最终返回一个element arrow function：一种函数定义的简化：() =\u003e … ()里面填参数，…填函数实现，多用于简单的one-liner fruits.forEach(fruit =\u003e console.log(fruit.calories)); this：使用当前对象作为this arrow function中的this的指向是代码位置确定的（词法定义域，Lexical Scope），而不是他被调用时确定的，因此在回调函数函数中可以绑定到当前对象（如果是普通函数作为回调函数，this在运行时才确定，已经和对象失联，只会绑定到global object） 可以用来实现constructor（ES6新特性支持class） super：使用父类方法 在子类中定义constructor时要首先使用父类的constructor，即super(xxx) setter和getter：用set和get关键字函数，定义class的私有属性的读写 外部只通过setter和getter读写私有属性，逻辑可控（比如类型检查） destructuring： 通过[]来解构array内容，例如swap a，b：[a,b] = [b,a] const elements = ['a', 'b', 'c', 'd', 'e']; // a = 'a', b = 'b', c = 'c', r = ['d', 'e'] const [a, b, c, ...r] = elements; 通过{}来解构object内容，可以在function的参数使用这个方式 function displayPerson({firstname, lastname}) { console.log(firstname); console.log(lastname); } sort：默认情况按lexicographic（字母+数字+符号）排序，即1、10、2、3… 可以添加cmp：numbers.sort((a,b) =\u003e a - b)，来进行数字排序 string属性不能用减法，转成NAN后都相等，需要a.name.localeCompare(b.name) shuffle：没有内置的shuffle，需要自己实现Fisher-Yates算法 setTimeout：等待一段时间（毫秒）后执行函数，可以通过clearTimeout取消 异常捕获： 使用try、catch、finally拦截异常 使用throw new Error()抛出异常 修改html： 添加三部曲：构建element、添加属性、插入DOM（基于父节点） 删除：removeChild，基于父节点 事件监听：eventListener 监听click、mouseover、mouseout、keydown、keyup事件 .addEventListener(event, callback) 可以通过DOMContentLoaded事件来等待dom加载完成后再加载图片 classList：通过api访问className，来修改元素的css类，相当于通过js给页面动态加样式 支持add、remove、toggle、replace、contains操作 JSON：stringify和parse实现js obect和json string的相互转换 fetch：异步函数，通过路径（本地或远程连接）读取数据 事件委托 ../Answers/Javascript中的事件委托 Module module：代码通过模块载入，类似python的import 在html的script标签里加上type=“module”，将index.js当作module载入 变量和函数前需要加上export关键字 在index.js头部加上import {变量名、函数名} from ‘文件路径’ 异步 setTimeout就是一个异步函数的例子，不阻碍主线程 一般通过callback、promise、async/await实现 当使用callback来串联异步函数时，会出现callback hell现象，即callback嵌套过度让代码可读性变差，可以通过promise或者async/await解决 promise：用来管理异步操作的对象 异步函数foo()返回promise对象，new Promise((resolve, reject)) =\u003e {异步逻辑} 用.then来承接异步函数，即foo().then(value =\u003e {…})，then里面写resolve处理逻辑 resolve函数可以返回另一个promise，用来串联下一个异步函数 用.catch来承接异步函数，即foo().catch(error =\u003e {…})，catch里面写reject处理逻辑 reject函数指定失败的逻辑，对整体异步链生效，无法针对其中的某个异步函数单独指定reject 如果想对某个异步函数实现单独失败处理，有如下两种方案： 方法一：.then中加入(error =\u003e {…})的部分 方法二：在异步函数中的reject参数中给出类型，在后续统一的catch中按类型实现单独逻辑 async/await：用同步的方式写异步 async让一个函数返回promise 是个语法糖，自动将函数返回结果包装在Promise.resolve()中 await让一个异步函数等待一个promise 通过写一个async关键词的函数来包含多个异步函数，每个异步函数用await来等待结果，用try/catch来捕获异步过程中的error，可以将上述promise的链式调用改写为同步风格的函数： function bar() { return new Promise((...)) } function fuz() { return new Promise((...)) } async function foo() { xxx = await bar(); yyy = await fuz(); } DOM的概念 DOM：Document Object Model 浏览器加载html构建DOM，将元素以树形结构展示 javascript可以通过DOM动态改变网页的内容、结构和样式 动态（live）获取，指查询后的改变能实时更新，类似于引用： getElementById：精准获取element getElementsClassName返回html collection，注意不等同于array，比如不支持forEach操作，可以用Array.from(XXX)来转化为array getElementsByTagName返回所有tag（如h2）下面的html collection 静态（static）获取，指查一次，子元素不再改变，类似于快照、拷贝： querySelector/querySelectorAll通过类似css的获取方式（./#） querySelector获取第一个element querySelectorAll获取nodelist，一般用forEach遍历 可以用console.dir(document)来显示结构 ES6新特性 let和const替代var： 明确的块级作用域， 避免变量提升（hoisting），鼓励先声明后使用 防止重复声明（var可以重复声明，后者覆盖前者） 引入const，var不具有const的属性 this：指向对象本身 只有在对象调用的时候才指向对象，否则指向global object（window） 可以通过bind将函数手动绑定到对象上来正确使用this 异步函数的回调函数中的this，不会自动绑定到对象上，需要使用arrow函数或者手动绑定 const person = { name: 'Alice', talk() { // 箭头函数继承了外部 talk() 的 this (即 person) setTimeout(() =\u003e { console.log(\"this\", this); // 输出: person }, 1000); } }; person.talk(); 解构： 同名结构，如果需要重命名，可以 address = { \"a\": 1, \"b\": 2 } const {a: e, b: f} = address // 这样 e 的值才是 address.a (即 1) …： 可以展开array，例如a.concat(b)等价于[…a, …b]，展开object，例如{…a, …b} 同样的可以用来clone array class： 预留constructor关键字作为构造函数名，类似之前的首字母大写的同名函数 继承：class b extends a { … }，需要在constructor中调用super() module： 对外部需要调用的class/function加上export关键字 named：export class a { … } default：export default class a { … } 对内部需要使用的部分进行import， named：import { a } from ‘文件路径’ default：import a from ‘文件路径’ 命名（named）导出和默认（default）导出： 命名导出只能有一个，而命名导出可以多个，import a, { b, c, d } from ‘文件路径’ 在html文件中需要标记\u003cscript type=‘module’ src=‘index.js’\u003e\u003c/script\u003e 面向对象 OOP的四大基本原则： encapsulation（封装）：对象的内部函数可以将属性用this关键词使用 abstraction（抽象）：定义接口，隐藏内部逻辑和变量 inheritance（继承）：减少重复代码 polymorphism（多态）：不同的子类对于相同函数签名的实现 构造函数： 函数名首字母大写，内部属性用this表示，隐式返回this 调用使用new关键字，否则当作普通函数对待 对象的属性可以随时增减，因为本质就是一个{} 可以用obj[‘xxx’] = yyy来新增属性（正常是obj.xxx = yyy），这样属性名可以不用写死 delete关键字删除属性 基本类型按值拷贝，对象（{}、函数、Array）按引用拷贝 函数内用let定义私有属性，防止外部使用 getter/setter： 通过Object.defineProperty来实现 Object.defineProperty(this, 'xxx', { get: function() { return xxx; }, set: function(value) { if (value is valid) xxx = value; } }) 或者直接定义set，get class Example { constructor(value) { this._value = value; // 约定：内部存储属性 } // 公共接口：控制对属性的访问 get value() { return this._value; } set value(newValue) { this._value = newValue; } } 扩展阅读 javascript游戏编程 Javascript Game Development Masterclass 2022",
    "tags": [
      "技术笔记",
      "Javascript"
    ],
    "title": "Javascript学习手册",
    "uri": "/hugo-blog/blogs/javascript%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "官网链接",
    "description": "官网链接",
    "tags": [],
    "title": "像素工具Asperite",
    "uri": "/hugo-blog/blogs/%E5%83%8F%E7%B4%A0%E5%B7%A5%E5%85%B7asperite/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Game Loop Game Loop的结构通常是以下三步的循环： Process Input Update Game State Draw Game 参考链接： Game Programming Patterns # Game States and Game Loops # Getting The Game Loop Right 扩展阅读",
    "description": "Game Loop Game Loop的结构通常是以下三步的循环： Process Input Update Game State Draw Game 参考链接： Game Programming Patterns # Game States and Game Loops # Getting The Game Loop Right 扩展阅读",
    "tags": [
      "技术笔记"
    ],
    "title": "游戏编程基本概念",
    "uri": "/hugo-blog/blogs/%E6%B8%B8%E6%88%8F%E7%BC%96%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 Javascript进阶学习 利用原生JS搭建Flappy Bird 了解Javascript的包管理器 Javascript面向对象编程 Javascript学习手册-面向对象 课程链接： # Object-oriented Programming in JavaScript: Made Super Simple | Mosh 学习Javascript ES6新特性 Javascript学习手册-ES6新特性 课程链接： # ES6 Tutorial: Learn Modern JavaScript in 1 Hour 开发Flappy Bird 从零开始构建Flappy Bird 了解Javascript的包管理器 Javascript包管理 知识 Javascript ES6 2015年发布，主要引入let、const、class、module等 Javascript的包管理器有npm、yarn、pnpm等 Bun作为nodejs的运行时替代，也包含包管理器，可以完成前后端整体链路 待办 利用原生JS搭建俄罗斯方块 学习React框架基础知识",
    "description": "总结 Javascript进阶学习 利用原生JS搭建Flappy Bird 了解Javascript的包管理器 Javascript面向对象编程 Javascript学习手册-面向对象 课程链接： # Object-oriented Programming in JavaScript: Made Super Simple | Mosh 学习Javascript ES6新特性 Javascript学习手册-ES6新特性 课程链接： # ES6 Tutorial: Learn Modern JavaScript in 1 Hour 开发Flappy Bird 从零开始构建Flappy Bird 了解Javascript的包管理器 Javascript包管理 知识 Javascript ES6 2015年发布，主要引入let、const、class、module等 Javascript的包管理器有npm、yarn、pnpm等 Bun作为nodejs的运行时替代，也包含包管理器，可以完成前后端整体链路 待办 利用原生JS搭建俄罗斯方块 学习React框架基础知识",
    "tags": [
      "周记"
    ],
    "title": "Week6 Flappy Bird",
    "uri": "/hugo-blog/weekly/week6/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 在完成css、html、javascript的基础上，构建Flappy Bird游戏，以巩固以上知识点 预备知识： HTML学习手册-Canvas 游戏编程基本概念-Game Loop 在线试玩 Flappy Bird Game Loop Flappy Bird的游戏逻辑设计 home：主页，显示标题和两个button（start、score），目前只实现了start ready：主页点击start进入ready页面，给出游戏提示“tap” play：在ready页面tap后进入游戏页面，包含： 初始化：小鸟、pipe和实时分数 监听用户tap： 没有tap的话有向下加速度，模拟重力 有tap行为则直接给一个向上的速度 记分：小鸟每次经过一个pipe，score加1，score超过best则覆盖best 结束：小鸟和pipe的碰撞检测，碰到上下边缘或者pipe则结束，进入结束页面 over： 结束页面显示记分牌，显示奖牌（是best则金牌，不然白牌），显示score和best 给出ok和menu按钮，其中ok回到ready页，menu回到home页 技术细节 精灵图集 Sprite 精灵图集将游戏中所有的小图片（如小鸟、管道、背景、按钮等）集中在一个大文件里 在网页中获取的时候，可以使用CSS Sprites技术：使用 background-image 和 background-position 两个属性来在网页元素中显示精灵图集上的某一个图标或图片 可以使用 Sprite Cow对精灵图集进行分割处理，得到每个图标的position 注意图标推荐放在div或者span中，而不是img中 如何实现游戏基本逻辑 利用requestAnimationFrame（简单来说，rAF告诉浏览器：“我要执行一个动画，请在下一次重绘（Repaint）之前调用我的回调函数。”），将Game Loop函数作为callback，从而不断刷新游戏状态，实现元素的移动，以及整体Game Loop function GameLoop() { //移动小鸟 ... // 更新游戏分数 updateGameScore(); // 更新pipes的状态 updatePipes(); // 边界碰撞检测 if (birdPosY \u003c 0 || birdPosY + 22 \u003e 400) { console.log(\"Hit Boundary\"); game_state = \"ended\"; } // pipe碰撞检测 ... if (game_state === \"ended\") { ... console.log(\"Game Over!\"); return; } requestAnimationFrame(GameLoop); } 如何实现背景的移动 css中将背景设置为精灵图集中的一部分，且repeat-x，水平循环 #scene { position: relative; /* 改为相对定位，由 flex 控制居中 */ border: 5px solid red; box-sizing: border-box; /* 让 border 包含在宽高内 */ display: flex; align-items: center; justify-content: center; flex-direction: column; background: url('../images/flappy-bird-sprite.png') repeat-x 0 0; image-rendering: pixelated; width: 225px; /* 原始宽度 */ height: 400px; /* 原始高度 */ overflow: hidden; /* 确保内容不溢出游戏画面 */ } 在js中设置scene的backgroundPostion可以整体移动 scene.style.backgroundPosition = `-${backgroundX}px 0px`; 如何实现小鸟的移动 水平方向上不移动，固定在屏幕30%位置上 #bird { position: absolute; background: url('../images/flappy-bird-sprite.png') no-repeat -179px -513px; width: 28px; height: 22px; top: 200px; left: 30%; transform: translateX(-50%); } 垂直方向上，根据游戏逻辑，确定位置后，通过top来实现移动 // 重力加速度下降 birdSpeedY += 0.1; birdPosY = birdPosY + birdSpeedY; bird.style.top = `${birdPosY}px`; 如何实现小鸟扇动翅膀 对于小鸟这个div，根据时间不断切换精灵图集中的backgroundPosition，从而切换不同图片 // 小鸟扇翅膀动画 birdFrameTimer++; if (birdFrameTimer % birdWingRate === 0) { // 每10帧切换一次图片，数字越小越快 birdFrameIndex = (birdFrameIndex + 1) % birdFrames.length; bird.style.backgroundPosition = birdFrames[birdFrameIndex]; if (birdFrameTimer === birdWingRate) { birdFrameTimer = 0; } } 如何实现管道的平移 在html中写三组pipe元素 在js中通过transform水平移动，如果移动超过屏幕，则从最左侧重置到最右侧 // 如果移出屏幕左侧，移动到最右侧 if (pipe.x \u003c -pipeWidth) { // 找到当前最右边的管道的 x 坐标 let maxX = Math.max(...pipesData.map(p =\u003e p.x)); pipe.x = maxX + pipeDistance; // 重新随机高度 pipe.y = Math.floor(Math.random() * (300 - 100)) + 100; pipe.scored = false; } 如何实现页面切换 在html中写在一起 \u003cdiv id=\"scene\"\u003e \u003c!-- 首页场景 --\u003e \u003cdiv id=\"scene_home\"\u003e ... \u003c/div\u003e \u003c!-- 游戏准备场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_ready\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_play\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏结束场景（默认隐藏）--\u003e \u003cdiv id=\"scene_over\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c/div\u003e 然后通过在js中设定某一个部分（div）的display是否为none，来切换显示不同部分 document.getElementById(\"ok_button\").addEventListener('click', function(){ // 隐藏首页场景 document.getElementById(\"scene_over\").style.display = 'none'; // 隐藏游戏层 document.getElementById(\"scene_play\").style.display = 'none'; // 显示游戏场景 document.getElementById(\"scene_ready\").style.display = 'flex'; // 游戏状态进入ready game_state = \"ready\"; }); 代码仓库 github链接： flappy-bird",
    "description": "背景 在完成css、html、javascript的基础上，构建Flappy Bird游戏，以巩固以上知识点 预备知识： HTML学习手册-Canvas 游戏编程基本概念-Game Loop 在线试玩 Flappy Bird Game Loop Flappy Bird的游戏逻辑设计 home：主页，显示标题和两个button（start、score），目前只实现了start ready：主页点击start进入ready页面，给出游戏提示“tap” play：在ready页面tap后进入游戏页面，包含： 初始化：小鸟、pipe和实时分数 监听用户tap： 没有tap的话有向下加速度，模拟重力 有tap行为则直接给一个向上的速度 记分：小鸟每次经过一个pipe，score加1，score超过best则覆盖best 结束：小鸟和pipe的碰撞检测，碰到上下边缘或者pipe则结束，进入结束页面 over： 结束页面显示记分牌，显示奖牌（是best则金牌，不然白牌），显示score和best 给出ok和menu按钮，其中ok回到ready页，menu回到home页 技术细节 精灵图集 Sprite 精灵图集将游戏中所有的小图片（如小鸟、管道、背景、按钮等）集中在一个大文件里 在网页中获取的时候，可以使用CSS Sprites技术：使用 background-image 和 background-position 两个属性来在网页元素中显示精灵图集上的某一个图标或图片 可以使用 Sprite Cow对精灵图集进行分割处理，得到每个图标的position 注意图标推荐放在div或者span中，而不是img中 如何实现游戏基本逻辑 利用requestAnimationFrame（简单来说，rAF告诉浏览器：“我要执行一个动画，请在下一次重绘（Repaint）之前调用我的回调函数。”），将Game Loop函数作为callback，从而不断刷新游戏状态，实现元素的移动，以及整体Game Loop function GameLoop() { //移动小鸟 ... // 更新游戏分数 updateGameScore(); // 更新pipes的状态 updatePipes(); // 边界碰撞检测 if (birdPosY \u003c 0 || birdPosY + 22 \u003e 400) { console.log(\"Hit Boundary\"); game_state = \"ended\"; } // pipe碰撞检测 ... if (game_state === \"ended\") { ... console.log(\"Game Over!\"); return; } requestAnimationFrame(GameLoop); } 如何实现背景的移动 css中将背景设置为精灵图集中的一部分，且repeat-x，水平循环 #scene { position: relative; /* 改为相对定位，由 flex 控制居中 */ border: 5px solid red; box-sizing: border-box; /* 让 border 包含在宽高内 */ display: flex; align-items: center; justify-content: center; flex-direction: column; background: url('../images/flappy-bird-sprite.png') repeat-x 0 0; image-rendering: pixelated; width: 225px; /* 原始宽度 */ height: 400px; /* 原始高度 */ overflow: hidden; /* 确保内容不溢出游戏画面 */ } 在js中设置scene的backgroundPostion可以整体移动 scene.style.backgroundPosition = `-${backgroundX}px 0px`; 如何实现小鸟的移动 水平方向上不移动，固定在屏幕30%位置上 #bird { position: absolute; background: url('../images/flappy-bird-sprite.png') no-repeat -179px -513px; width: 28px; height: 22px; top: 200px; left: 30%; transform: translateX(-50%); } 垂直方向上，根据游戏逻辑，确定位置后，通过top来实现移动 // 重力加速度下降 birdSpeedY += 0.1; birdPosY = birdPosY + birdSpeedY; bird.style.top = `${birdPosY}px`; 如何实现小鸟扇动翅膀 对于小鸟这个div，根据时间不断切换精灵图集中的backgroundPosition，从而切换不同图片 // 小鸟扇翅膀动画 birdFrameTimer++; if (birdFrameTimer % birdWingRate === 0) { // 每10帧切换一次图片，数字越小越快 birdFrameIndex = (birdFrameIndex + 1) % birdFrames.length; bird.style.backgroundPosition = birdFrames[birdFrameIndex]; if (birdFrameTimer === birdWingRate) { birdFrameTimer = 0; } } 如何实现管道的平移 在html中写三组pipe元素 在js中通过transform水平移动，如果移动超过屏幕，则从最左侧重置到最右侧 // 如果移出屏幕左侧，移动到最右侧 if (pipe.x \u003c -pipeWidth) { // 找到当前最右边的管道的 x 坐标 let maxX = Math.max(...pipesData.map(p =\u003e p.x)); pipe.x = maxX + pipeDistance; // 重新随机高度 pipe.y = Math.floor(Math.random() * (300 - 100)) + 100; pipe.scored = false; } 如何实现页面切换 在html中写在一起 \u003cdiv id=\"scene\"\u003e \u003c!-- 首页场景 --\u003e \u003cdiv id=\"scene_home\"\u003e ... \u003c/div\u003e \u003c!-- 游戏准备场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_ready\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏场景 (默认隐藏) --\u003e \u003cdiv id=\"scene_play\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c!-- 游戏结束场景（默认隐藏）--\u003e \u003cdiv id=\"scene_over\" style=\"display: none;\"\u003e ... \u003c/div\u003e \u003c/div\u003e 然后通过在js中设定某一个部分（div）的display是否为none，来切换显示不同部分 document.getElementById(\"ok_button\").addEventListener('click', function(){ // 隐藏首页场景 document.getElementById(\"scene_over\").style.display = 'none'; // 隐藏游戏层 document.getElementById(\"scene_play\").style.display = 'none'; // 显示游戏场景 document.getElementById(\"scene_ready\").style.display = 'flex'; // 游戏状态进入ready game_state = \"ready\"; }); 代码仓库 github链接： flappy-bird",
    "tags": [
      "技术笔记"
    ],
    "title": "从零开始构建Flappy Bird",
    "uri": "/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAflappy-bird/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 完成neovim环境搭建 熟练vim操作 完成Javascript基础知识学习 neovim配置 学习Lua的基础知识： Lua学习手册 配置Neovim环境，安装所需插件： Vim和NeoVim 学习javascript基础知识 Javascript学习手册-基础用法 课程链接 # JavaScript Full Course for free # JavaScript Tutorial Full Course - Beginner to Pro 知识 NeoVim是Vim的新版本重构 通过LazyNvim插件配置 支持语法高亮、语义补全、代码跳转、git等几乎全部所需功能 基本替代vscode，熟练掌握vim操作后，效率提升 html负责内容、css负责样式、javascript负责交互 通过nodejs的live-server可以同步更新目录内的网页状态，类似vscode里的go live插件 node开启javascript命令行 待办 了解Javscript进阶知识：ES6+新特性 Flappy Bird开发",
    "description": "总结 完成neovim环境搭建 熟练vim操作 完成Javascript基础知识学习 neovim配置 学习Lua的基础知识： Lua学习手册 配置Neovim环境，安装所需插件： Vim和NeoVim 学习javascript基础知识 Javascript学习手册-基础用法 课程链接 # JavaScript Full Course for free # JavaScript Tutorial Full Course - Beginner to Pro 知识 NeoVim是Vim的新版本重构 通过LazyNvim插件配置 支持语法高亮、语义补全、代码跳转、git等几乎全部所需功能 基本替代vscode，熟练掌握vim操作后，效率提升 html负责内容、css负责样式、javascript负责交互 通过nodejs的live-server可以同步更新目录内的网页状态，类似vscode里的go live插件 node开启javascript命令行 待办 了解Javscript进阶知识：ES6+新特性 Flappy Bird开发",
    "tags": [
      "周记"
    ],
    "title": "Week5 Javascript学习",
    "uri": "/hugo-blog/weekly/week5/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 优化本地开发环境 tmux配置 Tmux使用教程 aerospace配置 Aerospace使用教程 iterm2配置 Terminal配置 知识 通过tmux解决同一个shell下多开进程的操作方式 tmux分为session、window、pane，每个session下可以快速切换window和pane 通过aerospace可以通过键盘快速切换操作页面，如chrome、terminal、obsidian之间 linux上使用i3，macOS上使用aerospace terminal配置主要包含以下几步 安装terminal 安装color themes 安装字体：支持定制化图标 安装插件：powerlevel10k（未使用）、zsh的语义补全和语法高亮 待办 配置neovim环境 熟练vim操作",
    "description": "总结 优化本地开发环境 tmux配置 Tmux使用教程 aerospace配置 Aerospace使用教程 iterm2配置 Terminal配置 知识 通过tmux解决同一个shell下多开进程的操作方式 tmux分为session、window、pane，每个session下可以快速切换window和pane 通过aerospace可以通过键盘快速切换操作页面，如chrome、terminal、obsidian之间 linux上使用i3，macOS上使用aerospace terminal配置主要包含以下几步 安装terminal 安装color themes 安装字体：支持定制化图标 安装插件：powerlevel10k（未使用）、zsh的语义补全和语法高亮 待办 配置neovim环境 熟练vim操作",
    "tags": [
      "周记"
    ],
    "title": "Week4 开发环境优化",
    "uri": "/hugo-blog/weekly/week4/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 Aerospace是linux系统上i3窗口管理器应用的macOS系统上的替代应用\n支持设置多个workspace，在workspace内管理（平铺或者移动）窗口 支持给每个workspace设定快捷键，进行快速切入 使用说明 配置文件：~/.config/aerospace/aerospace.toml\nmain mode： 设置workspace快捷键：alt+数字/字母 切换窗口：alt+hjkl 移动窗口：alt+shift+hjkl 放大/缩小窗口：alt+shift+（=-） 切换横竖分割：alt+/ 切换横竖全屏：alt+, service mode： reload config：alt+shift+; 相关链接：\n# Aerospace Is The Best Tiling Window Manager I’ve Tried On macOS",
    "description": "背景 Aerospace是linux系统上i3窗口管理器应用的macOS系统上的替代应用\n支持设置多个workspace，在workspace内管理（平铺或者移动）窗口 支持给每个workspace设定快捷键，进行快速切入 使用说明 配置文件：~/.config/aerospace/aerospace.toml\nmain mode： 设置workspace快捷键：alt+数字/字母 切换窗口：alt+hjkl 移动窗口：alt+shift+hjkl 放大/缩小窗口：alt+shift+（=-） 切换横竖分割：alt+/ 切换横竖全屏：alt+, service mode： reload config：alt+shift+; 相关链接：",
    "tags": [],
    "title": "Aerospace使用教程",
    "uri": "/hugo-blog/blogs/aerospace%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "参考链接：\nLearn X in Y minutes # Lua, the simplest language to learn",
    "description": "参考链接：\nLearn X in Y minutes # Lua, the simplest language to learn",
    "tags": [
      "技术笔记"
    ],
    "title": "Lua学习手册",
    "uri": "/hugo-blog/blogs/lua%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "iTerm2 color themes 下载iterm2的色彩主题，可以选择以下颜色组合，搭配neovim使用\n# Iterm2-color-schemes Oh My Zsh 安装oh my zsh，一键配置zsh的.zshrc\ngithub链接 fonts 下载字体以支持一些开发相关的图标\nnerd fonts 插件 语法高亮： zsh-syntax-highlighting 语义补全： zsh-autosuggestions fzf：\nctrl + f：进行目录下的文件查找（目前扩展为在home目录下的dir） ctrl + t：进行目录下的目录查找（同上） ctrl + r：显示history **：会根据当前命令前缀查找候选 cd **：列出目录，仅限当前目录下，和上面的生效区域不同 export **：列出环境变量 ssh **：列出最近访问过的hostname kill -9 **：列出进程 参考链接：\n# How to setup your Mac Terminal to be beautiful",
    "description": "iTerm2 color themes 下载iterm2的色彩主题，可以选择以下颜色组合，搭配neovim使用\n# Iterm2-color-schemes Oh My Zsh 安装oh my zsh，一键配置zsh的.zshrc\ngithub链接 fonts 下载字体以支持一些开发相关的图标",
    "tags": [],
    "title": "Terminal配置",
    "uri": "/hugo-blog/blogs/terminal%E9%85%8D%E7%BD%AE/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "pip 一般通过freeze的命令记录目前全部环境依赖\npip freeze \u003e requirements.txt 其他人可以通过pip的方式一键安装全部环境以来\npip install -r requirements.txt 缺陷 pip install会将库安装到全局目录，多用户共享，导致版本依赖和兼容性问题 pip freeze的问题在于无法明确区分哪些是项目的直接依赖，只是一股脑的记录 pip uninstall后，对应的间接依赖不会被卸载掉 venv 在项目目录里执行，创建.venv虚拟python环境，名称推荐叫.venv，因为vscode等可以自动识别\npython3 -m venv .venv 通过active激活环境，并通过deactivate关闭环境\n// 激活环境 source .venv/bin/activate // 关闭环境 deactivate pyproject.toml 目标就是统一不同的配置文件，把所有与项目构建（打包、依赖管理）和工具配置相关的设置都放在这一个文件里\n完整示例：\n# 指定构建项目的工具 [build-system] requires = [\"setuptools\u003e=45\", \"wheel\"] build-backend = \"setuptools.build_meta\" [project] name = \"my-cli-tool\" version = \"0.1.0\" description = \"一个强大的命令行工具\" authors = [{name = \"王五\", email = \"wangwu@example.com\"}] readme = \"README.md\" license = {text = \"MIT\"} requires-python = \"\u003e=3.8\" # 运行时依赖 dependencies = [ \"requests\u003e=2.25.1\", \"rich\u003e=10.0.0\", ] classifiers = [ \"License :: OSI Approved :: MIT License\", \"Programming Language :: Python :: 3\", ] # 可选依赖 [project.optional-dependencies] dev = [\"pytest\", \"black\", \"flake8\"] # 命令行运行模块函数 [project.scripts] my-tool = \"my_tool.main:cli\" # 工具配置参数 [tool.black] line-length = 88 [tool.pytest.ini_options] addopts = \"-v\" testpaths = [\"tests\"] 通过以下命令可以一键安装\npip install -e . 缺陷 通过venv和pyproject.toml的方式管理和安装库，导致每次安装新库，需要查找对应的版本号，并手动添加到toml配置文件中 UV 为用户封装了管理项目的库安装和配置过程 安装uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh 通过uv add命令替代pip install\nuv add flask 可以对目录下的pyproject.toml进行自动更新 检查并自动创建.venv虚拟环境 将库和所有依赖均安装到.venv环境中 通过sync命令搭建虚拟环境并安装项目全部依赖\nuv sync 通过run命令可以自动找到venv环境，并省略手动activate环境这步，直接运行代码\n# 直接运行 uv run main.py # 使用特定目录作为虚拟环境 uv --python 3.11 run script.py # 使用系统Python uv --system run script.py # 指定虚拟环境路径 uv --with-venv /path/to/venv run script.py 通过tool install命令可以安装虚拟环境外的工具库，整个系统可用\nuv tool install ruff 通过build命令可以进行项目打包成whl文件\nuv build 参考链接：\n# 从pip到uv：一口气梳理现代Python项目管理全流程！ # 用uv管理Python的一切！",
    "description": "pip 一般通过freeze的命令记录目前全部环境依赖\npip freeze \u003e requirements.txt 其他人可以通过pip的方式一键安装全部环境以来\npip install -r requirements.txt 缺陷 pip install会将库安装到全局目录，多用户共享，导致版本依赖和兼容性问题 pip freeze的问题在于无法明确区分哪些是项目的直接依赖，只是一股脑的记录 pip uninstall后，对应的间接依赖不会被卸载掉 venv 在项目目录里执行，创建.venv虚拟python环境，名称推荐叫.venv，因为vscode等可以自动识别\npython3 -m venv .venv 通过active激活环境，并通过deactivate关闭环境\n// 激活环境 source .venv/bin/activate // 关闭环境 deactivate pyproject.toml 目标就是统一不同的配置文件，把所有与项目构建（打包、依赖管理）和工具配置相关的设置都放在这一个文件里",
    "tags": [
      "技术笔记"
    ],
    "title": "Python项目管理",
    "uri": "/hugo-blog/blogs/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Vim Vim分为两个概念，编辑器和motion\n编辑器指ui，功能提供的模块，在neovim中进行插件化配置优化 motion指vim的基础操作命令，这部分在neovim中通用 操作命令 删除：d 修改：c 替换：r 撤销：u 重做：ctrl+r 移动： 按字符移动：上：k；下：j；左：h；右：l 行数+上下：例如向上8行是8k 按词移动，向前到词首：b；向后到下一个词首：w，向后到词尾：e 到行首：0，到行首非空字符：_ ，到行尾：$ 到指定字符： 到下一个指定字符：f，例如ft到下一个t；F到上一个 到下一个指定字符前：t，例如ta到下一个a前；T到上一个 重复到下一个：; 重复到上一个：, 按paragraph移动：上：{；下：}; 按页移动：ctrl+u：向上半页；ctrl+d：向下半页 进入insert mode： 到下一个字符：a 到最后一个字符后面：A 到第一个字符前面：I 新建下一行：o 新建上一行：O 手动缩进：向左：\u003c；向右：\u003e； 可以同时缩进多行，在首行\u003e4j：包括首行一共缩进5行 搜索： /：搜索下一个；？：默认搜索上一个 *：搜索下一个当前光标对应单词；#：搜索上一个当前光标对应单词 光标居中（中间行）：zz text object：动作（v/y/d）+选中类型（i/a）+object类型（w/W/s/p） 例如：viw（选中当前单词）；viW（选中当前连续字符） 将下一行拼接到当前行，用空格分割： J 重复上一次操作：. 重要概念 Text Object 参考链接：\n# Vim As Your Editor # Vim中的重要概念 Text Object NeoVim neovim内置lua引擎，可以用lua编写插件和配置，易读且功能强大 配置文件位置：~/.config/nvim/init.lua 重要概念 LSP 代码编辑过程中，需要和ls（language server）进行交互，来获取当前代码的状态（定义、引用、诊断等），两者之间的交互基于LSP（language server protocol）\nlspconfig 每次编辑器启动ls的时候，需要给出配置（包括什么时候启动、当前语言等），这时候就需要lspconfig插件： nvim-lspconfig\n对每一种语言安装对应的插件，来支持对应语言的lsp # LSP in Neovim (with like 3 lines of code) nvim在0.11之后可以更方便的安装配置lsp # How to Setup Neovim LSP Like A Pro in 2025 (v0.11+) 插件 可以使用lazy.nvim来帮助安装插件，其中重要的包括：\nneo-tree 提供左侧文件目录，可以浏览目录结构，进行文件的增删改 通过ctrl-w + 方向键可以切换目录和文件窗口 在目录中文件上按t可以在新的tab中打开 bufferline 管理buffer的工具，可以方便在buffer之间切换： 上一个buffer：[b 下一个buffer：]b telescope 方便在目录内查找文件，字符串等 leader + ff（find files） 进行文件查找 leader + fg（live grep） 进行关键词查找 treesitter 基于语言将文本进行结构化理解 实现语法高亮，支持text object、incremental selection等能力 text object支持diw（internal delete word）、vap（visual around paragraph）等操作 incremental selection：ctrl+space nvim-cmp 基于treesitter的理解，实现代码补全 另外autopair实现引号等配对补全，autotag实现html标签配对补全 mason mason是一个管理lsp下载、安装的工具 通过ui完成lsp、包括linter、formatter的安装 mason-lspconfig可以帮助nvim找到nvim-lspconfig的配置，传给lsp 最终lsp实现代码的定义、引用等跳转，并可以进行代码错误识别，给出诊断和code action 定义跳转：gd 引用跳转：gR 显示诊断：leader+d（当前行），leader+D（当前文件） lazygit 在nvim中进行git操作，支持add、commit、push等 减少从nvim出来进入命令行git的操作 参考链接： # Lazygit - The Best Way To Use Git On The Terminal \u0026 Neovim 参考链接：\n# The Only Video You Need to Get Started with Neovim # NeoVim 从平凡到非凡 # How I Setup Neovim To Make It AMAZING in 2024: The Ultimate Guide",
    "description": "Vim Vim分为两个概念，编辑器和motion\n编辑器指ui，功能提供的模块，在neovim中进行插件化配置优化 motion指vim的基础操作命令，这部分在neovim中通用 操作命令 删除：d 修改：c 替换：r 撤销：u 重做：ctrl+r 移动： 按字符移动：上：k；下：j；左：h；右：l 行数+上下：例如向上8行是8k 按词移动，向前到词首：b；向后到下一个词首：w，向后到词尾：e 到行首：0，到行首非空字符：_ ，到行尾：$ 到指定字符： 到下一个指定字符：f，例如ft到下一个t；F到上一个 到下一个指定字符前：t，例如ta到下一个a前；T到上一个 重复到下一个：; 重复到上一个：, 按paragraph移动：上：{；下：}; 按页移动：ctrl+u：向上半页；ctrl+d：向下半页 进入insert mode： 到下一个字符：a 到最后一个字符后面：A 到第一个字符前面：I 新建下一行：o 新建上一行：O 手动缩进：向左：\u003c；向右：\u003e； 可以同时缩进多行，在首行\u003e4j：包括首行一共缩进5行 搜索： /：搜索下一个；？：默认搜索上一个 *：搜索下一个当前光标对应单词；#：搜索上一个当前光标对应单词 光标居中（中间行）：zz text object：动作（v/y/d）+选中类型（i/a）+object类型（w/W/s/p） 例如：viw（选中当前单词）；viW（选中当前连续字符） 将下一行拼接到当前行，用空格分割： J 重复上一次操作：. 重要概念 Text Object",
    "tags": [],
    "title": "Vim和NeoVim",
    "uri": "/hugo-blog/blogs/vim%E5%92%8Cneovim/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "背景 全称terminal multiplexer 一个session可以包含多个子进程window，可以切换显示多个子进程 一个window又可以分割成多个分屏pane，方便编辑和后台运行任务 一个session保存了多个子进程信息，关闭terminal后可以重新恢复session 可以使用tmux运行长时间任务，后台运行防止关闭terminal导致杀死进程 可以在不同session、window和pane之间切换 使用说明 命令行操作 tmux：自动创建session tmux new -s 名字：创建命名session tmux ls：列出tmux全部session tmux a -t 数字：进入指定session，如果只有一个session，可以省略-t参数 tmux kill-session -t 数字或名字：退出指定session tmux has-session -t 数字或名字：是否运行着指定session 内部操作 前缀（ctrl+s）+：\nc：新建window 数字：切换window编号 d：退出当前session，但不杀死（后台运行） n/p：切换到下一个/上一个window $：对当前session重命名 ,：对当前window重命名 \u0026：杀死当前session w：查看当前整体window层级结构 /：显示层级结构后，输入斜杠，进行进一步搜索 %：向右分屏 “：向下分屏 方向键：切换分屏 z：全屏/恢复当前分屏 x：关闭分屏（exit），当window的全部分屏退出后，自动关闭session { } ctrl+up ctrl+down：将当前pane移动到左/右/上/下 配置文件 在home目录新建~/.tmux.conf文件：\nset -g mouse on：开启鼠标支持（调整分屏大小） set -g prefix C-s：tmux默认前缀改为control+s bind | split-window -h -c “#{pane_current_path}\"：水平分屏到cwd bind - split-window -v -c “#{pane_current_path}\"：垂直分屏到cwd 启动脚本示例 #!/bin/bash # tmux 开发环境启动脚本 # 检查是否已经在 tmux 会话中 if [ -n \"$TMUX\" ]; then echo \"Error: Already in a tmux session\" exit 1 fi # 检查会话是否已存在 tmux has-session -t dev 2\u003e/dev/null if [ $? != 0 ]; then # 创建新会话 tmux new-session -d -s dev -n \"editor\" # 第一个窗口：代码编辑器 tmux send-keys -t dev:1 \"cd ~/projects/myapp\" Enter tmux send-keys -t dev:1 \"vim\" Enter # 创建第二个窗口：服务器 tmux new-window -t dev:2 -n \"server\" tmux send-keys -t dev:2 \"cd ~/projects/myapp\" Enter tmux send-keys -t dev:2 \"npm run dev\" Enter # 创建第三个窗口：数据库 tmux new-window -t dev:3 -n \"database\" tmux send-keys -t dev:3 \"docker ps\" Enter tmux send-keys -t dev:3 \"docker exec -it postgres psql -U user mydb\" Enter # 创建第四个窗口：日志 tmux new-window -t dev:4 -n \"logs\" tmux send-keys -t dev:4 \"cd ~/projects/myapp/logs\" Enter tmux send-keys -t dev:4 \"tail -f app.log\" Enter # 创建第五个窗口：系统监控 tmux new-window -t dev:5 -n \"monitor\" tmux send-keys -t dev:5 \"htop\" Enter # 水平分割第二个窗口（服务器窗口） tmux split-window -h -t dev:2 tmux send-keys -t dev:2.1 \"cd ~/projects/myapp\" Enter tmux send-keys -t dev:2.1 \"git status\" Enter # 设置初始窗口 tmux select-window -t dev:1 fi # 附加到会话 tmux attach -t dev 参考链接：\n# tmux 使用和基礎配置 從入門到加班 一個視頻全搞定！ # Tmux + Vim 工作流! 同时操作多个项目, 追求极致的. 滑流畅! # Tmux has forever changed the wa. I write code. # you need to learn tmux RIGHT NO. !!",
    "description": "背景 全称terminal multiplexer 一个session可以包含多个子进程window，可以切换显示多个子进程 一个window又可以分割成多个分屏pane，方便编辑和后台运行任务 一个session保存了多个子进程信息，关闭terminal后可以重新恢复session 可以使用tmux运行长时间任务，后台运行防止关闭terminal导致杀死进程 可以在不同session、window和pane之间切换 使用说明 命令行操作 tmux：自动创建session tmux new -s 名字：创建命名session tmux ls：列出tmux全部session tmux a -t 数字：进入指定session，如果只有一个session，可以省略-t参数 tmux kill-session -t 数字或名字：退出指定session tmux has-session -t 数字或名字：是否运行着指定session 内部操作 前缀（ctrl+s）+：",
    "tags": [],
    "title": "Tmux使用教程",
    "uri": "/hugo-blog/blogs/tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "Zapier官网 自动化流程配置工具，将你的多种不同的app的信息串联起来 gmail邮箱收到邮件，在whatsapp上设置bot提醒 stripe收到货款，导入google sheet新建一行 等等 支持AI辅助新建工作流 参考链接：\n# Zapier AI Tutorial for Beginners: Automation Made Simple 🟧",
    "description": "Zapier官网 自动化流程配置工具，将你的多种不同的app的信息串联起来 gmail邮箱收到邮件，在whatsapp上设置bot提醒 stripe收到货款，导入google sheet新建一行 等等 支持AI辅助新建工作流 参考链接：\n# Zapier AI Tutorial for Beginners: Automation Made Simple 🟧",
    "tags": [],
    "title": "自动化流程平台Zapier",
    "uri": "/hugo-blog/blogs/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B%E5%B9%B3%E5%8F%B0zapier/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "这里汇总了不同的内容（主要是文档，如obsidian和notion中）整理方式\nACE Atlas：与时间无关的想法，笔记 Calendar：按时间例行化的内容，如日记、周记 Efforts：项目进度 参考链接：\n# Create Your Digital Home: Obsidian Walkthrough PARA Projects：项目进展，有时间节点限制，有任务要求 Areas：某一方面的计划和记录，不像projects那样有任物属性（时间节点或ddl），比如健康，某个领域的学习进展等 Resources：可以理解成与时间无关的ideas，例如blog、来自某本书的quote等 Archives：过时的内容，暂存 参考链接：\n# Organize Your ENTIRE Digital Life in Seconds (The PARA Method)",
    "description": "这里汇总了不同的内容（主要是文档，如obsidian和notion中）整理方式\nACE Atlas：与时间无关的想法，笔记 Calendar：按时间例行化的内容，如日记、周记 Efforts：项目进度 参考链接：\n# Create Your Digital Home: Obsidian Walkthrough PARA Projects：项目进展，有时间节点限制，有任务要求 Areas：某一方面的计划和记录，不像projects那样有任物属性（时间节点或ddl），比如健康，某个领域的学习进展等 Resources：可以理解成与时间无关的ideas，例如blog、来自某本书的quote等 Archives：过时的内容，暂存 参考链接：\n# Organize Your ENTIRE Digital Life in Seconds (The PARA Method)",
    "tags": [],
    "title": "内容整理方式",
    "uri": "/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E6%95%B4%E7%90%86%E6%96%B9%E5%BC%8F/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "内容管理系统（CMS）方便记录各种信息，包括文章、想法、日程、计划等\nObsidian 优点 可以对笔记及内部元素进行双向连接，从而形成关系图谱 支持插件，包括built-in和自定义，例如图片上传等 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持高级markdown格式，如callout等 数据本地化，方便离线操作 缺点 相比notion，缺少日程表类似的功能 不支持AI功能 参考链接：\n# 39分鐘上手Obsidian！基礎操作介紹（電腦、平板、手機全面教學） # Give Me 15 Minutes. I’ll Teach You 80% of Obsidian Notion 优点 页面UI丰富，支持各种拖拽、增减等操作 方便地进行任务规划，例如对database进行table或calendar格式的view 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持丰富的页面可视化，例如icon等 支持团队协作编辑文档，可以给外部分享并评论 目前notion 3.0提供 Notion Agent的AI功能 缺点 对于纯粹的内容管理系统而言，功能冗余 对于每日行程规律的人友好，但反之就会有很多冗余操作 页面之间的关系是上下级，缺乏obsidian点对点的联系 可以通过@方式实现页面的跳转，但是不是概念（单词）粒度的联系 参考链接：\n# 全世界在学的软件，到底怎么用？Notion十分钟入门指南。 # How to Get Started with Notion (without losing your mind) Notion Agent 也称为notion 3.0，1.0为document，2.0为database，3.0加入AI agent 通过添加不同的agent为你工作，他们的工作包括：\n创建、查询和编辑database 创作和更新内容 在Notion、外部工具（如gmail、slack等）和网络中查询分析信息 参考链接：\n# Getting started with Notion Agent # Notion’s AI Agent is a Game-Changer (Notion made EASY!)",
    "description": "内容管理系统（CMS）方便记录各种信息，包括文章、想法、日程、计划等\nObsidian 优点 可以对笔记及内部元素进行双向连接，从而形成关系图谱 支持插件，包括built-in和自定义，例如图片上传等 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持高级markdown格式，如callout等 数据本地化，方便离线操作 缺点 相比notion，缺少日程表类似的功能 不支持AI功能 参考链接：\n# 39分鐘上手Obsidian！基礎操作介紹（電腦、平板、手機全面教學） # Give Me 15 Minutes. I’ll Teach You 80% of Obsidian Notion 优点 页面UI丰富，支持各种拖拽、增减等操作 方便地进行任务规划，例如对database进行table或calendar格式的view 支持自定义模版，方便新建笔记时快速初始化properties和格式 支持丰富的页面可视化，例如icon等 支持团队协作编辑文档，可以给外部分享并评论 目前notion 3.0提供 Notion Agent的AI功能 缺点 对于纯粹的内容管理系统而言，功能冗余 对于每日行程规律的人友好，但反之就会有很多冗余操作 页面之间的关系是上下级，缺乏obsidian点对点的联系 可以通过@方式实现页面的跳转，但是不是概念（单词）粒度的联系 参考链接：",
    "tags": [],
    "title": "内容管理系统",
    "uri": "/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 学习notion基本功能，并基于notion搭建个人博客 学习css、html、javascript基础知识，搭建简单的github.io主页 基于notion + notion next + vercel搭建个人博客 调研notion建站方案 原生publish：直接点击页面的publish，notion原生样式 开源notion next： 通过next js实现notion内容拉取和前端渲染，可以定制化页面样式 参考链接 notion next fork仓库后可以修改配置 复制notion next作者的模版页面到自己的notion主页中，编辑内容进行博客编写 官网 vercel负责托管部署 关联github账号，import上述仓库，填写NOTION_PAGE_ID，部署 配置域名等与github pages类似 博客链接 学习css、html基础知识并简单搭建github.io css \u0026 html课程学习 HTML学习手册-基础概念 CSS学习手册-基础用法 课程链接 # 成為網頁設計師的第一步！快速上手 HTML \u0026 CSS 展開你的網頁設計之旅！ # HTML \u0026 CSS Full Course for free # HTML \u0026 CSS Full Course - Beginner to Pro 搭建简单静态网页 github.io 学习javascript基础知识 javascript课程学习 课程链接 # JavaScript 快速上手！用一個實戰範例迅速掌握所有重點語法！ 知识 作为CMS，notion更偏向于项目管理，而obsidian更偏向于文档记录 在html的body中加入script可以插入js代码，页面f12可以看到console.log的打印 通过在元素上添加listener来捕捉用户行为（点击、输入等），实现相应逻辑（如添加表单元素等） 待办 javascript基础知识 javascript进阶：ES6+新特性",
    "description": "总结 学习notion基本功能，并基于notion搭建个人博客 学习css、html、javascript基础知识，搭建简单的github.io主页 基于notion + notion next + vercel搭建个人博客 调研notion建站方案 原生publish：直接点击页面的publish，notion原生样式 开源notion next： 通过next js实现notion内容拉取和前端渲染，可以定制化页面样式 参考链接 notion next fork仓库后可以修改配置 复制notion next作者的模版页面到自己的notion主页中，编辑内容进行博客编写 官网 vercel负责托管部署 关联github账号，import上述仓库，填写NOTION_PAGE_ID，部署 配置域名等与github pages类似 博客链接 学习css、html基础知识并简单搭建github.io css \u0026 html课程学习 HTML学习手册-基础概念 CSS学习手册-基础用法 课程链接 # 成為網頁設計師的第一步！快速上手 HTML \u0026 CSS 展開你的網頁設計之旅！ # HTML \u0026 CSS Full Course for free # HTML \u0026 CSS Full Course - Beginner to Pro 搭建简单静态网页 github.io 学习javascript基础知识 javascript课程学习 课程链接 # JavaScript 快速上手！用一個實戰範例迅速掌握所有重點語法！ 知识 作为CMS，notion更偏向于项目管理，而obsidian更偏向于文档记录 在html的body中加入script可以插入js代码，页面f12可以看到console.log的打印 通过在元素上添加listener来捕捉用户行为（点击、输入等），实现相应逻辑（如添加表单元素等） 待办 javascript基础知识 javascript进阶：ES6+新特性",
    "tags": [
      "周记"
    ],
    "title": "Week3 个人博客搭建",
    "uri": "/hugo-blog/weekly/week3/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Blogs",
    "content": "OSI模型是计算机通信网络的抽象模型",
    "description": "OSI模型是计算机通信网络的抽象模型",
    "tags": [
      "技术笔记"
    ],
    "title": "OSI模型",
    "uri": "/hugo-blog/blogs/osi%E6%A8%A1%E5%9E%8B/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log \u003e First Day",
    "content": "first day things",
    "description": "first day things",
    "tags": [],
    "title": "First Day Things",
    "uri": "/hugo-blog/log/first-day/first-day-things/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log",
    "content": "hello from third day",
    "description": "hello from third day",
    "tags": [],
    "title": "Third Day",
    "uri": "/hugo-blog/log/third-day/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log",
    "content": "hello from second day",
    "description": "hello from second day",
    "tags": [],
    "title": "Second Day",
    "uri": "/hugo-blog/log/second-day/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Log",
    "content": "hello from first day",
    "description": "hello from first day",
    "tags": [],
    "title": "First Day",
    "uri": "/hugo-blog/log/first-day/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 域名注册 \u0026 图床搭建 尝试利用现有平台进行个人博客的搭建部署，不涉及具体框架或代码开发 学习obsidian基本功能 cloudflare域名注册 ruoruoliu.com，每年10刀左右 cloudflare账户 基于clodflare R2 + piclist搭建免费图床 由于picgo的S3插件上传有问题，替换为piclist 参考链接 基于hexo + github pages搭建个人博客 hexo主要是基于模版的页面生成和部署工具，可以命令行生成并部署到github pages中 每次修改source中的md文件，hexo在generate过程中更新public中对应的部分 github.io作为个人主页，对于仓库名有要求：ruoruoliu.github.io 页面搭建在github pages中的项目页面中： 博客链接 是否需要nginx解决网址栏输入最后没有/导致访问不同的问题？ 参考链接 基于obsidian + hugo + github pages搭建个人博客 hugo将md文件转化为html页面，并进行部署 调研hugo的theme，包括paper、paperMod、terminal和relearn 后续可以在博客更新过程中不断学习relearn基本功能： 主题官网 Obsidian编写md文件，hugo是和hexo类似的cms工具 处理obsidian的内部链接，转化为hugo可接受链接\n使用obsidian插件“image auto upload”，插入图片时自动上传图床\n通过obsidian插件“shell commands”一键同步文件到本地github目录： 参考链接， 插件连接，再push到github远程仓库，触发github pages更新部署\n整体仓库push到main 网页代码通过”git subtree“ push到gh-pages分支 加入obsidian内部链接转化为markdown格式的处理 页面搭建在github pages中的项目页面中： 博客链接\n知识 个人博客平台化搭建范式 内容编辑：通常是md格式，可以基于obsidian、notion等 内容到页面转化工具：hexo、hugo、nextjs等 服务托管平台：github pages、cloudflare pages、vercel等 待办 学习notion基本功能，并基于notion搭建个人博客 学习css、html、javascript基础知识",
    "description": "总结 域名注册 \u0026 图床搭建 尝试利用现有平台进行个人博客的搭建部署，不涉及具体框架或代码开发 学习obsidian基本功能 cloudflare域名注册 ruoruoliu.com，每年10刀左右 cloudflare账户 基于clodflare R2 + piclist搭建免费图床 由于picgo的S3插件上传有问题，替换为piclist 参考链接 基于hexo + github pages搭建个人博客 hexo主要是基于模版的页面生成和部署工具，可以命令行生成并部署到github pages中 每次修改source中的md文件，hexo在generate过程中更新public中对应的部分 github.io作为个人主页，对于仓库名有要求：ruoruoliu.github.io 页面搭建在github pages中的项目页面中： 博客链接 是否需要nginx解决网址栏输入最后没有/导致访问不同的问题？ 参考链接 基于obsidian + hugo + github pages搭建个人博客 hugo将md文件转化为html页面，并进行部署 调研hugo的theme，包括paper、paperMod、terminal和relearn 后续可以在博客更新过程中不断学习relearn基本功能： 主题官网 Obsidian编写md文件，hugo是和hexo类似的cms工具 处理obsidian的内部链接，转化为hugo可接受链接\n使用obsidian插件“image auto upload”，插入图片时自动上传图床\n通过obsidian插件“shell commands”一键同步文件到本地github目录： 参考链接， 插件连接，再push到github远程仓库，触发github pages更新部署",
    "tags": [
      "周记"
    ],
    "title": "Week2 个人博客搭建",
    "uri": "/hugo-blog/weekly/week2/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0 \u003e Weeklies",
    "content": "总结 完成科学上网环境搭建 完成远程开发环境搭建 科学上网配置 购买vps节点： 尝试了vultr和bandwagon两个服务商 vultr按小时收费，最低每月5刀，搬瓦工按quarter收费，三个月50刀 节点选择： 考虑是否支持运营商vip骨干网络，比如移动为cmi 搬瓦工可以使用日本节点，延迟控制在100ms以内，vscode响应时间可接受 参考链接 vscode remote配置 github copilot配置 免费试用 pro 1个月，后续切换到 free plan 后续可以尝试cursor以及claude code 知识 科学上网流程 购买vps节点 vps节点安装并配置代理客户端（s-ui）的tls 本地v2ray配置vps的tls 计算机网络 OSI模型 计算机网络通讯的【系统性】扫盲——从“基本概念”到“OSI 模型” 待办 搭建个人博客，方便后续记笔记",
    "description": "总结 完成科学上网环境搭建 完成远程开发环境搭建 科学上网配置 购买vps节点： 尝试了vultr和bandwagon两个服务商 vultr按小时收费，最低每月5刀，搬瓦工按quarter收费，三个月50刀 节点选择： 考虑是否支持运营商vip骨干网络，比如移动为cmi 搬瓦工可以使用日本节点，延迟控制在100ms以内，vscode响应时间可接受 参考链接 vscode remote配置 github copilot配置 免费试用 pro 1个月，后续切换到 free plan 后续可以尝试cursor以及claude code 知识 科学上网流程 购买vps节点 vps节点安装并配置代理客户端（s-ui）的tls 本地v2ray配置vps的tls 计算机网络 OSI模型 计算机网络通讯的【系统性】扫盲——从“基本概念”到“OSI 模型” 待办 搭建个人博客，方便后续记笔记",
    "tags": [
      "周记"
    ],
    "title": "Week1 开发环境配置",
    "uri": "/hugo-blog/weekly/week1/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/hugo-blog/categories/index.html"
  },
  {
    "breadcrumb": "Ruoruoliu 2.0",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Projects",
    "uri": "/hugo-blog/projects/index.html"
  }
]
