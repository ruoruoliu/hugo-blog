<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.152.2">
    <meta name="generator" content="Relearn 8.2.0+740f9bc7eaab0594601604e7f7c29ba660a33b9a">
    <meta name="description" content="总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM训练技术学习手册 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\mu$ 和 $\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好： GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题 DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题 RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习 PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率 待办 跟进大模型强化学习技术 强化学习技术实战">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Week12 强化学习技术跟进 :: Ruoruoliu 2.0">
    <meta name="twitter:description" content="总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM训练技术学习手册 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\mu$ 和 $\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好： GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题 DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题 RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习 PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率 待办 跟进大模型强化学习技术 强化学习技术实战">
    <meta property="og:url" content="https://ruoruoliu.github.io/hugo-blog/weekly/week12/index.html">
    <meta property="og:site_name" content="Ruoruoliu 2.0">
    <meta property="og:title" content="Week12 强化学习技术跟进 :: Ruoruoliu 2.0">
    <meta property="og:description" content="总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM训练技术学习手册 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\mu$ 和 $\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好： GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题 DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题 RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习 PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率 待办 跟进大模型强化学习技术 强化学习技术实战">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="Weeklies">
    <meta property="article:published_time" content="2026-01-19T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-19T00:00:00+00:00">
    <meta property="article:tag" content="周记">
    <meta itemprop="name" content="Week12 强化学习技术跟进 :: Ruoruoliu 2.0">
    <meta itemprop="description" content="总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM训练技术学习手册 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\mu$ 和 $\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好： GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题 DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题 RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习 PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率 待办 跟进大模型强化学习技术 强化学习技术实战">
    <meta itemprop="datePublished" content="2026-01-19T00:00:00+00:00">
    <meta itemprop="dateModified" content="2026-01-19T00:00:00+00:00">
    <meta itemprop="wordCount" content="43">
    <meta itemprop="keywords" content="周记">
    <title>Week12 强化学习技术跟进 :: Ruoruoliu 2.0</title>
    <link href="/hugo-blog/images/favicon.png?1770866785" rel="icon" type="image/png">
    <link href="/hugo-blog/css/auto-complete/auto-complete.min.css?1770866785" rel="stylesheet">
    <script src="/hugo-blog/js/auto-complete/auto-complete.min.js?1770866785" defer></script>
    <script src="/hugo-blog/js/search-lunr.min.js?1770866785" defer></script>
    <script src="/hugo-blog/js/search.min.js?1770866785" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/hugo-blog/searchindex.en.js?1770866785";
    </script>
    <script src="/hugo-blog/js/lunr/lunr.min.js?1770866785" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.stemmer.support.min.js?1770866785" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.multi.min.js?1770866785" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.en.min.js?1770866785" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/hugo-blog/fonts/fontawesome/css/fontawesome-all.min.css?1770866785" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/hugo-blog/fonts/fontawesome/css/fontawesome-all.min.css?1770866785" rel="stylesheet"></noscript>
    <link href="/hugo-blog/css/perfect-scrollbar/perfect-scrollbar.min.css?1770866785" rel="stylesheet">
    <link href="/hugo-blog/css/theme.min.css?1770866785" rel="stylesheet">
    <link href="/hugo-blog/css/format-html.min.css?1770866785" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = `.min`;
      window.relearn.path='\/weekly\/week12\/index.html';
      window.relearn.relBasePath='..\/..';
      window.relearn.relBaseUri='..\/..\/..';
      window.relearn.absBaseUri='https:\/\/ruoruoliu.github.io\/hugo-blog';
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy text to clipboard`;
      window.T_Copied_to_clipboard = `Text copied to clipboard!`;
      window.T_Link_copied_to_clipboard = `Link copied to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      window.T_Browser_unsupported_feature = `This browser doesn't support this feature`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
  </head>
  <body class="mobile-support html" data-url="/hugo-blog/weekly/week12/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button></span>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button></span>
              <div class="topbar-content">
                <div class="topbar-content-wrapper"> 
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/hugo-blog/index.html"><span itemprop="name">Ruoruoliu 2.0</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/hugo-blog/weekly/index.html"><span itemprop="name">Weeklies</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">Week12 强化学习技术跟进</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><a href="/hugo-blog/weekly/week13/index.html" title="Week13 大模型强化学习技术跟进 (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a></span>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><a href="/hugo-blog/weekly/week11/index.html" title="Week11 强化学习基础知识 (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a></span>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button></span>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable weekly" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
<div class="R-taxonomy taxonomy-tags cstyle tags" title="Tags" style="--VARIABLE-TAGS-BG-color: var(--INTERNAL-TAG-BG-color);">
  <i class="fa-fw fas fa-layer-group"></i>
  <ul>
    <li><a class="term-link" href="/hugo-blog/tags/%E5%91%A8%E8%AE%B0/index.html">周记</a></li>
  </ul>
</div>
  </header>

<h1 id="week12-强化学习技术跟进">Week12 强化学习技术跟进</h1>


<details open class="box cstyle notices note">
  <summary class="box-label" tabindex="-1">
    <i class="fa-fw fas fa-exclamation-circle"></i> 
    总结
  </summary>
  <div class="box-content">
<ul>
<li>强化学习基础知识</li>
<li>跟进大模型强化学习技术</li>
</ul>
  </div>
</details>

<details open class="box cstyle notices info">
  <summary class="box-label" tabindex="-1">
    <i class="fa-fw fas fa-info-circle"></i> 
    强化学习基础知识
  </summary>
</details>
<ul>
<li>



    
<a href="/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/">Reinforcement Learning学习手册</a></li>
</ul>

<details open class="box cstyle notices info">
  <summary class="box-label" tabindex="-1">
    <i class="fa-fw fas fa-info-circle"></i> 
    跟进大模型强化学习技术
  </summary>
</details>
<ul>
<li>



    
<a href="/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/">LLM训练技术学习手册</a></li>
</ul>

<details open class="box cstyle notices tip">
  <summary class="box-label" tabindex="-1">
    <i class="fa-fw fas fa-lightbulb"></i> 
    知识
  </summary>
  <div class="box-content">
<ul>
<li>value-based方法在DQN的基础上：
<ul>
<li>target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题</li>
<li>double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题</li>
<li>dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性</li>
</ul>
</li>
<li>经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正
<ul>
<li>优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑）</li>
</ul>
</li>
<li>policy-based方法在Actor-Critic的基础上：
<ul>
<li>TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差</li>
<li>PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速</li>
<li>对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络</li>
<li>同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\mu$ 和 $\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度</li>
</ul>
</li>
<li>LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好：
<ul>
<li>GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题</li>
<li>DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题</li>
<li>RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习</li>
<li>PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率</li>
</ul>
</li>
</ul>
  </div>
</details>

<details open class="box cstyle notices warning">
  <summary class="box-label" tabindex="-1">
    <i class="fa-fw fas fa-exclamation-triangle"></i> 
    待办
  </summary>
  <div class="box-content">
<ul>
<li>跟进大模型强化学习技术</li>
<li>强化学习技术实战</li>
</ul>
  </div>
</details>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Jan 19, 2026
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/hugo-blog/index.html">
            <div class="logo-title">Ruoruoliu 2.0</div>
          </a>
        </div>
        <search><form action="/hugo-blog/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/index.html"><a class="padding" href="/hugo-blog/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/log/index.html"><a class="padding" href="/hugo-blog/log/index.html">Log</a><ul id="R-subsections-23c22f75c84b98545748a65894702014" class="collapsible-menu"></ul></li>
            <li class="parent " data-nav-url="/hugo-blog/weekly/index.html"><a class="padding" href="/hugo-blog/weekly/index.html">Weeklies</a><ul id="R-subsections-fa6cd6a93db3107c38820d614c82d18d" class="collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/weekly/week15/index.html"><a class="padding" href="/hugo-blog/weekly/week15/index.html">Week15 LangGraph框架学习</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week14/index.html"><a class="padding" href="/hugo-blog/weekly/week14/index.html">Week14 LangChain框架学习</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week13/index.html"><a class="padding" href="/hugo-blog/weekly/week13/index.html">Week13 大模型强化学习技术跟进</a></li>
            <li class="active " data-nav-url="/hugo-blog/weekly/week12/index.html"><a class="padding" href="/hugo-blog/weekly/week12/index.html">Week12 强化学习技术跟进</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week11/index.html"><a class="padding" href="/hugo-blog/weekly/week11/index.html">Week11 强化学习基础知识</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week10/index.html"><a class="padding" href="/hugo-blog/weekly/week10/index.html">Week10 强化学习基础知识</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week9/index.html"><a class="padding" href="/hugo-blog/weekly/week9/index.html">Week9 大模型进展</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week8/index.html"><a class="padding" href="/hugo-blog/weekly/week8/index.html">Week8 大模型进展</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week7/index.html"><a class="padding" href="/hugo-blog/weekly/week7/index.html">Week7 React学习</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week6/index.html"><a class="padding" href="/hugo-blog/weekly/week6/index.html">Week6 Flappy Bird</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week5/index.html"><a class="padding" href="/hugo-blog/weekly/week5/index.html">Week5 Javascript学习</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week4/index.html"><a class="padding" href="/hugo-blog/weekly/week4/index.html">Week4 开发环境优化</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week3/index.html"><a class="padding" href="/hugo-blog/weekly/week3/index.html">Week3 个人博客搭建</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week2/index.html"><a class="padding" href="/hugo-blog/weekly/week2/index.html">Week2 个人博客搭建</a></li>
            <li class="" data-nav-url="/hugo-blog/weekly/week1/index.html"><a class="padding" href="/hugo-blog/weekly/week1/index.html">Week1 开发环境配置</a></li></ul></li>
            <li class="" data-nav-url="/hugo-blog/blogs/index.html"><a class="padding" href="/hugo-blog/blogs/index.html">Blogs</a><ul id="R-subsections-0c82701243222eae0a7f46ed20de4880" class="collapsible-menu"></ul></li>
            <li class="" data-nav-url="/hugo-blog/projects/index.html"><a class="padding" href="/hugo-blog/projects/index.html">Projects</a></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script>
      window.MathJax = Object.assign( window.MathJax || {}, {
        tex: {
          inlineMath:  [['\\(', '\\)'], ['$',  '$']],  
          displayMath: [['\\[', '\\]'], ['$$', '$$']], 
        },
        options: {
          enableMenu: false 
        }
      }, JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/hugo-blog/js/mathjax/tex-mml-chtml.js?1770866785"></script>
    <script src="/hugo-blog/js/perfect-scrollbar/perfect-scrollbar.min.js?1770866785" defer></script>
    <script src="/hugo-blog/js/theme.min.js?1770866785" defer></script>
    <div id="toast-container" role="status" aria-live="polite" aria-atomic="false"></div>
  </body>
</html>
