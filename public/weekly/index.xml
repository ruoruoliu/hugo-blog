<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Weeklies :: Ruoruoliu 2.0</title>
    <link>https://ruoruoliu.github.io/hugo-blog/weekly/index.html</link>
    <description></description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://ruoruoliu.github.io/hugo-blog/weekly/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Week15 Agent框架学习：LangGraph</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week15/index.html</link>
      <pubDate>Tue, 10 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week15/index.html</guid>
      <description>总结 Agent框架学习：LangGraph Agent框架学习：LangGraph LangGraph学习手册 知识 LangGraph [!warning] 待办</description>
    </item>
    <item>
      <title>Week14 Agent框架学习：LangChain</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week14/index.html</link>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week14/index.html</guid>
      <description>总结 Agent框架学习：LangChain Agent框架学习：LangChain LangChain学习手册 知识 typedDict通过约定的方式说明dict中的字段有哪些，如果不符合则会在运行前警告；而pydantic是dataclass的第三方强化版本，定义一个数据类，并保证类中的字段，如果不符合则会在运行时报错，pydantic可以进行自动转化，并支持字段级别的初始化处理逻辑 asyncio支持python中的异步逻辑，使用单线程避免任务等待 将函数定义为async，使用await来挂起等待结果返回 如果多个任务同时触发，可以使用asyncio.gather 使用asyncio.run来启动事件循环（EventLoop），触发全部协程 LangChain是帮助用户快速搭建agent的平台，用户通过设计system prompt、tool、中间件等，实现agentic逻辑，在交互过程中更新agent的state，控制模型上下文，得到结果 Model：LangChain支持各大厂商和开源model Tools：LangChain支持用@tool将函数封装为工具使用 中间件：中间件方便用户在模型调用前后，工具调用前后进行拦截，并实现特定逻辑，包括模型、工具、system prompt的修改等 一些内置的中间件，包括Human-in-the-loop中间件等 Memory：LangChain的短期记忆就是state，长期记忆就是store state就是对话序列，包含AIMessage、ToolMessage、HumanMessage等，以及用户定义的一些字段，通过state_schema传入 store就是外部存储，默认内存，可以选择数据库 response_format：通过ToolStrategy或者厂商原生能力，支持结构化输出 待办 Agent框架学习：LangGraph</description>
    </item>
    <item>
      <title>Week13 大模型强化学习技术跟进</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week13/index.html</link>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week13/index.html</guid>
      <description>总结 跟进大模型强化学习技术 跟进大模型强化学习技术 LLM训练技术学习手册 知识 DAPO 通过对gradient的clip的上下限解耦，防止模型探索不够的问题 通过只采样reward不都等于0或1的batch，来提升训练效率，防止噪声引入的波动 通过缓冲区软长度惩罚，配合token-level的loss，提升模型对过长序列和错误的区分 GSPO 将策略梯度转化为序列级，使之与reward粒度对齐 通过几何平均的形式避免数值溢出，并对长度归一化 解决MoE模型需要Routing Replay的复杂度问题 待办 Agent框架学习</description>
    </item>
    <item>
      <title>Week12 强化学习技术跟进</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week12/index.html</link>
      <pubDate>Mon, 19 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week12/index.html</guid>
      <description>总结 强化学习基础知识 跟进大模型强化学习技术 强化学习基础知识 Reinforcement Learning学习手册 跟进大模型强化学习技术 LLM训练技术学习手册 知识 value-based方法在DQN的基础上： target network：target network同结构不同参数，用来选择和预估下一时刻的动作的价值，短时间内固定参数来缓解预估目标移动导致的训练稳定性问题 double DQN：通过online network负责选择下一时刻action，target network负责预估下一时刻动作的价值，来彻底解决Q-learning中价值预估高估问题 dueling network：拆分价值预估中状态和动作的价值，提高训练的效率和鲁棒性 经验回放避免了训练数据的浪费，不过使学习变为了off-policy（在线策略学习旧策略的数据分布），需要重要性采样修正 优先经验回放替代了原始的随机采样经验，而采用对新经验高优先级、对TD error较大的高优先级的采样方式，提高难样本的重要性（小步多跑） policy-based方法在Actor-Critic的基础上： TRPO基于置信域，替代了原始的对参数设定学习率（步长）的做法，改为保证策略变化不大（KL散度）的情况下，求解最优收益的带约束最优化问题，使训练收敛稳定，减小方差 PPO解决了TRPO求解带约束最优化问题计算量大的问题，用clip的方式代替KL散度约束，使得工程上训练加速 对于连续控制问题，DDPG将actor网络的输出直接设定为action（数值而不是概率），然后通过梯度链式法则直接求解最优action，同时优化critic网络 同样对于连续控制问题，SAC使用随机策略的方法，actor网络输出action分布的 $\mu$ 和 $\sigma$，通过对这个分布的采样得到action，通过重参数化的技巧仍然支持梯度链式法则，可以让actor直接对收益求梯度 LLM的后训练中，RLHF通过强化学习帮助模型的回复接近人类偏好： GRPO在PPO的基础上去掉Critic模型，用组内平均计算优势值，并调整分布，适合逻辑推理、代码或者有明确reward衡量方式的问题 DPO直接舍弃RL框架，只比较两个结果的相对好坏，并调整分布，适合没有明确reward衡量的人类偏好对齐的问题 RLVR是指利用verifiable reward（比如数学题答案、代码运行结果等可准确衡量的reward）来替代reward model，避免模型出现reward hacking，适合推理任务的学习 PRM首先利用人工标注每一个推理步的正确与否，学习process reward model模型，并将这个reward的预测加入到critic模型的学习中，帮助actor模型加强正确中间步骤的生成概率 待办 跟进大模型强化学习技术 强化学习技术实战</description>
    </item>
    <item>
      <title>Week11 强化学习基础知识</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week11/index.html</link>
      <pubDate>Mon, 12 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week11/index.html</guid>
      <description>总结 强化学习基础知识 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Prediction DeepMind x UCL Introduction to RL 2015 课程笔记-Model-Free Control DeepMind x UCL Introduction to RL 2015 课程笔记-Value Function Approximation DeepMind x UCL Introduction to RL 2015 课程笔记-Policy Gradient DeepMind x UCL Introduction to RL 2015 课程笔记-Integrating Learning and Planning DeepMind x UCL Introduction to RL 2015 课程笔记-Exploration and Exploitation 知识 当我们不知道环境的具体信息（model）时，如何进行策略的评估（prediction）和最优策略的选择（control），称为model-free RL model-free主要基于采样，即在环境中通过采样拿到真实reward，来迭代value function，从而评价state/action的好坏 采样完整序列的reward的方式称为Monte-Carlo采样，采样下一步然后基于下一步的value function进行bootstrapping的方式称为TD learning；进一步地，通过TD($\lambda$)可以在MC和TD之间平滑，效果一般更优 选择最优策略的方式一般是基于评估后通过greedy或者$\epsilon-greedy$的方式选取action；如果行为策略和目标策略相同，称为on-policy，如果不同，称为off-policy TD learning + $\epsilon-greedy$ = SARSA（on-policy） TD learning + max = Q-learning（off-policy） 简单问题可以通过查表完成，但是真实环境的RL问题状态空间通常很大，需要使用Value Function Approximator来表示，一般可以理解为神经网络的近似，比如DQN policy gradient的方法直接计算最优策略，避免了value-based的max操作，具有更好的收敛性质 可以通过采样reward结合策略梯度的方式，计算最优策略对应的action的分布 actor-critic通过同时优化evaluation（critic）和improvement（actor）减小variance 另外我们可以先通过采样求解model，把问题转化为model-based RL，再求解策略 同时结合model-based以及model-free来充分的压榨样本数据，比如Dyna-Q 基于价值函数进行动作采样，基于model生成sub MDP，再通过model-free求解，比如MCTS 利用和探索是希望我们在”最大化即时收益“和”获取新信息“之间做出权衡，人们通过多臂赌博机问题发展出很多策略：包括UCB和Thompson Sampling，且这些策略均可以在MDP上应用 待办 强化学习基础知识 跟进大模型强化学习技术</description>
    </item>
    <item>
      <title>Week10 强化学习基础知识</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week10/index.html</link>
      <pubDate>Mon, 05 Jan 2026 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week10/index.html</guid>
      <description>总结 强化学习基础知识 搭建商品收货记录系统 搭建商品收货记录系统 学习bootstrap基础知识 前端开源css框架，主要用于快速开发响应式的网站，拥有Grid System和丰富预设组件 预设div的一些class：方便基于grid布局的响应式变化，包括grid的数量、顺序以及offset 提供预设的组件：buttons、下拉栏、以及modal（支持js）等 参考链接 整体思路采用flask+sqlite+bootstrap的方式: 从零开始构建商品收货系统 强化学习基础知识 DeepMind x UCL Introduction to RL 2015 课程笔记-基本概念 DeepMind x UCL Introduction to RL 2015 课程笔记-MDP DeepMind x UCL Introduction to RL 2015 课程笔记-Planning by Dynamic Programming 知识 强化学习主要解决agent在环境中选择policy从而达到最大化序列reward的问题 agent与环境的交互（state和action）可以表示为Markov Process 如果加入reward，就变成MRP（Markov Reward Process） 如果再加入decision（$\pi$），就变成MDP（Markov Decision Process） value function，即从当前state起到最后的序列reward的带衰减的累积，用来衡量当前状态的好坏： state value function：只基于当前state，称为$v(s)$ action value function：基于当前state和选择的action，称为$q(s,a)$ 只考虑MRP的情况，value function是线性可解的，只是复杂度为$O(n^3)$ 通过基于policy的加权求和，可以在v和q之间转化，也可以经过两次转化（v到q到v）得到状态之间的递推关系，称为Bellman Expectation Equation 对于固定policy，可以简化为MRP，value function是线性可解的； MDP求最优policy，即Bellman Optimality Equation，递推关系中由于引入了max操作，是线性不可解的，只能采用迭代法等： policy iteration：通过迭代value function（减小计算复杂度）来评估policy，然后优化policy（选择最大value的state作为action），可以理解为利用Bellman Expectation Equation+Greedy Policy value iteration：不直接迭代value function，而是每次都选择当前最大value的state作为action，然后再根据新的action来更新value，可以理解为利用Bellman Optimality Equation value iteration可以看成每次只迭代一次求value function的policy iteration，避免把时间浪费在“烂策略”的value迭代上 待办 强化学习基础知识 跟进大模型强化学习技术</description>
    </item>
    <item>
      <title>Week9 大模型进展</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week9/index.html</link>
      <pubDate>Wed, 31 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week9/index.html</guid>
      <description>总结 跟进大模型进展 跟进大模型进展 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Memory Deep Agents Multi-agent 知识 Memory主要包含working、episodic、semantic和procedure四种 working基本就是短期的上文对话 episodic和context engineering主要相关，用于提取长上文摘要信息 semantic和rag主要相关，用于提取外部信息 procedure则更多通过模型训练方式内化到参数中，形成解决任务方式的记忆 Deep Agents是agent解决长程任务的一种范式，其中重要的一类任务是Deep Research，通过Agentic Search方式搜索信息生成详细的技术报告 Multi-agent目前主要受限于context共享和任务分工后产生的矛盾问题，基本采用简单的planner/executor的串行模式，基于上下文压缩解决过长的问题 待办 强化学习基础知识</description>
    </item>
    <item>
      <title>Week8 大模型进展</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week8/index.html</link>
      <pubDate>Mon, 22 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week8/index.html</guid>
      <description>总结 跟进大模型进展 跟进大模型进展 了解MCP和Skills： MCP和Skills 跟进RAG技术进展： RAG学习手册 了解Context Engineering： Context Engineering学习手册 跟进Agent技术进展， Agent学习手册，主要跟进以下几方面： Planning Memory 知识 MCP统一模型调用外部接口的协议，从而让模型方和外部接口方共同努力实现自己到协议的转写，方便打通链路 Skills和workflow中的规划很像，只是用md文件表示，定义了统一的文件结构，方便模型理解workflow RAG演变到2025年主要采用图谱+向量的混合搜索方式（hybrid search），且通常采用agentic的方式进行图谱的构建 Context Engineering是agent如何选取对于当前轮对话最有用的context的技术 Agent=LLM + 规划 + 记忆 + 工具使用 人为定义好的规划，称为workflow，大模型在过程中实现自我规划，称为Agent 待办 跟进Agent技术进展</description>
    </item>
    <item>
      <title>Week7 React学习</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week7/index.html</link>
      <pubDate>Sun, 14 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week7/index.html</guid>
      <description>总结 利用原生JS完成tetris 学习react基础知识 利用原生JS完成tetris 从零开始构建Tetris React基础知识学习 React学习手册 知识 React是javascript的library，通过JSX（javascript XML）编写，提供了一种通过compenent复用样式的能力，支持基于数据驱动的页面更新 待办 跟进大模型进展</description>
    </item>
    <item>
      <title>Week6 Flappy Bird</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week6/index.html</link>
      <pubDate>Mon, 08 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week6/index.html</guid>
      <description>总结 Javascript进阶学习 利用原生JS搭建Flappy Bird 了解Javascript的包管理器 Javascript面向对象编程 Javascript学习手册-面向对象 课程链接： # Object-oriented Programming in JavaScript: Made Super Simple | Mosh 学习Javascript ES6新特性 Javascript学习手册-ES6新特性 课程链接： # ES6 Tutorial: Learn Modern JavaScript in 1 Hour 开发Flappy Bird 从零开始构建Flappy Bird 了解Javascript的包管理器 Javascript包管理 知识 Javascript ES6 2015年发布，主要引入let、const、class、module等 Javascript的包管理器有npm、yarn、pnpm等 Bun作为nodejs的运行时替代，也包含包管理器，可以完成前后端整体链路 待办 利用原生JS搭建俄罗斯方块 学习React框架基础知识</description>
    </item>
    <item>
      <title>Week5 Javascript学习</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week5/index.html</link>
      <pubDate>Thu, 04 Dec 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week5/index.html</guid>
      <description>总结 完成neovim环境搭建 熟练vim操作 完成Javascript基础知识学习 neovim配置 学习Lua的基础知识： Lua学习手册 配置Neovim环境，安装所需插件： Vim和NeoVim 学习javascript基础知识 Javascript学习手册-基础用法 课程链接 # JavaScript Full Course for free # JavaScript Tutorial Full Course - Beginner to Pro 知识 NeoVim是Vim的新版本重构 通过LazyNvim插件配置 支持语法高亮、语义补全、代码跳转、git等几乎全部所需功能 基本替代vscode，熟练掌握vim操作后，效率提升 html负责内容、css负责样式、javascript负责交互 通过nodejs的live-server可以同步更新目录内的网页状态，类似vscode里的go live插件 node开启javascript命令行 待办 了解Javscript进阶知识：ES6+新特性 Flappy Bird开发</description>
    </item>
    <item>
      <title>Week4 开发环境优化</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week4/index.html</link>
      <pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week4/index.html</guid>
      <description>总结 优化本地开发环境 tmux配置 Tmux使用教程 aerospace配置 Aerospace使用教程 iterm2配置 Terminal配置 知识 通过tmux解决同一个shell下多开进程的操作方式 tmux分为session、window、pane，每个session下可以快速切换window和pane 通过aerospace可以通过键盘快速切换操作页面，如chrome、terminal、obsidian之间 linux上使用i3，macOS上使用aerospace terminal配置主要包含以下几步 安装terminal 安装color themes 安装字体：支持定制化图标 安装插件：powerlevel10k（未使用）、zsh的语义补全和语法高亮 待办 配置neovim环境 熟练vim操作</description>
    </item>
    <item>
      <title>Week3 个人博客搭建</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week3/index.html</link>
      <pubDate>Sun, 16 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week3/index.html</guid>
      <description>总结 学习notion基本功能，并基于notion搭建个人博客 学习css、html、javascript基础知识，搭建简单的github.io主页 基于notion + notion next + vercel搭建个人博客 调研notion建站方案 原生publish：直接点击页面的publish，notion原生样式 开源notion next： 通过next js实现notion内容拉取和前端渲染，可以定制化页面样式 参考链接 notion next fork仓库后可以修改配置 复制notion next作者的模版页面到自己的notion主页中，编辑内容进行博客编写 官网 vercel负责托管部署 关联github账号，import上述仓库，填写NOTION_PAGE_ID，部署 配置域名等与github pages类似 博客链接 学习css、html基础知识并简单搭建github.io css &amp; html课程学习 HTML学习手册-基础概念 CSS学习手册-基础用法 课程链接 # 成為網頁設計師的第一步！快速上手 HTML &amp; CSS 展開你的網頁設計之旅！ # HTML &amp; CSS Full Course for free # HTML &amp; CSS Full Course - Beginner to Pro 搭建简单静态网页 github.io 学习javascript基础知识 javascript课程学习 课程链接 # JavaScript 快速上手！用一個實戰範例迅速掌握所有重點語法！ 知识 作为CMS，notion更偏向于项目管理，而obsidian更偏向于文档记录 在html的body中加入script可以插入js代码，页面f12可以看到console.log的打印 通过在元素上添加listener来捕捉用户行为（点击、输入等），实现相应逻辑（如添加表单元素等） 待办 javascript基础知识 javascript进阶：ES6+新特性</description>
    </item>
    <item>
      <title>Week2 个人博客搭建</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week2/index.html</link>
      <pubDate>Fri, 14 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week2/index.html</guid>
      <description>总结 域名注册 &amp; 图床搭建 尝试利用现有平台进行个人博客的搭建部署，不涉及具体框架或代码开发 学习obsidian基本功能 cloudflare域名注册 ruoruoliu.com，每年10刀左右 cloudflare账户 基于clodflare R2 + piclist搭建免费图床 由于picgo的S3插件上传有问题，替换为piclist 参考链接 基于hexo + github pages搭建个人博客 hexo主要是基于模版的页面生成和部署工具，可以命令行生成并部署到github pages中 每次修改source中的md文件，hexo在generate过程中更新public中对应的部分 github.io作为个人主页，对于仓库名有要求：ruoruoliu.github.io 页面搭建在github pages中的项目页面中： 博客链接 是否需要nginx解决网址栏输入最后没有/导致访问不同的问题？ 参考链接 基于obsidian + hugo + github pages搭建个人博客 hugo将md文件转化为html页面，并进行部署 调研hugo的theme，包括paper、paperMod、terminal和relearn 后续可以在博客更新过程中不断学习relearn基本功能： 主题官网 Obsidian编写md文件，hugo是和hexo类似的cms工具 处理obsidian的内部链接，转化为hugo可接受链接&#xA;使用obsidian插件“image auto upload”，插入图片时自动上传图床&#xA;通过obsidian插件“shell commands”一键同步文件到本地github目录： 参考链接， 插件连接，再push到github远程仓库，触发github pages更新部署</description>
    </item>
    <item>
      <title>Week1 开发环境配置</title>
      <link>https://ruoruoliu.github.io/hugo-blog/weekly/week1/index.html</link>
      <pubDate>Fri, 07 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://ruoruoliu.github.io/hugo-blog/weekly/week1/index.html</guid>
      <description>总结 完成科学上网环境搭建 完成远程开发环境搭建 科学上网配置 购买vps节点： 尝试了vultr和bandwagon两个服务商 vultr按小时收费，最低每月5刀，搬瓦工按quarter收费，三个月50刀 节点选择： 考虑是否支持运营商vip骨干网络，比如移动为cmi 搬瓦工可以使用日本节点，延迟控制在100ms以内，vscode响应时间可接受 参考链接 vscode remote配置 github copilot配置 免费试用 pro 1个月，后续切换到 free plan 后续可以尝试cursor以及claude code 知识 科学上网流程 购买vps节点 vps节点安装并配置代理客户端（s-ui）的tls 本地v2ray配置vps的tls 计算机网络 OSI模型 计算机网络通讯的【系统性】扫盲——从“基本概念”到“OSI 模型” 待办 搭建个人博客，方便后续记笔记</description>
    </item>
  </channel>
</rss>