<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.152.2">
    <meta name="generator" content="Relearn 8.2.0+740f9bc7eaab0594601604e7f7c29ba660a33b9a">
    <meta name="description" content="LLM训练主要包含三部分：
Pre-training：通过大量互联网文本数据，训练base model SFT：通过人工标注对话预料进行微调，让base model掌握对话（ai assistant）能力 RLHF：通过强化学习让模型知道人类偏好，什么是好的回复，什么是不好的回复 参考链接：
# Deep Dive into LLMs like ChatGPT Pre-training 利用大量文本序列，通过预测下一个token的任务训练，得到基座模型（base model），此时模型具备序列文本生成能力
DeepSeek MTP 训练过程中通过多个head同时预测后续多个token，强迫模型预测长期未来 预测的时候，MTP作为 投机解码的提议者，由MTP生成n个token，然后主干模型再计算是否正确，即符合主干模型的概率分布，如果正确则保留，不正确则抛弃，以此循环 MTP像一个滑动窗口在每个主干模型截止的位置发起下一次多token预测 MTP可以比主干模型小一些，但整体速度收益还是在于多token的并发预测 接受MTP结果的判断通常使用拒绝采样来完成 投机解码中使用拒绝采样判断是否接受 投机解码中使用拒绝采样的修正逻辑 # 为什么LLM投机推理小模型被拒绝后要从修正分布获取正确的token？ 参考链接：">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="LLM训练技术学习手册 :: Ruoruoliu 2.0">
    <meta name="twitter:description" content="LLM训练主要包含三部分：
Pre-training：通过大量互联网文本数据，训练base model SFT：通过人工标注对话预料进行微调，让base model掌握对话（ai assistant）能力 RLHF：通过强化学习让模型知道人类偏好，什么是好的回复，什么是不好的回复 参考链接：
# Deep Dive into LLMs like ChatGPT Pre-training 利用大量文本序列，通过预测下一个token的任务训练，得到基座模型（base model），此时模型具备序列文本生成能力
DeepSeek MTP 训练过程中通过多个head同时预测后续多个token，强迫模型预测长期未来 预测的时候，MTP作为 投机解码的提议者，由MTP生成n个token，然后主干模型再计算是否正确，即符合主干模型的概率分布，如果正确则保留，不正确则抛弃，以此循环 MTP像一个滑动窗口在每个主干模型截止的位置发起下一次多token预测 MTP可以比主干模型小一些，但整体速度收益还是在于多token的并发预测 接受MTP结果的判断通常使用拒绝采样来完成 投机解码中使用拒绝采样判断是否接受 投机解码中使用拒绝采样的修正逻辑 # 为什么LLM投机推理小模型被拒绝后要从修正分布获取正确的token？ 参考链接：">
    <meta property="og:url" content="https://ruoruoliu.github.io/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">
    <meta property="og:site_name" content="Ruoruoliu 2.0">
    <meta property="og:title" content="LLM训练技术学习手册 :: Ruoruoliu 2.0">
    <meta property="og:description" content="LLM训练主要包含三部分：
Pre-training：通过大量互联网文本数据，训练base model SFT：通过人工标注对话预料进行微调，让base model掌握对话（ai assistant）能力 RLHF：通过强化学习让模型知道人类偏好，什么是好的回复，什么是不好的回复 参考链接：
# Deep Dive into LLMs like ChatGPT Pre-training 利用大量文本序列，通过预测下一个token的任务训练，得到基座模型（base model），此时模型具备序列文本生成能力
DeepSeek MTP 训练过程中通过多个head同时预测后续多个token，强迫模型预测长期未来 预测的时候，MTP作为 投机解码的提议者，由MTP生成n个token，然后主干模型再计算是否正确，即符合主干模型的概率分布，如果正确则保留，不正确则抛弃，以此循环 MTP像一个滑动窗口在每个主干模型截止的位置发起下一次多token预测 MTP可以比主干模型小一些，但整体速度收益还是在于多token的并发预测 接受MTP结果的判断通常使用拒绝采样来完成 投机解码中使用拒绝采样判断是否接受 投机解码中使用拒绝采样的修正逻辑 # 为什么LLM投机推理小模型被拒绝后要从修正分布获取正确的token？ 参考链接：">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="Blogs">
    <meta property="article:published_time" content="2026-01-05T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-05T00:00:00+00:00">
    <meta property="article:tag" content="技术笔记">
    <meta itemprop="name" content="LLM训练技术学习手册 :: Ruoruoliu 2.0">
    <meta itemprop="description" content="LLM训练主要包含三部分：
Pre-training：通过大量互联网文本数据，训练base model SFT：通过人工标注对话预料进行微调，让base model掌握对话（ai assistant）能力 RLHF：通过强化学习让模型知道人类偏好，什么是好的回复，什么是不好的回复 参考链接：
# Deep Dive into LLMs like ChatGPT Pre-training 利用大量文本序列，通过预测下一个token的任务训练，得到基座模型（base model），此时模型具备序列文本生成能力
DeepSeek MTP 训练过程中通过多个head同时预测后续多个token，强迫模型预测长期未来 预测的时候，MTP作为 投机解码的提议者，由MTP生成n个token，然后主干模型再计算是否正确，即符合主干模型的概率分布，如果正确则保留，不正确则抛弃，以此循环 MTP像一个滑动窗口在每个主干模型截止的位置发起下一次多token预测 MTP可以比主干模型小一些，但整体速度收益还是在于多token的并发预测 接受MTP结果的判断通常使用拒绝采样来完成 投机解码中使用拒绝采样判断是否接受 投机解码中使用拒绝采样的修正逻辑 # 为什么LLM投机推理小模型被拒绝后要从修正分布获取正确的token？ 参考链接：">
    <meta itemprop="datePublished" content="2026-01-05T00:00:00+00:00">
    <meta itemprop="dateModified" content="2026-01-05T00:00:00+00:00">
    <meta itemprop="wordCount" content="622">
    <meta itemprop="keywords" content="技术笔记">
    <title>LLM训练技术学习手册 :: Ruoruoliu 2.0</title>
    <link href="/hugo-blog/images/favicon.png?1770712255" rel="icon" type="image/png">
    <link href="/hugo-blog/css/auto-complete/auto-complete.min.css?1770712255" rel="stylesheet">
    <script src="/hugo-blog/js/auto-complete/auto-complete.min.js?1770712255" defer></script>
    <script src="/hugo-blog/js/search-lunr.min.js?1770712255" defer></script>
    <script src="/hugo-blog/js/search.min.js?1770712255" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/hugo-blog/searchindex.en.js?1770712255";
    </script>
    <script src="/hugo-blog/js/lunr/lunr.min.js?1770712255" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.stemmer.support.min.js?1770712255" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.multi.min.js?1770712255" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.en.min.js?1770712255" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/hugo-blog/fonts/fontawesome/css/fontawesome-all.min.css?1770712255" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/hugo-blog/fonts/fontawesome/css/fontawesome-all.min.css?1770712255" rel="stylesheet"></noscript>
    <link href="/hugo-blog/css/perfect-scrollbar/perfect-scrollbar.min.css?1770712255" rel="stylesheet">
    <link href="/hugo-blog/css/theme.min.css?1770712255" rel="stylesheet">
    <link href="/hugo-blog/css/format-html.min.css?1770712255" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = `.min`;
      window.relearn.path='\/blogs\/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C\/index.html';
      window.relearn.relBasePath='..\/..';
      window.relearn.relBaseUri='..\/..\/..';
      window.relearn.absBaseUri='https:\/\/ruoruoliu.github.io\/hugo-blog';
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy text to clipboard`;
      window.T_Copied_to_clipboard = `Text copied to clipboard!`;
      window.T_Link_copied_to_clipboard = `Link copied to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      window.T_Browser_unsupported_feature = `This browser doesn't support this feature`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
  </head>
  <body class="mobile-support html" data-url="/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button></span>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button></span>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#deepseek">DeepSeek</a>
      <ul>
        <li><a href="#mtp">MTP</a></li>
      </ul>
    </li>
    <li><a href="#moe">Moe</a></li>
  </ul>

  <ul>
    <li><a href="#reject-sampling">Reject Sampling</a></li>
  </ul>

  <ul>
    <li><a href="#ppo">PPO</a></li>
    <li><a href="#dpo">DPO</a></li>
    <li><a href="#grpo">GRPO</a></li>
    <li><a href="#dapo">DAPO</a></li>
    <li><a href="#gspo">GSPO</a></li>
    <li><a href="#tree-grpo">Tree-GRPO</a></li>
    <li><a href="#rlvr">RLVR</a></li>
    <li><a href="#prm">PRM</a></li>
    <li><a href="#reward-hacking">Reward Hacking</a></li>
  </ul>

  <ul>
    <li><a href="#vllm">vLLM</a></li>
    <li><a href="#deepspeed">DeepSpeed</a></li>
    <li><a href="#megatron">Megatron</a></li>
    <li><a href="#verl">Verl</a></li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/hugo-blog/index.html"><span itemprop="name">Ruoruoliu 2.0</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/hugo-blog/blogs/index.html"><span itemprop="name">Blogs</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">LLM训练技术学习手册</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><a href="/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html" title="DeepMind x UCL Introduction to RL 2015 课程笔记 (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a></span>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><a href="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%95%86%E5%93%81%E6%94%B6%E8%B4%A7%E7%B3%BB%E7%BB%9F/index.html" title="从零开始构建商品收货系统 (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a></span>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button></span>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable blogs" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
<div class="R-taxonomy taxonomy-tags cstyle tags" title="Tags" style="--VARIABLE-TAGS-BG-color: var(--INTERNAL-TAG-BG-color);">
  <i class="fa-fw fas fa-layer-group"></i>
  <ul>
    <li><a class="term-link" href="/hugo-blog/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html">技术笔记</a></li>
  </ul>
</div>
  </header>

<h1 id="llm训练技术学习手册">LLM训练技术学习手册</h1>

<p>LLM训练主要包含三部分：</p>
<ul>
<li>Pre-training：通过大量互联网文本数据，训练base model</li>
<li>SFT：通过人工标注对话预料进行微调，让base model掌握对话（ai assistant）能力</li>
<li>RLHF：通过强化学习让模型知道人类偏好，什么是好的回复，什么是不好的回复</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://www.youtube.com/watch?v=7xTGNNLPyMI" target="_blank"># Deep Dive into LLMs like ChatGPT</a></li>
</ul>
<h1 id="pre-training">
  Pre-training







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<p>利用大量文本序列，通过预测下一个token的任务训练，得到基座模型（base model），此时模型具备序列文本生成能力</p>
<h2 id="deepseek">
  DeepSeek







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h3 id="mtp">
  MTP







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><ul>
<li>训练过程中通过多个head同时预测后续多个token，强迫模型预测长期未来</li>
<li>预测的时候，MTP作为



    
<a href="/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/">投机解码</a>的提议者，由MTP生成n个token，然后主干模型再计算是否正确，即符合主干模型的概率分布，如果正确则保留，不正确则抛弃，以此循环
<ul>
<li>MTP像一个滑动窗口在每个主干模型截止的位置发起下一次多token预测</li>
<li>MTP可以比主干模型小一些，但整体速度收益还是在于多token的并发预测</li>
<li>接受MTP结果的判断通常使用拒绝采样来完成
<ul>
<li>



<a href="../Answers/%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6%E6%8E%A5%E5%8F%97.md">投机解码中使用拒绝采样判断是否接受</a></li>
<li>



<a href="../Answers/%E6%8A%95%E6%9C%BA%E8%A7%A3%E7%A0%81%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E7%9A%84%E4%BF%AE%E6%AD%A3%E9%80%BB%E8%BE%91.md">投机解码中使用拒绝采样的修正逻辑</a></li>
<li>



<a href="https://www.zhihu.com/question/1925342080953222752" target="_blank"># 为什么LLM投机推理小模型被拒绝后要从修正分布获取正确的token？</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://zhuanlan.zhihu.com/p/16730036197" target="_blank"># deepseek技术解读(1)-彻底理解MLA（Multi-Head Latent Attention）</a></li>
<li>



<a href="https://zhuanlan.zhihu.com/p/18056041194" target="_blank"># deepseek技术解读(2)-MTP（Multi-Token Prediction）的前世今生</a></li>
<li>



<a href="https://zhuanlan.zhihu.com/p/18565423596" target="_blank"># deepseek技术解读(3)-MoE的演进之路</a></li>
</ul>
<h2 id="moe">
  Moe







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>#todo 主流大模型采用moe结构</p>
<h1 id="sft">
  SFT







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<p>利用人工编写的高质量指令回答数据，在基座模型的基础上训练，让模型模仿人类的回答方式（sft model），此时模型具备问答对话能力</p>
<h2 id="reject-sampling">
  Reject Sampling







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>拒绝采样一般分为四步：</p>
<ul>
<li>生成：基于sft模型利用较高的temperature对同一个prompt进行多次生成</li>
<li>验证：基于reward model或者RLVR来判定</li>
<li>筛选：丢弃所有错误的、低分的回答；选出质量最高的一个或几个正确答案</li>
<li>再训练：将这些被选中的答案加入训练集，对模型进行新一轮的 SFT</li>
</ul>
<h1 id="rlhf">
  RLHF







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<p>进一步微调模型，使其对齐人类的偏好：</p>
<ul>
<li>采样与排序 (Sampling &amp; Ranking)：让 SFT 模型针对同一个问题生成多个不同的回答，然后评价者根据这些回答的质量、安全性、逻辑性进行排序</li>
<li>奖励模型 (Reward Model）：将上述人类排序的数据喂给另一个较小的模型，学习预测人类给回答打分，代替昂贵的人工评价</li>
<li>强化学习算法（PPO/DPO等）：
<ul>
<li>模型生成回答</li>
<li>奖励模型给回答打分</li>
<li>使用强化学习算法根据分数更新LLM参数</li>
</ul>
</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://www.youtube.com/watch?v=aB7ddsbhhaU" target="_blank"># LLM Training &amp; Reinforcement Learning from Google Engineer | SFT + RLHF | PPO vs GRPO vs DPO</a></li>
</ul>
<h2 id="ppo">
  PPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>算法原理：



    
<a href="/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/">Reinforcement Learning学习手册-PPO</a></p>
<p>$\mathcal{J}<em>{\text{PPO}}(\theta) = \mathbb{E}</em>{(q,a) \sim \mathcal{D}, o \le t \sim \pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \min \left( \frac{\pi_\theta(o_t | q, o_{&lt;t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{&lt;t})} \hat{A}<em>t, \text{clip} \left( \frac{\pi</em>\theta(o_t | q, o_{&lt;t})}{\pi_{\theta_{\text{old}}}(o_t | q, o_{&lt;t})}, 1 - \varepsilon, 1 + \varepsilon \right) \hat{A}_t \right) \right]$</p>
<p>其中：$\hat{A}<em>t^{\text{GAE}(\gamma, \lambda)} = \sum</em>{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_l = R_l + \gamma V(s_{l+1}) - V(s_l)$</p>
<p>PPO在RLHF中包含：</p>
<ul>
<li>actor模型：LLM本身，输出下一个token，得到当前状态</li>
<li>reference模型：防止对齐过程中离SFT模型太远，保留sft之后的原始模型</li>
<li>reward模型：用于对模型生成序列打分</li>
<li>critic模型：判断当前状态的价值</li>
</ul>
<p>PPO在RLHF中的流程是：</p>
<ul>
<li>经验收集 (Rollout / Generation)：基于prompt生成回答，作为训练数据</li>
<li>评分与计算 (Evaluation)：
<ul>
<li>基于reward模型给序列打分，为了防止模型为了拿高分而“投机取巧”（比如产生乱码但得分很高），通常会引入KL 散度约束，确保优化后的模型不要偏离reference模型太远</li>
<li>基于critic模型给每个状态打分，计算优势函数 $A_t$，即生成当前token得价值比平均高多少
<ul>
<li>$A(s_t, a_t) = Q(s_t, a_t) - V(s_t) \approx r_t + \gamma V(s_{t+1}) - V(s_t)$</li>
<li>其中 $r_t$ 为即时奖励， 包含：
<ul>
<li>$R_{penalty}(s_t, a_t) = -\beta \cdot \text{KL}(\pi_{\phi}(a_t|s_t) || \pi_{ref}(a_t|s_t))$</li>
<li>$R_{reward_model} = \text{RM}(prompt + response)$ if $t = T$ else 0</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>优化更新 (Optimization)：
<ul>
<li>actor更新：使用PPO算法，如果一个token的 $A_t$ 是正的，就增加生成它的概率；如果是负的，就降低</li>
<li>critic更新：利用TD目标值的MSE损失更新critic模型参数，让评估更准确




<a href="../Answers/GAE%E5%9C%A8%E6%9B%B4%E6%96%B0Critic%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E6%97%B6%E7%9A%84%E4%BD%9C%E7%94%A8.md">GAE在更新Critic模型参数时的作用</a>




<a href="../Answers/GAE%E5%92%8C%E8%B5%84%E6%A0%BC%E8%BF%B9%EF%BC%88Eligibility%20Trace%EF%BC%89%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%9F.md">GAE和资格迹（Eligibility Trace）的关系？</a></li>
</ul>
</li>
</ul>
<h2 id="dpo">
  DPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p><a href="#R-image-4662d7e757002a892bd7ac61c910458b" class="lightbox-link"><img alt="image.png" class="lazy lightbox figure-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/7671020b4ccbe488a483f4771c4a7ba8.png?width=400px" style="height: auto;width: 400px;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-4662d7e757002a892bd7ac61c910458b"><img alt="image.png" class="lazy lightbox lightbox-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/7671020b4ccbe488a483f4771c4a7ba8.png?width=400px"></a>
</p>
<p>PPO存在以下问题：</p>
<ul>
<li>显存占用很大：critic模型的训练更新、reference和reward模型的前向推理</li>
<li>reward model容易有偏，导致模型“刷分”而不解决问题</li>
<li>训练流程复杂，收敛不稳定，超参敏感</li>
</ul>
<p>DPO在RLHF中的流程是：</p>
<ul>
<li>对一个prompt准备正负样本：$y_w$ 和 $y_l$</li>
<li>通过下面公式直接使模型生成 $y_w$ 的概率变大，$y_l$ 的概率变小
<ul>
<li>$\pi_{ref}$ 是sft后的原始模型，$\pi_{\theta}$ 是正在训练的模型
$$L_{DPO} = -\mathbb{E} [\log \sigma (\beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{ref}(y_l|x)})]$$
DPO的优缺点：</li>
</ul>
</li>
<li>只维护两套模型参数、训练极其稳定、在大多数场景中效果优于复杂的PPO</li>
<li>容易过拟合：会过度压低某些词项的概率，导致模型说话变得死板或“复读”</li>
<li>缺乏探索：DPO是离线学习，只能学习你给它的 $y_w$ 和 $y_l$，不会在尝试中发现“更优解”
<ul>
<li>



<a href="../Answers/Online%20DPO%E5%A2%9E%E5%8A%A0%E6%8E%A2%E7%B4%A2%E6%9C%BA%E5%88%B6.md">Online DPO增加探索机制</a></li>
</ul>
</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/2305.18290" target="_blank">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li>
</ul>
<h2 id="grpo">
  GRPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>DeepSeek R1等模型利用GRPO+RLVR实现逻辑推理能力</p>
<p><a href="#R-image-01bff2e1b1cde8c06cb797ab19bb6163" class="lightbox-link"><img alt="image.png" class="lazy lightbox figure-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/82060490fe3d0d9a1297c68559644ea3.png?width=400px" style="height: auto;width: 400px;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-01bff2e1b1cde8c06cb797ab19bb6163"><img alt="image.png" class="lazy lightbox lightbox-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/82060490fe3d0d9a1297c68559644ea3.png?width=400px"></a>
</p>
<p>GRPO的目标函数：
$\mathcal{J}<em>{GRPO}(\theta) = \mathbb{E}</em>{(q,a) \sim \mathcal{D}, {o_i}<em>{i=1}^G \sim \pi</em>{\theta_{\text{old}}}(\cdot|q)} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left( \min \left( r_{i,t}(\theta) \hat{A}<em>{i,t}, \text{clip}(r</em>{i,t}(\theta), 1 - \varepsilon, 1 + \varepsilon) \hat{A}<em>{i,t} \right) - \beta D</em>{\text{KL}}(\pi_\theta || \pi_{\text{ref}}) \right) \right]$
其中：$\hat{A}<em>{i,t} = \frac{r_i - \text{mean}({R_i}</em>{i=1}^G)}{\text{std}({R_i}<em>{i=1}^G)}$，$r</em>{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} | q, o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t} | q, o_{i,&lt;t})}$</p>
<p>针对PPO训练复杂且显存占用高的问题，相比DPO直接去掉RL框架，GRPO选择只去掉critic模型：</p>
<ul>
<li>分组采样：针对同一个Prompt，让模型一次性生成一组回答</li>
<li>计算奖励：用验证器（如RLVR）给这组回答分别打分</li>
<li>相对优势计算 (Relative Advantage)：优势值 = (单个回答的得分 - 组内平均分) / 标准差
<ul>
<li>舍弃critic计算每一个s下V的估计，用基于最终reward的优势替代</li>
<li>critic模型往往比actor模型学习的慢（滞后性），造成眼高手低或者眼低手高</li>
<li>大规模采样下，最终reward（每个token的credit一样）也可以实现对正确率高路径的收敛</li>
</ul>
</li>
<li>更新模型：增加优势值高路径的生成概率，降低优势值低路径的生成概率
<ul>
<li>保留了 PPO的Clipped Surrogate Objective</li>
<li>在损失函数里直接加入 KL 散度，确保新模型不要偏离原始模型太远</li>
</ul>
</li>
</ul>
<p>与DPO使用场景的对比：</p>
<ul>
<li>GRPO更适合推理问题，或者对于奖励有定制化定义的问题</li>
<li>DPO更适合偏好对齐




<a href="../Answers/GRPO%E4%B8%8EDPO%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%8A%E7%9A%84%E5%8C%BA%E5%88%86.md">GRPO与DPO在使用场景上的区分</a></li>
</ul>
<p>为什么GRPO没有学习过程reward也能实现长链路的推理能力？</p>
<ul>
<li>只要你的采样组里有多个回答共享了相同的前缀，GRPO就会自动执行“步骤级”的信用分配，是一个隐形的“PRM”</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/2501.12948" target="_blank">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></li>
<li>



<a href="https://arxiv.org/pdf/2402.03300" target="_blank">DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a></li>
<li>



<a href="https://www.bilibili.com/video/BV15cZYYvEhz/?spm_id_from=333.788.recommend_more_video.0&amp;trackid=web_related_0.router-related-2206419-76tx6.1768750473576.907&amp;vd_source=c8a3c83e361aa2a357093342a046ceed" target="_blank"># 【大白话04】一文理清强化学习PPO和GRPO算法流程 | 原理图解</a></li>
<li>



<a href="https://arxiv.org/pdf/2509.21154" target="_blank">GRPO IS SECRETLY A PROCESS REWARD MODEL</a></li>
</ul>
<h2 id="dapo">
  DAPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>DAPO的目标函数：
$\mathcal{J}<em>{DAPO}(\theta) = \mathbb{E}</em>{(q,a) \sim \mathcal{D}, {o_i}<em>{i=1}^G \sim \pi</em>{\theta_{\text{old}}}(\cdot|q)} \left[ \frac{1}{\sum_{i=1}^G |o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \min \left( r_{i,t}(\theta) \hat{A}<em>{i,t}, \text{clip}(r</em>{i,t}(\theta), 1 - \varepsilon_{\text{low}}, 1 + \varepsilon_{\text{high}}) \hat{A}_{i,t} \right) \right]$
$\text{s.t.} \quad 0 &lt; |{o_i \mid \text{is_equivalent}(a, o_i)}| &lt; G$</p>
<p>其中：$r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} | q, o_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(o_{i,t} | q, o_{i,&lt;t})}$，$\hat{A}<em>{i,t} = \frac{r_i - \text{mean}({R_i}</em>{i=1}^G)}{\text{std}({R_i}_{i=1}^G)}$</p>
<p>DAPO的全称：Decouple Clip and Dynamic sAmpling Policy Optimization</p>
<ul>
<li>Decouple Clip：GRPO的clip的上下限是固定的（比如0.2），导致actor模型的探索不够，容易出现熵崩溃（重复模式）的情况：对于高概率token，乘1.2都已经超过1了，对于低概率token，乘1.2基本变化不大；对clip的上下限解耦，下限保持0.2，上限提高到0.28</li>
<li>Dynamic Sampling：如果一个batch内reward都是0或者1，则梯度消失，浪费算力，因此DAPO过滤掉那些全对或全错的无效样本组，通过反复采样直到这一组里既有对的也有错的，大幅提升了训练效率</li>
<li>Token-Level Policy Gradient Loss：按token进行损失平均，防止长思维链（Long CoT）训练中的“奖励稀释”问题</li>
<li>Overlong Reward Shaping：一般对于超长回复直接reward置零；DAPO改为分段软惩罚，在长度进入缓冲区后开始慢慢加入惩罚，由于是token级别的loss，可以防止模型不知道过长和做错的区别</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/2503.14476" target="_blank">DAPO: An Open-Source LLM Reinforcement Learning System at Scale</a></li>
</ul>
<h2 id="gspo">
  GSPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>GSPO认为reward是在序列级给出的，但GRPO却在token级计算重要性比率，粒度不匹配，token级的局部概率变化会导致重要性权重极端化，累积产生高方差梯度，最终引发不可逆的模型崩溃，因此GSPO将重要性ratio和裁剪改为序列级，可以原生支持MoE训练，不需要



    
<a href="/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/">Routing Replay</a></p>
<p>$\mathcal{J}<em>{\text{GSPO}}(\theta) = \mathbb{E}</em>{x \sim \mathcal{D}, {y_i}<em>{i=1}^G \sim \pi</em>{\theta_{\text{old}}}(\cdot|x)} \left[ \frac{1}{G} \sum_{i=1}^G \min \left( s_i(\theta) \hat{A}_i, \text{clip} \left( s_i(\theta), 1 - \varepsilon, 1 + \varepsilon \right) \hat{A}_i \right) \right]$</p>
<p>其中：$s_i(\theta) = \left( \frac{\pi_\theta(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)} \right)^{\frac{1}{|y_i|}} = \exp \left( \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log \frac{\pi_\theta(y_{i,t}|x, y_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,&lt;t})} \right)$</p>
<p>GSPO牺牲了token级的策略优化，但是在RLHF下，这种牺牲往往是利大于弊的：</p>
<ul>
<li>奖励模型的局限性：现有的RM通常也是序列级的，用Token级的GRPO去强行拟合一个序列级的RM，本身就会引入巨大的噪声</li>
<li>训练稳定性：Token 级的重要性权重比率波动极大，极易触发clipping导致训练停滞。GSPO 通过长度归一化（Length Normalization），让梯度的更新更加平滑</li>
</ul>
<p>GSPO的长度归一化：</p>
<ul>
<li>随着生成序列变长，重要性权重会由于乘法效应发生剧烈的数值波动</li>
<li>使用几何平均，将累乘转变为累加，避免了数值溢出</li>
<li>除以 $|y_i|$：将整条路径的总偏离度转化成了平均每个Token的偏离度</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/2507.18071" target="_blank">Group Sequence Policy Optimization</a></li>
</ul>
<h2 id="tree-grpo">
  Tree-GRPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>#todo 看tree-based的方法</p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/2509.21240" target="_blank">TREE SEARCH FOR LLM AGENT REINFORCEMENT LEARNING</a></li>
</ul>
<h2 id="rlvr">
  RLVR







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>RLVR在PPO/GRPO中用可验证奖励（Verifiable Rewards）替换原始reward model：</p>
<ul>
<li>reward model可能有偏，即一段看起来很专业但逻辑全错的代码，也可能高分</li>
<li>verifiable rewards则只验证明确结论，通过给1分，不通过给0分：
<ul>
<li>数学题的最终答案是否正确</li>
<li>代码题是否能在沙盒环境中跑test</li>
<li>格式是否严格满足要求</li>
</ul>
</li>
</ul>
<p>通过RLVR：</p>
<ul>
<li>彻底解决reward hacking的问题</li>
<li>支持超长链条推理：因为最终的reward很明确，链条再长也可以把reward回传</li>
<li>对于数学/代码这种领域，数据量大且利用率极高</li>
</ul>
<p>值得注意的是：</p>
<ul>
<li>RL训练只是提升模型在Pass@1上采样正确路径的概率，但并没有提升推理能力</li>
<li>过度RL训练会导致模型多样性坍塌</li>
</ul>
<p>



<a href="https://arxiv.org/pdf/2504.13837" target="_blank">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a></p>
<h2 id="prm">
  PRM







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>OpenAI o1推理模型利用PRM实现逻辑推理能力</p>
<p>PRM针对推理过程进行reward预测，细粒度帮助模型学习推理的链路，提高准确性：</p>
<ul>
<li>在解决多步推理问题时，对每一个推理步进行人工打标，获得数据集训练PRM模型</li>
<li>在RL学习过程中，critic模型的reward加入PRM的预测，并将这个能力通过critic内化到actor中</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/2305.20050" target="_blank">Let’s Verify Step by Step</a></li>
</ul>
<h2 id="reward-hacking">
  Reward Hacking







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>



<a href="../Answers/%E4%B8%8D%E5%8F%AF%E9%AA%8C%E8%AF%81%E4%BB%BB%E5%8A%A1%E5%A6%82%E4%BD%95%E7%BC%93%E8%A7%A3Reward%20Hacking%EF%BC%9F.md">不可验证任务如何缓解Reward Hacking？</a></p>
<p>#todo 强化学习调整agent行为模式：retroformer、voyager
#todo 田渊栋：latent reasoning coconut / attention sync / streaming llm
#todo thinking machine lab: tinker api / 自己搭megatron、deepspeed
#todo 



<a href="https://www.bilibili.com/video/BV15yA3eWE5b/?spm_id_from=333.1387.collection.video_card.click&amp;vd_source=c8a3c83e361aa2a357093342a046ceed" target="_blank">R1论文解析</a>
#todo Search-R1、interleaving thinking后训练</p>
<h1 id="distillation">
  Distillation







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<p>#todo on-policy distillation</p>
<h1 id="框架">
  框架







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<p>#todo  框架学习</p>
<h2 id="vllm">
  vLLM







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="deepspeed">
  DeepSpeed







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="megatron">
  Megatron







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="verl">
  Verl







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>参考链接：</p>
<ul>
<li>



<a href="https://www.bilibili.com/video/BV1Pz9tYbEeZ" target="_blank">RL4LLM PPO workflow 及 OpenRLHF、veRL 初步介绍，ray distributed debugger</a></li>
</ul>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Jan 5, 2026
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/hugo-blog/index.html">
            <div class="logo-title">Ruoruoliu 2.0</div>
          </a>
        </div>
        <search><form action="/hugo-blog/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/index.html"><a class="padding" href="/hugo-blog/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/log/index.html"><a class="padding" href="/hugo-blog/log/index.html">Log</a><ul id="R-subsections-23c22f75c84b98545748a65894702014" class="collapsible-menu"></ul></li>
            <li class="" data-nav-url="/hugo-blog/weekly/index.html"><a class="padding" href="/hugo-blog/weekly/index.html">Weeklies</a><ul id="R-subsections-fa6cd6a93db3107c38820d614c82d18d" class="collapsible-menu"></ul></li>
            <li class="parent " data-nav-url="/hugo-blog/blogs/index.html"><a class="padding" href="/hugo-blog/blogs/index.html">Blogs</a><ul id="R-subsections-0c82701243222eae0a7f46ed20de4880" class="collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/blogs/langchain%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/langchain%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LangChain学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/langgraph%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/langgraph%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LangGraph学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/python%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/python%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Python学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%82%E7%82%B9andrej-karpathy%E5%92%8Crichard-sutton/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%82%E7%82%B9andrej-karpathy%E5%92%8Crichard-sutton/index.html">大模型观点：Andrej Karpathy和Richard Sutton</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E7%AE%80%E5%8E%86%E9%A1%B9%E7%9B%AEqa/index.html"><a class="padding" href="/hugo-blog/blogs/%E7%AE%80%E5%8E%86%E9%A1%B9%E7%9B%AEqa/index.html">简历项目Q&amp;A</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LLM推理技术学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Reinforcement Learning学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html"><a class="padding" href="/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html">DeepMind x UCL Introduction to RL 2015 课程笔记</a></li>
            <li class="active " data-nav-url="/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LLM训练技术学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%95%86%E5%93%81%E6%94%B6%E8%B4%A7%E7%B3%BB%E7%BB%9F/index.html"><a class="padding" href="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%95%86%E5%93%81%E6%94%B6%E8%B4%A7%E7%B3%BB%E7%BB%9F/index.html">从零开始构建商品收货系统</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/llm%E8%BF%9B%E5%B1%95%E4%B8%8E%E5%BA%94%E7%94%A8/index.html"><a class="padding" href="/hugo-blog/blogs/llm%E8%BF%9B%E5%B1%95%E4%B8%8E%E5%BA%94%E7%94%A8/index.html">LLM进展与应用</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E9%95%BF%E7%A8%8B%E4%BB%BB%E5%8A%A1deep-agent/index.html"><a class="padding" href="/hugo-blog/blogs/%E9%95%BF%E7%A8%8B%E4%BB%BB%E5%8A%A1deep-agent/index.html">长程任务：Deep Agent</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/agent%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/agent%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Agent学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/context-engineering%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/context-engineering%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Context Engineering学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/rag%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/rag%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">RAG学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/mcp%E5%92%8Cskills/index.html"><a class="padding" href="/hugo-blog/blogs/mcp%E5%92%8Cskills/index.html">MCP和Skills</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/react%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/react%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">React学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAtetris/index.html"><a class="padding" href="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAtetris/index.html">从零开始构建Tetris</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/css%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/css%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">CSS学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/html%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/html%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">HTML学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/javascript%E5%8C%85%E7%AE%A1%E7%90%86/index.html"><a class="padding" href="/hugo-blog/blogs/javascript%E5%8C%85%E7%AE%A1%E7%90%86/index.html">Javascript包管理</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/javascript%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/javascript%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Javascript学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%83%8F%E7%B4%A0%E5%B7%A5%E5%85%B7asperite/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%83%8F%E7%B4%A0%E5%B7%A5%E5%85%B7asperite/index.html">像素工具Asperite</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E6%B8%B8%E6%88%8F%E7%BC%96%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/index.html"><a class="padding" href="/hugo-blog/blogs/%E6%B8%B8%E6%88%8F%E7%BC%96%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/index.html">游戏编程基本概念</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAflappy-bird/index.html"><a class="padding" href="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAflappy-bird/index.html">从零开始构建Flappy Bird</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/aerospace%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"><a class="padding" href="/hugo-blog/blogs/aerospace%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html">Aerospace使用教程</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/lua%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/lua%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Lua学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/terminal%E9%85%8D%E7%BD%AE/index.html"><a class="padding" href="/hugo-blog/blogs/terminal%E9%85%8D%E7%BD%AE/index.html">Terminal配置</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/index.html"><a class="padding" href="/hugo-blog/blogs/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/index.html">Python项目管理</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/vim%E5%92%8Cneovim/index.html"><a class="padding" href="/hugo-blog/blogs/vim%E5%92%8Cneovim/index.html">Vim和NeoVim</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"><a class="padding" href="/hugo-blog/blogs/tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html">Tmux使用教程</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B%E5%B9%B3%E5%8F%B0zapier/index.html"><a class="padding" href="/hugo-blog/blogs/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B%E5%B9%B3%E5%8F%B0zapier/index.html">自动化流程平台Zapier</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E6%95%B4%E7%90%86%E6%96%B9%E5%BC%8F/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E6%95%B4%E7%90%86%E6%96%B9%E5%BC%8F/index.html">内容整理方式</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html">内容管理系统</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/osi%E6%A8%A1%E5%9E%8B/index.html"><a class="padding" href="/hugo-blog/blogs/osi%E6%A8%A1%E5%9E%8B/index.html">OSI模型</a></li></ul></li>
            <li class="" data-nav-url="/hugo-blog/projects/index.html"><a class="padding" href="/hugo-blog/projects/index.html">Projects</a></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script>
      window.MathJax = Object.assign( window.MathJax || {}, {
        tex: {
          inlineMath:  [['\\(', '\\)'], ['$',  '$']],  
          displayMath: [['\\[', '\\]'], ['$$', '$$']], 
        },
        options: {
          enableMenu: false 
        }
      }, JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/hugo-blog/js/mathjax/tex-mml-chtml.js?1770712255"></script>
    <script src="/hugo-blog/js/perfect-scrollbar/perfect-scrollbar.min.js?1770712255" defer></script>
    <script src="/hugo-blog/js/theme.min.js?1770712255" defer></script>
    <div id="toast-container" role="status" aria-live="polite" aria-atomic="false"></div>
  </body>
</html>
