<!DOCTYPE html>
<html lang="en-us" dir="ltr" itemscope itemtype="http://schema.org/Article" data-r-output-format="html">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.156.0">
    <meta name="generator" content="Relearn 8.2.0+740f9bc7eaab0594601604e7f7c29ba660a33b9a">
    <meta name="description" content="基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction &amp; Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation &amp; Exploration 参考链接:
DeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：">
    <meta name="author" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Reinforcement Learning学习手册 :: Ruoruoliu 2.0">
    <meta name="twitter:description" content="基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction &amp; Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation &amp; Exploration 参考链接:
DeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：">
    <meta property="og:url" content="https://ruoruoliu.github.io/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">
    <meta property="og:site_name" content="Ruoruoliu 2.0">
    <meta property="og:title" content="Reinforcement Learning学习手册 :: Ruoruoliu 2.0">
    <meta property="og:description" content="基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction &amp; Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation &amp; Exploration 参考链接:
DeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：">
    <meta property="og:locale" content="en_us">
    <meta property="og:type" content="article">
    <meta property="article:section" content="Blogs">
    <meta property="article:published_time" content="2026-01-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-15T00:00:00+00:00">
    <meta property="article:tag" content="技术笔记">
    <meta itemprop="name" content="Reinforcement Learning学习手册 :: Ruoruoliu 2.0">
    <meta itemprop="description" content="基础知识 Markov Decision Process Bellman Optimality Equation Model-Free Prediction &amp; Control SARSA Q-Learning Value Function Approximation DQN Policy Gradient Actor-Critic Exploitation &amp; Exploration 参考链接:
DeepMind x UCL Introduction to RL 2015 课程笔记 Value-based DQN Target Network 为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：">
    <meta itemprop="datePublished" content="2026-01-15T00:00:00+00:00">
    <meta itemprop="dateModified" content="2026-01-15T00:00:00+00:00">
    <meta itemprop="wordCount" content="692">
    <meta itemprop="keywords" content="技术笔记">
    <title>Reinforcement Learning学习手册 :: Ruoruoliu 2.0</title>
    <link href="/hugo-blog/images/favicon.png?1771772396" rel="icon" type="image/png">
    <link href="/hugo-blog/css/auto-complete/auto-complete.min.css?1771772396" rel="stylesheet">
    <script src="/hugo-blog/js/auto-complete/auto-complete.min.js?1771772396" defer></script>
    <script src="/hugo-blog/js/search-lunr.min.js?1771772396" defer></script>
    <script src="/hugo-blog/js/search.min.js?1771772396" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.index_js_url="/hugo-blog/searchindex.en.js?1771772396";
    </script>
    <script src="/hugo-blog/js/lunr/lunr.min.js?1771772396" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.stemmer.support.min.js?1771772396" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.multi.min.js?1771772396" defer></script>
    <script src="/hugo-blog/js/lunr/lunr.en.min.js?1771772396" defer></script>
    <script>
      window.relearn = window.relearn || {};
      window.relearn.contentLangs=['en'];
    </script>
    <link href="/hugo-blog/fonts/fontawesome/css/fontawesome-all.min.css?1771772396" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/hugo-blog/fonts/fontawesome/css/fontawesome-all.min.css?1771772396" rel="stylesheet"></noscript>
    <link href="/hugo-blog/css/perfect-scrollbar/perfect-scrollbar.min.css?1771772396" rel="stylesheet">
    <link href="/hugo-blog/css/theme.min.css?1771772396" rel="stylesheet">
    <link href="/hugo-blog/css/format-html.min.css?1771772396" rel="stylesheet" id="R-format-style">
    <script>
      window.relearn = window.relearn || {};
      // configuration
      window.relearn.min = `.min`;
      window.relearn.path='\/blogs\/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C\/index.html';
      window.relearn.relBasePath='..\/..';
      window.relearn.relBaseUri='..\/..\/..';
      window.relearn.absBaseUri='https:\/\/ruoruoliu.github.io\/hugo-blog';
      window.relearn.disableInlineCopyToClipboard=false;
      window.relearn.enableBlockCodeWrap=true;
      // legal
      window.relearn.getItem = (s,n) => {return s.getItem(n)};
      window.relearn.setItem = (s,n,v) => {return s.setItem(n,v)};
      window.relearn.removeItem = (s,n) => {return s.removeItem(n)};
      // translations
      window.T_Copy_to_clipboard = `Copy text to clipboard`;
      window.T_Copied_to_clipboard = `Text copied to clipboard!`;
      window.T_Link_copied_to_clipboard = `Link copied to clipboard!`;
      window.T_Reset_view = `Reset view`;
      window.T_View_reset = `View reset!`;
      window.T_No_results_found = `No results found for "{0}"`;
      window.T_N_results_found = `{1} results found for "{0}"`;
      window.T_Browser_unsupported_feature = `This browser doesn't support this feature`;
      // variant stuff
      window.relearn.themevariants = [ 'auto' ];
      window.relearn.customvariantname = "my-custom-variant";
      window.relearn.changeVariant = function(variant) {
        var oldVariant = document.documentElement.dataset.rThemeVariant;
        window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        document.documentElement.dataset.rThemeVariant = variant;
        if (oldVariant != variant) {
          document.dispatchEvent( new CustomEvent('themeVariantLoaded', { detail: { variant, oldVariant } }) );
          window.relearn.markVariant();
        }
      }
      window.relearn.markVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant");
        document.querySelectorAll(".R-variantswitcher select").forEach((select) => {select.value = variant;});
      }
      window.relearn.initVariant = function() {
        var variant = window.relearn.getItem(window.localStorage, window.relearn.absBaseUri + "/variant") ?? "";
        if( variant == window.relearn.customvariantname ){
        }else if( !variant || !window.relearn.themevariants.includes(variant) ){
          variant = window.relearn.themevariants[0];
          window.relearn.setItem(window.localStorage, window.relearn.absBaseUri + "/variant", variant);
        }
        document.documentElement.dataset.rThemeVariant = variant;
      }
      window.relearn.initVariant();
      window.relearn.markVariant();
    </script>
  </head>
  <body class="mobile-support html" data-url="/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">
    <div id="R-body" class="default-animation">
      <div id="R-body-overlay"></div>
      <nav id="R-topbar">
        <div class="topbar-wrapper">
          <div class="topbar-sidebar-divider"></div>
          <div class="topbar-area topbar-area-start" data-area="start">
            <div class="topbar-button topbar-button-sidebar" data-content-empty="disable" data-width-s="show" data-width-m="hide" data-width-l="hide">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleNav()" type="button" title="Menu (CTRL&#43;ALT&#43;n)"><i class="fa-fw fas fa-bars"></i></button></span>
            </div>
            <div class="topbar-button topbar-button-toc" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleTopbarFlyout(this)" type="button" title="Table of Contents (CTRL&#43;ALT&#43;t)"><i class="fa-fw fas fa-list-alt"></i></button></span>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
<nav class="TableOfContents">
  <ul>
    <li><a href="#dqn">DQN</a>
      <ul>
        <li><a href="#target-network">Target Network</a></li>
        <li><a href="#double-dqn">Double DQN</a></li>
        <li><a href="#dueling-network">Dueling Network</a></li>
      </ul>
    </li>
    <li><a href="#experience-replay">Experience Replay</a>
      <ul>
        <li><a href="#prioritized-experience-replay">Prioritized Experience Replay</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#actor-critic">Actor-Critic</a>
      <ul>
        <li><a href="#trpo">TRPO</a></li>
        <li><a href="#ppo">PPO</a></li>
        <li><a href="#ddpg">DDPG</a></li>
        <li><a href="#sac">SAC</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#sparse-rewards">Sparse Rewards</a></li>
    <li><a href="#imitation-learning">Imitation Learning</a>
      <ul>
        <li><a href="#behavior-cloning">Behavior Cloning</a></li>
        <li><a href="#inverse-rl">Inverse RL</a></li>
      </ul>
    </li>
    <li><a href="#model-predictive-control">Model Predictive Control</a></li>
    <li><a href="#offline-rl">Offline RL</a></li>
    <li><a href="#multi-agent-rl">Multi-agent RL</a></li>
  </ul>

  <ul>
    <li><a href="#gymnasium">Gymnasium</a></li>
    <li><a href="#tianshou">TianShou</a></li>
    <li><a href="#sim2real">Sim2Real</a></li>
    <li><a href="#alphago">AlphaGo</a></li>
    <li><a href="#alphastar">AlphaStar</a></li>
    <li><a href="#alphazero">AlphaZero</a></li>
    <li><a href="#muzero">MuZero</a></li>
    <li><a href="#dreamer">Dreamer</a></li>
  </ul>
</nav>
                </div>
              </div>
            </div>
          </div>
          <ol class="topbar-breadcrumbs breadcrumbs highlightable" itemscope itemtype="http://schema.org/BreadcrumbList"><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/hugo-blog/index.html"><span itemprop="name">Ruoruoliu 2.0</span></a><meta itemprop="position" content="1">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><a itemprop="item" href="/hugo-blog/blogs/index.html"><span itemprop="name">Blogs</span></a><meta itemprop="position" content="2">&nbsp;>&nbsp;</li><li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement" class=""><span itemprop="name">Reinforcement Learning学习手册</span><meta itemprop="position" content="3"></li>
          </ol>
          <div class="topbar-area topbar-area-end" data-area="end">
            <div class="topbar-button topbar-button-prev" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><a href="/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html" title="LLM推理技术学习手册 (🡐)"><i class="fa-fw fas fa-chevron-left"></i></a></span>
            </div>
            <div class="topbar-button topbar-button-next" data-content-empty="disable" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><a href="/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html" title="DeepMind x UCL Introduction to RL 2015 课程笔记 (🡒)"><i class="fa-fw fas fa-chevron-right"></i></a></span>
            </div>
            <div class="topbar-button topbar-button-more" data-content-empty="hide" data-width-s="show" data-width-m="show" data-width-l="show">







<span class="btn cstyle button link noborder notitle interactive"><button onclick="toggleTopbarFlyout(this)" type="button" title="More"><i class="fa-fw fas fa-ellipsis-v"></i></button></span>
              <div class="topbar-content">
                <div class="topbar-content-wrapper">
                  <div class="topbar-area topbar-area-more" data-area="more">
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </nav>
      <div id="R-main-overlay"></div>
      <main id="R-body-inner" class="highlightable blogs" tabindex="-1">
        <div class="flex-block-wrapper">
<article class="default">
  <header class="headline">
<div class="R-taxonomy taxonomy-tags cstyle tags" title="Tags" style="--VARIABLE-TAGS-BG-color: var(--INTERNAL-TAG-BG-color);">
  <i class="fa-fw fas fa-layer-group"></i>
  <ul>
    <li><a class="term-link" href="/hugo-blog/tags/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/index.html">技术笔记</a></li>
  </ul>
</div>
  </header>

<h1 id="reinforcement-learning学习手册">Reinforcement Learning学习手册</h1>

<h1 id="基础知识">
  基础知识







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<ul>
<li>Markov Decision Process
<ul>
<li>Bellman Optimality Equation</li>
</ul>
</li>
<li>Model-Free
<ul>
<li>Prediction &amp; Control
<ul>
<li>SARSA</li>
<li>Q-Learning</li>
</ul>
</li>
<li>Value Function Approximation
<ul>
<li>DQN</li>
</ul>
</li>
</ul>
</li>
<li>Policy Gradient
<ul>
<li>Actor-Critic</li>
</ul>
</li>
<li>Exploitation &amp; Exploration</li>
</ul>
<p>参考链接:</p>
<ul>
<li>



    
<a href="/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">DeepMind x UCL Introduction to RL 2015 课程笔记</a></li>
</ul>
<h1 id="value-based">
  Value-based







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<h2 id="dqn">
  DQN







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h3 id="target-network">
  Target Network







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>为了避免Q-learning中Bootstrapping带来的模型追逐变化的目标导致训练不稳定的情况，DQN引入了两套结构完全一样、但参数不同的网络：</p>
<ul>
<li>Online Network（在线网络 / 评估网络）：负责产生即时的 $Q(s, a)$，并进行反向传播更新参数，每一步都在更新</li>
<li>Target Network（目标网络）：专门用来计算目标值（TD Target）中的 $Q(s&rsquo;, a&rsquo;)$ 部分，不经过梯度更新，而是每隔一段时间（比如1000步），从Online Network拷贝参数</li>
</ul>
<p>$Q_{online}(s, a) \leftarrow R + \gamma Q_{target}(s&rsquo;, \arg\max_{a&rsquo;} Q_{target}(s&rsquo;, a&rsquo;))$</p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf" target="_blank">Human-level control through deep reinforcement learning</a></li>
</ul>
<h3 id="double-dqn">
  Double DQN







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>DQN采用Bootstrapping估计Q，而Bootstrapping与Q-learning中的最大化操作会导致对Q的高估：</p>
<ul>
<li>在训练初期或由于函数近似的不精确，预估通常包含噪声，而max操作会自动选择噪声并将包含噪声的Q+noise作为更新目标</li>
<li>Bootstrapping用高估的目标去估计下一个目标，导致高估被传递并放大</li>
</ul>
<p>高估问题对当前state的不同action是不均匀的，因为每次数据采样到就会被高估一次，采样多的 $（s_t, a_t）$ 就会被高估的严重，导致最终最优policy有问题</p>
<p>为了缓解DQN的高估问题，Double DQN将选action和计算action的value拆分为两部分，分别交给Online Network和Target Network完成：</p>
<ul>
<li>Online Network：负责选动作</li>
<li>Target Network：负责估计动作的value</li>
</ul>
<p>这样即便由于高估选择了一个错误的动作，在计算value的时候，由于是另一个网络，仍然会给他较低的分数，防止高估传递下去</p>
<p>$Q_{online}(s, a) \leftarrow R + \gamma Q_{target}(s&rsquo;, \arg\max_{a&rsquo;} Q_{online}(s&rsquo;, a&rsquo;))$</p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/1509.06461" target="_blank">Deep Reinforcement Learning with Double Q-learning</a></li>
</ul>
<h3 id="dueling-network">
  Dueling Network







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>在Dueling Network中，DQN中的Q被分成了两部分，$Q(s, a) = V(s) + A(s, a)$：</p>
<ul>
<li>状态价值函数 $V(s)$：评估当前state本身有多好</li>
<li>优势函数 $A(s, a)$：评估在当前state下，选择某个action比平均水平好多少</li>
</ul>
<p>为了让优势函数A的均值为0，强制V分支去捕捉状态的平均价值，采用对A去中心化的方法：
$Q(s, a) = V(s) + \left( A(s, a) - \frac{1}{n} \sum_{a&rsquo;} A(s, a&rsquo;) \right)$</p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/1511.06581" target="_blank">Dueling Network Architectures for Deep Reinforcement Learning</a></li>
</ul>
<h2 id="experience-replay">
  Experience Replay







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>经验回放有两个主要优点：</p>
<ul>
<li>避免训练数据($s_t, a_t, r_t, s_{t+1}$)的浪费，可以重复使用</li>
<li>避免训练过程中采样的同分布，即相邻序列在训练过程中相邻，防止过拟合</li>
</ul>
<p>经验回放引入了off-policy，即训练数据的策略（旧）已经不是当前学习的策略（新），因此对于policy-based的算法，需要重要性采样（Importance Sampling）进行修正




<a href="../Answers/Policy-Based%E5%9C%A8%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE%E6%97%B6%E4%BD%BF%E7%94%A8%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E4%BF%AE%E6%AD%A3%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7.md">Policy-Based在经验回放时使用重要性采样修正的必要性</a></p>
<h3 id="prioritized-experience-replay">
  Prioritized Experience Replay







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>用非均匀采样代替均匀采样，对于TD error（$\delta_t$）较大的样本采样权重更大：</p>
<ul>
<li>基于TD error：$p_t \propto |\delta_t| + \epsilon$</li>
<li>基于TD error的排序：$p_t \propto \frac{1}{\text{rank}(t)}$




<a href="../Answers/Prioritized%20Experience%20Replay%E4%B8%AD%E7%9A%84%E9%87%87%E6%A0%B7%E6%95%88%E7%8E%87%E9%97%AE%E9%A2%98.md">Prioritized Experience Replay中的采样效率问题</a></li>
</ul>
<p>由于采样权重导致数据分布发生变化（参考off policy），需要对学习率进行调整，即采样权重大的样本，学习率调小：$w_i = \left( \frac{1}{N} \cdot \frac{1}{P(i)} \right)^\beta$，其中在均匀采样中 $P(i)=\frac{1}{N}$，不影响学习率




<a href="../Answers/DQN%E5%9C%A8PER%E6%97%B6%E4%BD%BF%E7%94%A8%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7%E4%BF%AE%E6%AD%A3%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7.md">DQN在PER时使用重要性采样修正的必要性</a></p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/1511.05952" target="_blank">PRIORITIZED EXPERIENCE REPLAY</a></li>
</ul>
<h1 id="policy-based">
  Policy-based







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<h2 id="actor-critic">
  Actor-Critic







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h3 id="trpo">
  TRPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>TRPO的背景在于RL中梯度更新的难度远比监督学习大，按监督学习的learning rate方案极不稳定：




<a href="../Answers/RL%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%9C%A8SGD%E4%B8%8A%E7%9A%84%E5%B7%AE%E5%BC%82.md">RL与监督学习在SGD上的差异</a></p>
<p>置信域算法（Trust Region Methods）分为两个阶段：</p>
<ul>
<li>Approximation：在当前点 $x_k$ 附近，用一个简单的数学模型来代替复杂的原始函数 $f(x)$</li>
<li>Maximization：在信任范围内（满足单调性），找到能让模型下降（或增益极大化）的最佳位移 $p$</li>
</ul>
<p>置信域算法通过自适应方法来确定置信域区间，从而在置信域区间进行迭代求解原函数最小值：
$$\rho_k = \frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}$$
其中：</p>
<ul>
<li>当 $\rho_k$ 接近1时，近似非常准确，优化阶段可以更激进，增大 $\Delta_k$</li>
<li>当 $\rho_k$ 接近0或负数时，近似完全失效，缩小 $\Delta_k$，重新进行近似</li>
<li>当 $\rho_k$ 适中时，近似尚可，保持现状</li>
</ul>
<p>



<a href="../Answers/%E4%BB%80%E4%B9%88%E6%98%AF%E7%BD%AE%E4%BF%A1%E5%9F%9F%E7%AE%97%E6%B3%95%EF%BC%88Trust%20Region%20Methods%EF%BC%89%EF%BC%9F.md">什么是置信域算法（Trust Region Methods）？</a></p>
<p>TRPO不直接约束参数空间的步长，而是约束策略空间的距离（新旧policy之间的KL 散度），确保了新旧policy之间的差距在“置信域”内，尽可能取最大的有效步长，避免了手动设置学习率，使训练收敛更稳定：
$$\max_{\theta} E_{s \sim \rho_{\theta_{old}}, a \sim \pi_{\theta_{old}}} \left[ \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)} A_{\theta_{old}}(s,a) \right]$$</p>
<p>$$\text{subject to } E_{s \sim \rho_{\theta_{old}}} [D_{KL}(\pi_{\theta_{old}}(\cdot|s) || \pi_{\theta}(\cdot|s))] \le \delta$$
这转化为一个约束最优化问题的求解，使用泰勒展开：</p>
<ul>
<li>对目标函数进行一阶展开（得到梯度 $g$）</li>
<li>对约束条件进行二阶展开（得到费雪信息矩阵 $H$，也叫Hessian矩阵）
最后推导出来的参数更新位置公式为：
$$\Delta \theta \approx \sqrt{\frac{2\delta}{g^T H^{-1} g}} H^{-1} g$$
直接把 $\theta$ 推到了那个理论上在约束范围内最完美的位置</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/1502.05477" target="_blank">Trust Region Policy Optimization</a></li>
<li>



<a href="https://www.youtube.com/watch?v=fcSYiyvPjm4" target="_blank"># TRPO 置信域策略优化 (Trust Region Policy Optimization)</a></li>
</ul>
<h3 id="ppo">
  PPO







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>PPO的背景在于TRPO中 $H^{-1}g$ 计算量巨大，于是简化约束条件KL散度，而采用截断的方式来控制新旧策略的差异，假设新旧策略差异为：
$$r_t(\theta) = \frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}$$
截断损失函数为：
$$L^{CLIP} = \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right)$$
假设 $\epsilon=0.2$，有以下几种情况：</p>
<ul>
<li>$r_t(\theta) &lt; 0.8$：新策略action概率变得过小
<ul>
<li>如果 $\hat{A}_t &gt; 0$（action是好动作）：min取真实$r_t(\theta)$，即修正错误（变大）不设限</li>
<li>如果 $\hat{A}_t &lt; 0$（action是坏动作）：min取0.8，梯度停止更新，即不会继续让action概率更小</li>
</ul>
</li>
<li>$r_t(\theta) &gt; 1.2$：新策略action概率变得过大
<ul>
<li>如果 $\hat{A}_t &gt; 0$（action是好动作）：min取1.2，梯度停止更新，即不会继续让action概率更大</li>
<li>如果 $\hat{A}_t &lt; 0$（action是坏动作）：min取真实$r_t(\theta)$，即修正错误（变小）不设限</li>
</ul>
</li>
<li>$0.8 &lt; r_t(\theta) &lt; 1.2$：新策略action小幅变化，clip不生效，取真实$r_t(\theta)$
<ul>
<li>如果 $\hat{A}_t &gt; 0$（action是好动作）：继续梯度更新，让action概率变大</li>
<li>如果 $\hat{A}_t &lt; 0$（action是坏动作）：继续梯度更新，让action概率变小</li>
</ul>
</li>
</ul>
<p>PPO两大工程优势：</p>
<ul>
<li>防止策略崩溃：即使你把 Learning Rate 设得稍微大了一点，clip也会挡住那些过度的更新</li>
<li>数据高效利用：只要 $r_t$ 还没被clip，这批数据就可以反复用来跑好几次 SGD




<a href="../Answers/PPO%E4%B8%AD%E9%87%87%E6%A0%B7%E6%95%B0%E6%8D%AE%E8%A2%AB%E9%AB%98%E6%95%88%E5%88%A9%E7%94%A8%E7%9A%84%E5%8E%9F%E5%9B%A0.md">PPO中采样数据被高效利用的原因</a></li>
</ul>
<p>



<a href="../Answers/PPO%E4%B8%ADClip%E6%93%8D%E4%BD%9C%E5%AF%B9Bias%E5%92%8CVariance%E7%9A%84Trade-Off.md">PPO中Clip操作对Bias和Variance的Trade-Off</a></p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/1707.06347" target="_blank">Proximal Policy Optimization Algorithms</a></li>
</ul>
<h3 id="ddpg">
  DDPG







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>离散控制与连续控制：</p>
<ul>
<li>离散动作空间只包含有限个动作，比如grid中的四个方向的运动</li>
<li>连续动作空间的动作是连续的，比如机械手转动的角度，属于一个连续范围</li>
<li>对连续动作离散化可以近似解决，但是会有维度爆炸的问题，所以只适用于维度小的问题</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://www.youtube.com/watch?v=rRIjgdxSvg8&amp;list=PLvOO0btloRnsNfDgwv0OCLVTsm5bUyE6L" target="_blank"># 离散控制与连续控制 (连续控制 1/3)</a>)</li>
</ul>
<p>由于连续动作空间中，action是一个数值变量：</p>
<ul>
<li>如果还按离散控制的方式，采样来计算梯度，会发现由于维度爆炸，采样数据稀疏导致极不稳定</li>
<li>action是数值变量意味着可以求导，因此可以通过链式法则，直接对 $\theta$ 求导：
$$\mathbf{g} = \frac{\partial q(s, \pi(s; \boldsymbol{\theta}); \mathbf{w})}{\partial \boldsymbol{\theta}} = \frac{\partial a}{\partial \boldsymbol{\theta}} \cdot \frac{\partial q(s, a; \mathbf{w})}{\partial a}$$</li>
</ul>
<p>DPG中由于critic和actor的梯度更新都依赖于critic的 $Q$ ，会导致不稳定相比离散控制更强烈：</p>
<ul>
<li>target network：引入target actor和target critic</li>
<li>参数软更新： target网络的参数 $w^-$ 以极小的比例 $\tau$（如 0.001）缓慢跟随主网络</li>
</ul>
<p><a href="#R-image-9f8e9e6f1ca02178961e36593640df1b" class="lightbox-link"><img alt="image.png" class="lazy lightbox figure-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/9624880b724caa65646153cd9e7c8208.png?width=400px" style="height: auto;width: 400px;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-9f8e9e6f1ca02178961e36593640df1b"><img alt="image.png" class="lazy lightbox lightbox-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/9624880b724caa65646153cd9e7c8208.png?width=400px"></a>
</p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://proceedings.mlr.press/v32/silver14.pdf" target="_blank">Deterministic Policy Gradient Algorithms</a></li>
<li>



<a href="https://arxiv.org/pdf/1509.02971" target="_blank">CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING</a></li>
<li>



<a href="https://www.youtube.com/watch?v=cmWejKRWLA8" target="_blank"># 确定策略梯度 Deterministic Policy Gradient, DPG (连续控制 2/3)</a></li>
</ul>
<h3 id="sac">
  SAC







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>SAC通过随机策略来解决连续控制问题，预测action的均值 $\mu$ 和方差 $\sigma$，将确定性action转化为高斯分布，从而最大化动作的累积收益</p>
<p>在 SAC 的目标函数中，除了奖励 $r$ 之外，还最大化熵 ($H$)：
$$J(\pi) = \sum_{t=0}^{T} E_{(s_t, a_t) \sim \rho_\pi} [r(s_t, a_t) + \alpha H(\pi(\cdot|s_t))]$$</p>
<ul>
<li>防止模型过早陷入局部最优解，鼓励去尝试不同的动作</li>
<li>随机策略可以学到多种种同样好的解法，如果环境发生轻微扰动，表现得更稳健</li>
</ul>
<p>连续控制中使用 $\nabla_{\theta} J(\theta) = \mathbb{E}<em>{a \sim \pi</em>{\theta}} [ \nabla_{\theta} \log \pi_{\theta}(a|s) \cdot Q(s, a) ]$ 求解策略梯度方差极大，因此SAC采用了和DDPG同样的确定性梯度的路径（链式法则）；由于action不是数值变量，而是从分布中采样得到的，因此需要重参数化的技巧，将随机采样的过程转化为一个确定的函数：
$$a = f_{\theta}(s, \epsilon) = \mu_{\theta}(s) + \sigma_{\theta}(s) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$</p>
<p><a href="#R-image-af6056329875b6eea102c316b20acd2d" class="lightbox-link"><img alt="image.png" class="lazy lightbox figure-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/8b76631eceafc626642117e9be110ef5.png?width=250px" style="height: auto;width: 250px;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-af6056329875b6eea102c316b20acd2d"><img alt="image.png" class="lazy lightbox lightbox-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/8b76631eceafc626642117e9be110ef5.png?width=250px"></a>
</p>
<p>



<a href="../Answers/SAC%E5%8D%95%E5%B3%B0%E5%81%87%E8%AE%BE%E9%81%87%E5%88%B0%E5%A4%9A%E5%B3%B0%E6%83%85%E5%86%B5%E6%97%B6%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F.md">SAC单峰假设遇到多峰情况时如何解决？</a></p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://arxiv.org/pdf/1801.01290" target="_blank">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a></li>
<li>



<a href="https://www.youtube.com/watch?v=McqFyl_W5Wc&amp;list=PLvOO0btloRnsNfDgwv0OCLVTsm5bUyE6L&amp;index=3" target="_blank"># 随机策略做连续控制 (连续控制 3/3)</a></li>
</ul>
<h1 id="exploitation--exploration">
  Exploitation &amp; Exploration







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<p>#todo 看下面这篇blog</p>
<p>参考链接：</p>
<ul>
<li>



<a href="https://lilianweng.github.io/posts/2020-06-07-exploration-drl/" target="_blank"># Exploration Strategies in Deep Reinforcement Learning</a></li>
</ul>
<h1 id="相关领域">
  相关领域







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<p>#todo 动手学强化学习这些章节</p>
<h2 id="sparse-rewards">
  Sparse Rewards







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>真实场景中奖励过于稀疏，学习很难，解决方向有以下几个：</p>
<ul>
<li>设计奖励：
<ul>
<li>在初期对Q建模不够好或者未来价值衰减过大时，需要人为设计奖励来鼓励进行特定action</li>
<li>人为设计的奖励需要领域知识，使策略与目标更近</li>
</ul>
</li>
<li>好奇心驱动的奖励：
<ul>
<li>基于当前状态和动作，对下一次状态进行预测，预测越不准奖励越大</li>
<li>为防止预测与动作无关（单纯环境的不可预测），另外对状态进行特征提取，并基于特征学习状态间的动作，特征网络学习好后，特征只与动作相关
<a href="#R-image-9cec13814167d3799afeebfc113c299a" class="lightbox-link"><img alt="image.png" class="lazy lightbox figure-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/02e6b983d1df5d1164910da6f17d19e9.png?width=300px" style="height: auto;width: 300px;"></a>
<a href="javascript:history.back();" class="lightbox-back" id="R-image-9cec13814167d3799afeebfc113c299a"><img alt="image.png" class="lazy lightbox lightbox-image" loading="lazy" src="https://images.ruoruoliu.com/2026/01/02e6b983d1df5d1164910da6f17d19e9.png?width=300px"></a>
</li>
</ul>
</li>
<li>课程学习：
<ul>
<li>通过安排学习的难度从易到难，需要人为设计</li>
<li>通过逆课程学习，从目标状态倒退找到合适学习的状态，避免人为设计</li>
</ul>
</li>
<li>分层强化学习：
<ul>
<li>将一个复杂强化学习问题分解成多个子问题，每个子问题（包含高层次策略和低层次策略），多agent采用合作的模式，分别处理不同层次的策略</li>
</ul>
</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://datawhalechina.github.io/easy-rl/#/chapter10/chapter10?id=%e7%ac%ac10%e7%ab%a0-%e7%a8%80%e7%96%8f%e5%a5%96%e5%8a%b1" target="_blank">蘑菇书EasyRL 第10章 稀疏奖励</a></li>
</ul>
<h2 id="imitation-learning">
  Imitation Learning







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>在模仿学习中，有一些专家的示范，智能体也可以与环境交互，但它无法从环境里得到任何的奖励，它只能通过专家的示范来学习动作的好坏</p>
<h3 id="behavior-cloning">
  Behavior Cloning







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>行为克隆可以看成是监督学习的问题，给定一组专家数据（ $s_t$ 到 $a_t$ 的映射），agent进行拟合。</p>
<p>专家数据覆盖不全，out-of-distribution的情况agent不知道怎么处理：</p>
<ul>
<li>数据集聚合（DAgger）：通过agent运行过程中遇到危险情况时，专家给出的action建议，加入数据中，进一步训练agent</li>
</ul>
<p>行为克隆的其他问题：</p>
<ul>
<li>行为克隆会学习专家全部action，即便有些action是无意义或者低效的</li>
<li>任何时刻action偏差了一点，会导致后续状态的偏差，慢慢累积，最终会相差很多</li>
</ul>
<h3 id="inverse-rl">
  Inverse RL







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h3><p>通过专家数据以及环境，反向找到奖励函数，有了奖励函数后，我们就可以用强化学习的方法来处理问题，这个过程称为逆强化学习</p>
<p>逆强化学习流程：</p>
<ul>
<li>观察专家：收集大量人类专家的驾驶轨迹（状态 $s$ 和动作 $a$ 的序列）</li>
<li>假设与反推：不断尝试建立一个奖励函数 $R(s, a)$，使得专家在这个函数下看起来是最优的</li>
<li>验证与循环：用奖励函数去跑一遍，如果结果和专家不像，回来修改奖励函数，直到一致</li>
</ul>
<p>逆强化学习问题：</p>
<ul>
<li>计算量巨大：每推测一次奖励函数，都要完整地进行一次强化学习训练来验证，非常耗时</li>
<li>专家未必完美：人类专家也非最优，把这些“坏习惯”背后的逻辑也学走也不好</li>
<li>多解性：专家的一个动作可能有多种解释</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://datawhalechina.github.io/easy-rl/#/chapter11/chapter11?id=%e7%ac%ac11%e7%ab%a0-%e6%a8%a1%e4%bb%bf%e5%ad%a6%e4%b9%a0" target="_blank">蘑菇书EasyRL 第11章 模仿学习</a></li>
</ul>
<h2 id="model-predictive-control">
  Model Predictive Control







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="offline-rl">
  Offline RL







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="multi-agent-rl">
  Multi-agent RL







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>多agent之间的关系分为以下几种：</p>
<ul>
<li>完全合作（fully cooperative）：多agent目标相同</li>
<li>完全竞争（fully competitive）：零和博弈</li>
<li>合作/竞争混合（mixed cooperative &amp; competitive）：自己团队内部为合作，对方团队为竞争</li>
<li>利己主义（self-interested）：只考虑自己，不在乎对方</li>
</ul>
<p>多agent策略收敛：</p>
<ul>
<li>纳什均衡时策略收敛，即当一个agent改变策略而其他agent不变时，他的return不会变好</li>
<li>除非所有agent彼此独立，才能用单agent方法求解最优策略
<ul>
<li>单agent方法有可能无法收敛，因为改变自己参数时，会改变其他agent的目标</li>
</ul>
</li>
<li>多agent（合作）需要通过通信共享信息从而达到最优策略的收敛</li>
</ul>
<p>多agent的通信方式与训练：</p>
<ul>
<li>完全去中心化（fully decentralized）：多agent不通信
<ul>
<li>agent视角与单agent完全一样，最终可能无法收敛</li>
</ul>
</li>
<li>完全中心化（fully centralized）：agent将信息发送给controller，controller为所有agent决策
<ul>
<li>由于controller的决策需要用到所有agent的观测，因此不能在agent上独立执行</li>
<li>可以把多agent理解为一个大的agent，汇合了所有agent的state、action和reward</li>
<li>同步比较耗时，很难做到实时性</li>
</ul>
</li>
<li>中心训练、去中心执行：训练时使用controller，执行时不使用
<ul>
<li>controller根据全部的信息为每个agent训练专属的critic网络</li>
<li>每个agent根据controller的critic训练自己的actor，然后在执行中使用</li>
</ul>
</li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://www.youtube.com/watch?v=KN-XMQFTD0o&amp;list=PLvOO0btloRntPRHgQo75wuMNFvtXHGn5A" target="_blank"># 多智能体强化学习(1/2)：基本概念 Multi-Agent Reinforcement Learning</a></li>
<li>



<a href="https://www.youtube.com/watch?v=0HV1hsjd1y8&amp;list=PLvOO0btloRntPRHgQo75wuMNFvtXHGn5A&amp;index=2" target="_blank"># 多智能体强化学习(2/2)：三种架构 Multi-Agent Reinforcement Learning</a></li>
</ul>
<h1 id="应用">
  应用







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h1><hr>
<h2 id="gymnasium">
  Gymnasium







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>



<a href="https://gymnasium.farama.org/" target="_blank">官网链接</a></p>
<h2 id="tianshou">
  TianShou







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="sim2real">
  Sim2Real







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="alphago">
  AlphaGo







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>参考链接：</p>
<ul>
<li>



<a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" target="_blank">Mastering the game of Go with deep neural networks and tree search</a></li>
</ul>
<h2 id="alphastar">
  AlphaStar







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>参考链接：</p>
<ul>
<li>



<a href="https://datawhalechina.github.io/easy-rl/#/chapter13/chapter13?id=%e7%ac%ac13%e7%ab%a0-alphastar-%e8%ae%ba%e6%96%87%e8%a7%a3%e8%af%bb" target="_blank">蘑菇书EasyRL 第13章 AlphaStar 论文解读</a></li>
</ul>
<h2 id="alphazero">
  AlphaZero







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><h2 id="muzero">
  MuZero







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>参考链接：</p>
<ul>
<li>



<a href="https://deepmind.google/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/" target="_blank"># MuZero: Mastering Go, chess, shogi and Atari without rules</a></li>
</ul>
<h2 id="dreamer">
  Dreamer







<span class="btn cstyle anchor copyanchor scrollanchor link noborder notitle interactive"><button type="button" title="Copy link to clipboard"><i class="fa-fw fas fa-link fa-lg"></i></button></span>
</h2><p>参考链接：</p>
<ul>
<li>



<a href="https://research.google/blog/introducing-dreamer-scalable-reinforcement-learning-using-world-models/" target="_blank"># Introducing Dreamer: Scalable Reinforcement Learning Using World Models</a></li>
<li>



<a href="https://research.google/blog/mastering-atari-with-discrete-world-models/" target="_blank"># Mastering Atari with Discrete World Models</a></li>
<li>



<a href="https://danijar.com/project/dreamerv3/" target="_blank"># Mastering Diverse Control Tasks through World Models</a></li>
<li>



<a href="https://danijar.com/project/dreamer4/" target="_blank"># Training Agents Inside of Scalable World Models</a></li>
</ul>
<p>参考链接：</p>
<ul>
<li>



<a href="https://www.youtube.com/watch?v=VnpRp7ZglfA" target="_blank"># The FASTEST introduction to Reinforcement Learning on the internet</a></li>
<li>



<a href="https://hrl.boyuai.com/" target="_blank">动手学强化学习</a></li>
<li>



<a href="https://www.youtube.com/playlist?list=PLvOO0btloRnsiqM72G4Uid0UWljikENlU" target="_blank"># 深度强化学习基础【王树森】</a></li>
<li>



<a href="https://datawhalechina.github.io/easy-rl/#/" target="_blank">蘑菇书EasyRL</a></li>
<li>



<a href="https://spinningup.openai.com/en/latest/#" target="_blank">OpenAI Spinning Up</a></li>
</ul>

  <footer class="footline">
              <i class='fa-fw fas fa-calendar'></i> Jan 15, 2026
  </footer>
</article>
        </div>
      </main>
    </div>
    <aside id="R-sidebar" class="default-animation">
      <div id="R-header-topbar" class="default-animation"></div>
      <div id="R-header-wrapper" class="default-animation">
        <div id="R-header" class="default-animation">
          <a id="R-logo" class="R-default" href="/hugo-blog/index.html">
            <div class="logo-title">Ruoruoliu 2.0</div>
          </a>
        </div>
        <search><form action="/hugo-blog/search/index.html" method="get">
          <div class="searchbox default-animation">
            <button class="search-detail" type="submit" title="Search (CTRL+ALT+f)"><i class="fas fa-search"></i></button>
            <label class="a11y-only" for="R-search-by">Search</label>
            <input data-search-input id="R-search-by" name="search-by" class="search-by" type="search" placeholder="Search...">
            <button class="search-clear" type="button" data-search-clear="" title="Clear search"><i class="fas fa-times" title="Clear search"></i></button>
          </div>
        </form></search>
      </div>
      <div id="R-homelinks" class="default-animation homelinks">
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-homelinks">
          <ul class="space collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/index.html"><a class="padding" href="/hugo-blog/index.html"><i class="fa-fw fas fa-home"></i> Home</a></li>
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-headercontrols">
          <ul class="">
          </ul>
        </div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
      </div>
      <div id="R-content-wrapper" class="highlightable">
        <div class="R-sidebarmenu R-shortcutmenu-main">
          <ul class="enlarge morespace collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/log/index.html"><a class="padding" href="/hugo-blog/log/index.html">Log</a><ul id="R-subsections-23c22f75c84b98545748a65894702014" class="collapsible-menu"></ul></li>
            <li class="" data-nav-url="/hugo-blog/weekly/index.html"><a class="padding" href="/hugo-blog/weekly/index.html">Weeklies</a><ul id="R-subsections-fa6cd6a93db3107c38820d614c82d18d" class="collapsible-menu"></ul></li>
            <li class="parent " data-nav-url="/hugo-blog/blogs/index.html"><a class="padding" href="/hugo-blog/blogs/index.html">Blogs</a><ul id="R-subsections-0c82701243222eae0a7f46ed20de4880" class="collapsible-menu">
            <li class="" data-nav-url="/hugo-blog/blogs/langchain%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/langchain%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LangChain学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/langgraph%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/langgraph%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LangGraph学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/python%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/python%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Python学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%82%E7%82%B9andrej-karpathy%E5%92%8Crichard-sutton/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%A7%82%E7%82%B9andrej-karpathy%E5%92%8Crichard-sutton/index.html">大模型观点：Andrej Karpathy和Richard Sutton</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E7%AE%80%E5%8E%86%E9%A1%B9%E7%9B%AEqa/index.html"><a class="padding" href="/hugo-blog/blogs/%E7%AE%80%E5%8E%86%E9%A1%B9%E7%9B%AEqa/index.html">简历项目Q&amp;A</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/llm%E6%8E%A8%E7%90%86%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LLM推理技术学习手册</a></li>
            <li class="active " data-nav-url="/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/reinforcement-learning%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Reinforcement Learning学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html"><a class="padding" href="/hugo-blog/blogs/deepmind-x-ucl-introduction-to-rl-2015-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/index.html">DeepMind x UCL Introduction to RL 2015 课程笔记</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/llm%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">LLM训练技术学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%95%86%E5%93%81%E6%94%B6%E8%B4%A7%E7%B3%BB%E7%BB%9F/index.html"><a class="padding" href="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%95%86%E5%93%81%E6%94%B6%E8%B4%A7%E7%B3%BB%E7%BB%9F/index.html">从零开始构建商品收货系统</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/llm%E8%BF%9B%E5%B1%95%E4%B8%8E%E5%BA%94%E7%94%A8/index.html"><a class="padding" href="/hugo-blog/blogs/llm%E8%BF%9B%E5%B1%95%E4%B8%8E%E5%BA%94%E7%94%A8/index.html">LLM进展与应用</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E9%95%BF%E7%A8%8B%E4%BB%BB%E5%8A%A1deep-agent/index.html"><a class="padding" href="/hugo-blog/blogs/%E9%95%BF%E7%A8%8B%E4%BB%BB%E5%8A%A1deep-agent/index.html">长程任务：Deep Agent</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/agent%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/agent%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Agent学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/context-engineering%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/context-engineering%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Context Engineering学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/rag%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/rag%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">RAG学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/mcp%E5%92%8Cskills/index.html"><a class="padding" href="/hugo-blog/blogs/mcp%E5%92%8Cskills/index.html">MCP和Skills</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/react%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/react%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">React学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAtetris/index.html"><a class="padding" href="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAtetris/index.html">从零开始构建Tetris</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/css%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/css%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">CSS学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/html%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/html%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">HTML学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/javascript%E5%8C%85%E7%AE%A1%E7%90%86/index.html"><a class="padding" href="/hugo-blog/blogs/javascript%E5%8C%85%E7%AE%A1%E7%90%86/index.html">Javascript包管理</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/javascript%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/javascript%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Javascript学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%83%8F%E7%B4%A0%E5%B7%A5%E5%85%B7asperite/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%83%8F%E7%B4%A0%E5%B7%A5%E5%85%B7asperite/index.html">像素工具Asperite</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E6%B8%B8%E6%88%8F%E7%BC%96%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/index.html"><a class="padding" href="/hugo-blog/blogs/%E6%B8%B8%E6%88%8F%E7%BC%96%E7%A8%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/index.html">游戏编程基本概念</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAflappy-bird/index.html"><a class="padding" href="/hugo-blog/blogs/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BAflappy-bird/index.html">从零开始构建Flappy Bird</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/aerospace%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"><a class="padding" href="/hugo-blog/blogs/aerospace%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html">Aerospace使用教程</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/lua%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html"><a class="padding" href="/hugo-blog/blogs/lua%E5%AD%A6%E4%B9%A0%E6%89%8B%E5%86%8C/index.html">Lua学习手册</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/terminal%E9%85%8D%E7%BD%AE/index.html"><a class="padding" href="/hugo-blog/blogs/terminal%E9%85%8D%E7%BD%AE/index.html">Terminal配置</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/index.html"><a class="padding" href="/hugo-blog/blogs/python%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/index.html">Python项目管理</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/vim%E5%92%8Cneovim/index.html"><a class="padding" href="/hugo-blog/blogs/vim%E5%92%8Cneovim/index.html">Vim和NeoVim</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html"><a class="padding" href="/hugo-blog/blogs/tmux%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/index.html">Tmux使用教程</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B%E5%B9%B3%E5%8F%B0zapier/index.html"><a class="padding" href="/hugo-blog/blogs/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B%E5%B9%B3%E5%8F%B0zapier/index.html">自动化流程平台Zapier</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E6%95%B4%E7%90%86%E6%96%B9%E5%BC%8F/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E6%95%B4%E7%90%86%E6%96%B9%E5%BC%8F/index.html">内容整理方式</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html"><a class="padding" href="/hugo-blog/blogs/%E5%86%85%E5%AE%B9%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/index.html">内容管理系统</a></li>
            <li class="" data-nav-url="/hugo-blog/blogs/osi%E6%A8%A1%E5%9E%8B/index.html"><a class="padding" href="/hugo-blog/blogs/osi%E6%A8%A1%E5%9E%8B/index.html">OSI模型</a></li></ul></li>
            <li class="" data-nav-url="/hugo-blog/projects/index.html"><a class="padding" href="/hugo-blog/projects/index.html">Projects</a></li>
          </ul>
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-shortcuts">
          <ul class="space collapsible-menu">
          </ul>
        </div>
        <div id="R-footer-margin"></div>
        <div class="R-menu-divider default-animation">
          <hr class="padding">
        </div>
        <div class="R-sidebarmenu R-shortcutmenu-footercontrols">
          <ul class="">
          </ul>
        </div>
<div id="R-footer"><p>Built with <a href="https://github.com/McShelby/hugo-theme-relearn" title="love"><i class="fas fa-heart"></i></a> by <a href="https://gohugo.io/">Hugo</a></p></div>
      </div>
    </aside>
    <script>
      window.MathJax = Object.assign( window.MathJax || {}, {
        tex: {
          inlineMath:  [['\\(', '\\)'], ['$',  '$']],  
          displayMath: [['\\[', '\\]'], ['$$', '$$']], 
        },
        options: {
          enableMenu: false 
        }
      }, JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="/hugo-blog/js/mathjax/tex-mml-chtml.js?1771772396"></script>
    <script src="/hugo-blog/js/perfect-scrollbar/perfect-scrollbar.min.js?1771772396" defer></script>
    <script src="/hugo-blog/js/theme.min.js?1771772396" defer></script>
    <div id="toast-container" role="status" aria-live="polite" aria-atomic="false"></div>
  </body>
</html>
