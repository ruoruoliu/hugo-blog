---
title: 大模型观点：Andrej Karpathy和Richard Sutton
date: 2026-01-31
tags:
draft: false
---
> [!NOTE] Andrej Karpathy
> - 与动物的进化和学习过程类比：
> 	- 动物在出生的时候就用一套基础配置（物理设备和算法），比如斑马出生一个小时就会跟着妈妈走，这不是强化学习学到的，而是类似DNA中遗传的某种算法
> 	- 这部分类比大模型的预训练，但是不同的是：
> 		- 大模型花费大量时间（动物很快）从零开始（动物出生有DNA）进行预训练，
> 		- 不光把思维模式学到了，还学习了知识
> 		- 应该有一种方式可以剥离预训练学到的知识，因为这部分知识反而会阻碍后续的发展（比如知识陈旧或者思维定式）
> 	- RL应该只在运动类任务（如走跑跳），但在需要智力任务中不应该完全依赖RL
> - RL在LLM中的问题：
> 	- RL基于一个reward对路径上的所有token进行加权或者减权，这不合理，因为中间的过程可能是错的
> 	- 人类在做智力任务（比如数学题）的时候，会反思，判断哪部分做对了，哪部分做错了，而不是基于最终的一个答案来调整所有的路径
> 	- 需要过程奖励，但是目前只能用LLM as judge来判断，而LLM是参数化的，因此可以进行reward hacking，因此这个过程也不可信
> - LLM的预训练过程缺乏深度理解：
> 	- 预训练过程需要反思，但是这个很难，因为LLM通常都有坍缩的问题，缺少多样性，如果在这种样本（比如阅读某一本书，然后让他总结这本书）继续训练，会加大坍缩的问题
> 	- 儿童就比成年人多样性强，随着经验增多，熵变小；人类因为记忆能力不如LLM，因此强迫需要掌握规律；这里又一次说明是否有一种方式可以将学习过程中的知识去掉，保留逻辑推理能力
> 	- 如何解决预训练带来的多样性问题？用naive的方式增加多样性（比如强制prob平滑）也不合理，因为有些任务就是需要确定性
 
参考链接：
- [# Andrej Karpathy — “We’re summoning ghosts, not building animals”](https://www.youtube.com/watch?v=lXUZvyajciY)

> [!NOTE] Richard Sutton
> - 关于LLM是不是智能的体现
> 	- 应该关注人类与动物相同的部分，这部分才是智能的体现（了解松鼠的进化已经完成了90%的工作），而不应该关注人类与动物不同的部分（语言）
> 	- LLM只是通过在静态数据集上训练，预测下一句话说什么，静态数据集不预测世界会发生什么，所以不是真正的世界模型
> 	- 智能需要经历真实世界，交互并获得反馈，持续学习，而LLM生成的时候没有反馈，不知道生成的句子是好是坏，也没有从对方的回复中获得反馈，进而改变自己
> 	- John McCarthy关于智能的定义：智能是通往目标的计算能力

参考链接：
- [# Richard Sutton – Father of RL thinks LLMs are a dead end](https://www.youtube.com/watch?v=21EYKqUsPfg)